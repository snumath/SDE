\documentclass[12pt]{report}
\usepackage[left=2cm,right=2.6cm,top=2.6cm,bottom=3cm,a4paper]{geometry}
\renewcommand{\baselinestretch}{1.3}

\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[inline]{enumitem}
\usepackage{linegoal}
\usepackage{amsmath,amssymb,latexsym,amsfonts, amsthm}
%\usepackage{verbatim}
\usepackage{xcolor}
%\usepackage{listings}
\usepackage{comment}
\usepackage{mathtools}
% \usepackage{kotex}

%\usepackage[math]{iwona}
\usepackage[tracking]{microtype}
\usepackage[sc,osf]{mathpazo}
%\usepackage{kpfonts}

\usepackage[all]{xy}

%\setlist[itemize]{label=\tiny\textbullet}
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}

% Modify Chapter Headings
% Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, Bjornstrup
% \usepackage[Lenny]{fncychap}

% Modify Section Style
\usepackage[explicit]{titlesec}
\usepackage{soul}
%\definecolor{titleblue}{HTML}{4a7aa4}
\newbox\TitleUnderlineTestBox
\newcommand*\TitleUnderline[1]
    {%
        \bgroup
        \setbox\TitleUnderlineTestBox\hbox{\colorbox{black}\strut}%
        \setul{\dimexpr\dp\TitleUnderlineTestBox-.3ex\relax}{.3ex}%
        \ul{#1}%
        \egroup
    }
\newcommand*\SectionNumberBox[1]
    {%
        \colorbox{black}
            {%
                \makebox[1.5em][c]
                {%
                    \vspace{7mm}
                    \color{white}%
                    \strut
                    \csname the#1\endcsname
                }
            }%
        \TitleUnderline{\ \ \ }%
    }
\titleformat{\section}
    {\Large\bfseries\sffamily\color{black}}
    {\SectionNumberBox{section}}
    {0pt}
    {\TitleUnderline{#1}}

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}



% tableofcontents with clickable
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, % make the links colored
    linkcolor=blue, % color TOC links in blue
    urlcolor=red, % color URLs in red
    linktoc=all % 'all' will create links for everything in the TOC
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New command
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\1}{\mathbb{1}}
\renewcommand{\O}{\Omega}
%\newcommand{\RP}{\mathbb{RP}}
%\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\r}{\rightarrow}
%\newcommand{\g}{\gamma}
%\newcommand{\E}{\mathcal{E}}
%\renewcommand{\S}{\mathcal{S}}
%\newcommand{\T}{\mathcal{T}}
%\newcommand{\M}{\mathfrak{M}}

%\newcommand{\inft}{\int_{-\infty}`^\infty}
%\newcommand{\rk}{\text{rank }}

\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\newcommand{\ri}{\Rightarrow}
\newcommand{\bigslant}[2]{{\raisebox{.1em}{$#1$}\left/\raisebox{-.1em}{$#2$}\right.}}

\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem 지정해주는 곳

\newtheoremstyle{break}
{\topsep}{\topsep}%
{\itshape}{}%
{\bfseries}{}%
{\newline}{}%
\theoremstyle{break}
\newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter

\newtheoremstyle{newdef}% name
{}%         Space above, empty = `usual value'
{}%         Space below
{}%         Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}%Thm head font
{}%        Punctuation after thm head
{\newline}% Space after thm head: \newline = linebreak
{}%         Thm head spec
\theoremstyle{newdef}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{remark}
\newtheorem*{exmp}{Example} % no numbering example
\newtheorem*{lem}{Lemma}
\newtheorem*{rem}{Remark} % no numbering remark



\usepackage[toc]{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{titlepage}
\centering
{\scshape\LARGE Seoul National University \par}
\vspace{1cm}
{\scshape\Large Lecture Note\par}
\vspace{5.5cm}
{\huge\bfseries Introduction to Stochastic \\Differential Equations\par}
\vspace{1.5cm}
\large Lecture by Seo Insuk \\
Notes taken by Lee Youngjae


\vfill
\vspace{1cm}\par
{\large \today\par}
\end{titlepage}

\setlength{\parindent}{0cm}

\tableofcontents

\setcounter{chapter}{-1}

\chapter{Introduction}
E-mail: \textit{insuk.seo@snu.ac.kr, 27-212}\\
Office Hour: Tuesday 15:00 - 16:00

Grading
\begin{itemize}
\item Mid-terms 1 (15\%, 10/10 or 17)
\item Mid-terms 2 (15\%, 11/7)
\item Fianl-term (40\%)
\item Assignment (20\%, 8-10 times)
\item Attendance (10\%, absent: -2\%, late: -1\%)
\end{itemize}


Let $X$ be a standard normal random variable in $\R$.
i.e., $\mathbb{P}[X \in [a,b]] = \int_a^b \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx$.
(Central Limit Theorem) If $x_1, x_2, \cdots, x_n \in X, E(x_i) = m, Var(x_i) = \sigma^2$, then
$$
\frac{\frac{x_1-m}{\sigma}+\frac{x_2-m}{\sigma}+\cdots+\frac{x_n-m}{\sigma}}{\sqrt n} \rightarrow X
$$

In this class, we study dynamic version of this theorem.
If $(W_t)_{t\geq 0}$ be a fluctuation, then $(W_t)_{t\geq 0}$ be a random variable in $C[0,T]$

\begin{exmp}
$\frac{dX_t}{dt} = rX_t; dX_1 = rX_tdt$. Then, $X_t = X_0 e^{rt}$ (unrisky assets, bank)\\
$dX_t = rX_tdt + \sigma X_tdW_t$, $\sigma:$ volatility (risky assets, stock)
\end{exmp}

We will study:
\begin{enumerate}
\item Probability Space
\item Random Variable
\item Expectation
\end{enumerate}

Textbooks:
\begin{enumerate}
\item Stochastic Calculus for Finance \RNum{2} (Shreve), covering chapter 1-3 or 4
\item Introduction to Stochastic Integration (Hui-Hsiung Kuo)
\end{enumerate}


\part{Stochastic calculus for finance}


\chapter{General Probability Theory}
\section{Infinite Probability Spaces}
There are three elements consisting probability space:
\begin{itemize}
\item $S$: Sample space
\item $\mathcal{E}$: Family of events $\mathcal{E} \subset 2^S$ ($\sigma$-algebra in measure theory)
\item $\mathbb{P}$: probability $\Rightarrow \mathbb{P}(E)$ is defined for all $E \in \mathcal{E}$ ($\mu$ with $\mu(S)=1$)
\end{itemize}

\begin{exmp}
\begin{minipage}[t]{\linegoal}
\begin{enumerate}
\item Toss a coin twice (H for Head, T for Tail)\\
Then, $S = \{HH, HT, TT, TT\}$
\item Uniform random variable in $[0,1]^3$\\
Then, $S = [0,1]^3$.
If $E = [0,\frac{1}{2}]^3$, then $\mathbb{P}(E) = Vol(E) = \frac{1}{8}$\\
\end{enumerate}
\end{minipage}

How to define $\mathcal{E}$?\\
In example 2, let $\mathcal{E} = $ family of all subsets of $[0,1]^3$ naively.
But Banach-Tarski Paradox says there are disjoint sets $E,F$ with $\mathbb{P}(E\cup F) \neq \mathbb{P}(E) + \mathbb{P}(F)$ in this $\mathcal{E}$.
Therefore we cannot naively set $\mathcal{E}$ (Use measure theory)\\

In example 1, suppose that we cannot see the second flip.
If $\{HH\} \not\in \mathcal{E}$ and $\{HT, HH\}\in\mathcal{E}$, then $\mathcal{E} = \{\phi, \{HH,HT\}, \{TH,TT\}, \{HH,HT,TH,TT\}\}$
\end{exmp}


\begin{defn}[Measure]
Let $\Omega$ be a non-empty set and $\mathcal{F}$ be family of subsets of $\Omega$ with
\begin{enumerate}
\item $\phi \in \mathcal{F}$
\item $A \in \mathcal{F} \Rightarrow A^C \in \mathcal{F}$
\item $A_1, A_2, \cdots \in \mathcal{F} \Rightarrow \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
\end{enumerate}
We say $\mathcal{F}$ as \textbf{$\sigma$-algebra} or \textbf{$\sigma$-field}, $A \in \mathcal{F}$ as \textbf{measurable}, and $\Omega$ as \textbf{measurable space}.
\end{defn}


\textit{Exercises.}
\leavevmode
\begin{enumerate}[label = \arabic*)]
\item $\Omega \in \mathcal{F}$
\item $A_1, A_2, \cdots \in \mathcal{F}$, then $A_1\cap A_2 \cdots \in \mathcal{F}$
\item $A_1, A_2, \cdots \in A_n \in \mathcal{F}$, then $A_1 \cup \cdots \cup A_n, A_1 \cap \cdots \cap A_n \in \mathcal{F}$.
\item $A, B \in \mathcal{F}$, then $A-B \in \mathcal{F}$
\end{enumerate}


\begin{defn}[Topological Space]
(See Rudin: \textit{Real and Complex Analysis, Chapter 1.})
Let $\Theta$ be non-empty set and $\tau$ be family of subsets of $\Theta$ with
\begin{enumerate}
\item $\phi, \Theta \in \tau$
\item $V_1, \cdots V_n \in \tau \Rightarrow V_1 \cap \cdots \cap V_n \in \tau$
\item $V_\alpha \in \tau \enspace \forall \alpha \in I \Rightarrow \bigcup_{\alpha\in I}V_\alpha \in \tau$.
\end{enumerate}
We say $V\in\tau$ be an \textbf{open set}, and $(\Theta,\tau)$ be a \textbf{topological space}.
\end{defn}


\begin{defn}[Measurable Function]
$f : (\Omega, \mathcal{F}) \rightarrow (\Theta, \tau)$ is \textbf{measurable} if
$f^{-1}(V) \in \mathcal{F} \enspace \forall V \in \tau$
\end{defn}


\begin{defn}[Positive Measure]
Let $\Omega$ be non-empty set and $\mathcal{F}$ be $\sigma$-algebra.
Then $\mu: \mathcal{F} \rightarrow [0,\infty]$ is called \textbf{measurable} if
\begin{enumerate}
\item $A_1, A_2, \cdots$: disjoint members of $\mathcal{F} \Rightarrow \mu(A_1\cup A_2\cup \cdots) = \sum_{i=1}^\infty \mu(A_i)$
\item $\mu(A) < \infty$ for some $A \in \mathcal{F}$,
\end{enumerate}
and $(\Omega, \mathcal{F}, \mu)$ is called a \textbf{measure space}.
\end{defn}


\begin{defn}[probability space, random variable]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $(\Omega, \mathcal{F}, \mathbb{P})$ is called a \textbf{probability space} if $\mathbb{P}(\Omega) = 1$.
\item $X$ is called a \textbf{random variable} if it is a function from $(\Omega, \mathcal{F}, \mathbb{P})$ to $\mathbb{R}$
\end{enumerate}
\end{defn}
% Note: Here the usage of the expression "be called as" is incorrect. 
% See: https://www.quora.com/What-is-the-difference-between-called-and-called-as


\underline{Next Class}
\begin{itemize}
\item Borel sets on $\mathbb{R}$ or $\mathbb{R}^d$
\item Lebesgue Measure
\item Lebesgue Integral (Define Expectation of random variable)
\end{itemize}

\vspace{5mm}

Last class, we define a sample space $\Omega$, a $\sigma$-algebra $\mathcal{F}$, and a (positive) measure $\mu : \mathcal{F} \rightarrow [0,\infty]$.
\vspace{5mm}

\textit{Exercises.}
\begin{itemize}
\item $A_1 \subset A_2 \subset \cdots \Rightarrow \mu(\bigcup_{i=1}^\infty A_i) = \lim_{n\rightarrow\infty}\mu(A_n)$
\item $A_1 \supset A_2 \supset \cdots, \mu(A_1) < \infty \Rightarrow \mu(\bigcap_{i=1}^\infty A_i) = \lim_{n\rightarrow\infty}\mu(A_n)$
\end{itemize}
% These are called continuity from below and continuity from above, resp.

\begin{thm}[Rudin 1.10]
Let $\mathcal{F}_0$ be a collection of subset of $\Omega$.
Then, $\exists! \mathcal{F}^*$ minimal $\sigma$-algebra containing $\mathcal F_0$.

\begin{proof}
Let $\{\mathcal{F}_\alpha, \alpha \in I\}$ be a family of $\sigma$-algebra containing $\mathcal{F}_0$.
Then, $\mathcal{F}^* = \bigcap_{\alpha\in I} F_\alpha$ satisfies the three condition:
1) contain $\mathcal{F}_0$
2) $\sigma$-algebra
3) minimal (trivial, $\mathcal{F}^* \subset \mathcal{F}_\alpha)$
\end{proof}
\end{thm}


\begin{defn}[Borel measurable]
$\mathcal{B}$ is a \textbf{Borel $\sigma$-algebra} on the topological space $(\Theta,\tau)$ if
$\mathcal{B}$ is a minimal $\sigma$-algebra containing $\tau$, and $B$ is a \textbf{Borel measurable} if $B \in \mathcal{B}$.
\end{defn}

\begin{rem}[Completion of measure space, Rudin 1.15]
\leavevmode\\
Consider an extension $(\Omega, \mathcal{F}, \mu) \rightarrow (\Omega, \overline{\mathcal{F}}, \mu)$ where
\begin{enumerate}
\item $\overline{\mathcal{F}} = \{ A \cup N : A \in \mathcal{F}, N \subset A_0 \in \mathcal{F}, \mu(A_0) = 0 \}$
\item $\mu(A \cup N) = \mu(A)$
\end{enumerate}
Then, (Check!)
\begin{enumerate}
\item (well-definedness) $A_1 \cup N_1 = A_2 \cup N_2 \Rightarrow \mu(A_1) = \mu(A_2)$
\item $\mu: \overline{\mathcal{F}}$ is $\sigma$-algebra.
\item $\mu : \overline{\mathcal{F}} \rightarrow [0,\infty]$ is a measure
\end{enumerate}
\end{rem}

\begin{exmp}
\leavevmode
\begin{enumerate}[label = \arabic*)]
\item $\mathbb{R}$
$$
\xymatrix@R+0.25em@C+2em{
\ar @{} [dr] |{}
\mathcal{F}_0 = \tau \ar[r]^{1.10} & \mathcal{B} \ar[r]^{\text{completion}} & \overline{\mathcal{B}}\\
\mathcal{L} \ar[r]^{\text{Rudin CH 2}} & \mathcal{L} \ar[r]^{\text{completion}} \ar[r] & \mathcal{L}
}
$$



\item $C[0,T] = \Omega = \{f ; f : [0,T] \rightarrow \mathbb{R}, \text{continuous}\}$.\\
Define $\mathcal{F}_0 = \{\bigcup_{t_1,t_2,\cdots,t_k}(A_1,A_2,\cdots,A_k)
: 0 \leq t_1 < t_2 < \cdots < t_k \leq T; A_1, \cdots A_k \in \overline{\mathcal{B}}
\}$.
We call $\{f \in C[0,T]: f(t_1) \in A_1, f(t_2) \in A_2, \cdots, f(t_k) \in A_k\}$ as \textbf{cylindrical set}.
Consider
% 깔끔하게좀 바꾸자
$$
\begin{aligned}
\mathcal{F}_0 &\overset{1.10}{\longrightarrow} &\mathcal{B} &\overset{\text{completion}}{\longrightarrow} &\overline{\mathcal{B}}\\
\mathbb{P}_{\text{BM}} &\overset{\text{KET}}{\longrightarrow} &\mathbb{P}_{\text{BM}} &\overset{\text{completion}}{\longrightarrow} &\mathbb{P}^*_{\text{BM}}
\end{aligned}
$$
(KET refers Kolmogorov's Extension Thm)
\end{enumerate}
\end{exmp}

\section{Random Variables and Distributions}
\begin{defn}
$f : \Omega \rightarrow \mathbb{R}$ is measurable if $f^{-1}(V) \in \mathcal{F}$ for any open set $V \subset \mathbb{R}$.
\end{defn}

\begin{rem}
$\mathcal{B}(\mathbb{R})$ = Borel $\sigma$-algebra in $\mathbb{R}$.
\end{rem}

\begin{rem}
If $f$: measurable, then $f^{-1}(B) \in \mathcal{F}$ for any $B \in \mathcal{B}(\mathbb{R})$.
\begin{proof}
Let $G = \{ A \subset \mathbb{R} : f^{-1}(A) \in \mathcal{F} \}$.
Then, $\tau \subset G$, $G: \sigma$-algebra (check!), hence $\mathcal{B}(\mathbb{R}) \subset G$.
\end{proof}
\end{rem}


\begin{defn}
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $(\Omega, \mathcal{F}, \mathbb{P})$ is a \textbf{probability space} if $(\mathbb{P}(\Omega) = 1$.
\item $X$ is a \textbf{random variable} if $X : \Omega \rightarrow \mathbb{R}$ is measurable.
\end{itemize}
\end{defn}

\begin{exmp}
\leavevmode
\begin{enumerate}
\item Toss a coin Twice.\\
$\Omega = \{HH,HT,TH,TT\}$,
$\mathcal{F} = 2^\Omega = \{$all subsets of $\Omega\}$,
$\mathbb{P}(A) = \frac{1}{4}|A|, \enspace A \in \mathcal{F}$.\\
Then, $X = $ the number of $H$'s is a random variable with $X(HH) = 2, X(HT)=X(TH)=1, X(TT)=1$.

\item Uniform random variable in $[0,1]$\\
$\Omega=  [0,1]$,
$\mathcal{F} = \{B \in \mathcal{B}(\mathbb{R}) : B \subset [0,1]\}$,
$\mathbb{P}(B) = \mathcal{L}(B)$ ($\mathbb{P}([0,1]) = \mathcal{L}([0,1]) = 1$).\\
Then, $X : [0,1] \rightarrow \mathbb{R}$ with $X(x) = 1$ is a (uniform) random variable in $[0,1]$.
\end{enumerate}
\end{exmp}


\begin{rem}
$\mathcal{L}$: Lebesgue measure on $\mathbb{R}$. i.e.,$\mathcal{L}(a,b) = b-a$.
Then, $\mathcal{L}(\{a\}) = 0$\\
$(\because
\{a\} = \bigcap_{i=1}^\infty (a-\frac{1}{n}, a+\frac{1}{n})
\Rightarrow \mathcal{L}(\{a\}) = \lim_{n\rightarrow\infty} \mathcal{L}((a-\frac{1}{n}, a+\frac{1}{n})) = 0
)$\\
Similarly, $\mathcal{L}([a,b]) = \mathcal{L}([a,b)) = \mathcal{L}((a,b]) = b-a$,
$\mathcal{L}(\mathbb{Q}) = \sum_{q \in \mathbb{Q}}\mathcal{L}(\{q\}) = 0$.
\end{rem}

Return to uniform random variable,
$$\mathbb{P}[X \in (a,b)] = \mathbb{P}[\{x : X(x) \in (a,b)\}] = \mathbb{P}[(a,b)] = b-a.$$

\begin{defn}[Distribution measure on $X$]
$X$ is a random variable in $(\Omega, \mathcal{F}, \mathbb{P})$.
$\mu_X$ is a \textbf{distribution measure} on $X$ if $\mu_X$ is a probability measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ such that 
$$
\mu_X(B) = \mathbb{P}[X \in B]
= \mathbb{P}[\{\omega : X(\omega) \in B\}]
= \mathbb{P}[X^{-1}(B)] \ \forall B \in \mathcal{B}(\mathbb{R})
$$
\end{defn}


\begin{defn}[Probability density function]
$f$ is a \textbf{probability density function} of $X$ if $\mu_X((a,b)) = \int_a^b f(x)dx$
\end{defn}

\begin{rem}
There is a measure with no pdf: Dirac measure
\end{rem}

\begin{rem}
Lebesgue-Radon-Nikodym decomposition implies that any measure can be decomposed as density part and singular part.
\end{rem}

\begin{exmp}[Standard Normal random variable]
\leavevmode\\
Let $\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$.
Define $F : (0,1) \rightarrow \mathbb{R}$ by $F(x) = N^{-1}(x)$ for $N(X) = \int_{-\infty}^x \phi(y)dy$.\\
Let $\Omega = (0,1), \mathcal{F} = \{B \in \mathcal{B}(\mathbb{R}) : B \subset (0,1)\}, \mathbb{P}(A) = \mathcal{L}(A) : A \in \mathcal{B}(\mathbb{R})$.\\
Then, $Y : \Omega \ni x \mapsto F(x) \in \mathbb{R}$ is a random variable with
$$
\begin{aligned}
\mathbb{P}[Y \in (a,b)]
&= \mathbb{P}[\{x : Y(x) \in (a,b)\}]\\
&= \mathbb{P}[\{x \in (N(a),N(b))\}]\\
&= N(b)-N(a) = \int_a^b \phi(x)dx,
\end{aligned}
$$
and a density function is $\phi$.
\end{exmp}


\vspace{5mm}
Previous Question: In the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and random variable $X : \Omega \rightarrow \mathbb{R}$, the random element or random realization $\omega \in \Omega$ is a element of events in sample space.
For example, $\omega = HHTTH$ is a random element in tossing a coin five times, and $X(\omega) = 3$. ($X(\omega)$ = \# of Heads)\\
In the previous example(Standard Normal random variable), define $(\Omega, \mathcal{F}, \mathbb{P}) = ((0,1), \mathcal{B}(0,1), \mathbb{P})$, $\mathbb{P}((a,b)) = b-a$, $F(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy$, $X : (0,1) \ni \omega \mapsto F^{-1}(\omega) \in \mathbb{R}$.
Then, $X$ is called a standard normal random variable.







\section{Expectations}
In the following, let $\Omega = (\Omega, \mathcal{F}, \mathbb{P})$.
Let $X : \Omega \rightarrow \mathbb{R}$.
Then the expection $\mathbb{E}(X)$ is a mean of $X(\omega)$ with respect to the randomness of $\omega$ (given by $\mathbb{P}$)


\begin{defn}[Lebesgue Integration]
$(\Omega, \mathcal{F}, \mu)$ is a measure space, and $f : \Omega \rightarrow \mathbb{R}$ is a measurable function.
\begin{enumerate}[label = (\arabic*)]
\item $f : \Omega \rightarrow [0,\infty)$\\
Let $0 = y_0 < y_1 < y_2 < \cdots \rightarrow \mathbb{R}$ be a partition of $[0,\infty)$,\\
$\Pi = \{y_0, y_1, y_2, \cdots\}$ : $\|\Pi\| = \sup_{i\geq 1}|y_i-y_{i-1}|$, and\\
$\text{LS}_\pi = \sum_{i=0}^\infty y_i \mu[f^{-1}([y_i,y_{i+1}))]$.\\
In Rudin's book, $\lim_{\|\Pi\|\rightarrow 0} \text{LS}_\Pi$ converges to an element belonging to $[0,\infty]$.\\
Now, $\int fd\mu := \lim_{\|\Pi\|\rightarrow 0}\text{LS}_{\Pi}$ is called a \textbf{Lebesgue Integral}.

\item $f : \Omega \rightarrow \mathbb{R}$\\
Let $f^+ = \max\{f,0\} \geq 0$, and $f^- = -\min\{f,0\} \geq 0$.
Then, $f = f^+ - f^-$, and $|f| = f^+ + f^-$.
If $\int f^+d\mu < \infty$ and $\int f^- d\mu < \infty$, then we say $f$ is Lebesgue integrable and $f \in L^1(\mu)$.
The Lebesgue integral of $f = \int fd\mu$ is defined as $\int f^+ d\mu - \int f^- d\mu$
\end{enumerate}
\end{defn}

\begin{rem}
\leavevmode
\begin{enumerate}
\item $\int f^+ d\mu < \infty$ and $\int f^- d\mu = \infty$, then $\int fd\mu = -\infty$.
The others are defined similarly.
\item $f \in L^1(\mu) \Leftrightarrow \int |f|d\mu < \infty$.
\end{enumerate}
\end{rem}


\begin{exmp}[Riemann vs Lebesgue Integral (p19-22)]
\leavevmode
\begin{itemize}
\item $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathcal{L})$ Lebesgue measure where $\mathcal{L}((a,b)) = b-a$.
\item $f : \mathbb{R} \rightarrow \mathbb{R} \in L^1(\mathcal{L})$
\item (Def) $A \subset \mathbb{R}$, $\int_A fd\mu := \int f \mathbb{1}_A d\mu$, where $\mathbb{1}_A(x) = 1$ if $x \in A$, and $0$ otherwise.
\end{itemize}
If $f$ is Riemann integrable, then $\int_{[a,b]} fd\mathcal{L} = \int_a^b fdx$.

Riemann integral is a limit of approximation by a partition of $x$-axis.
On the other hand, Lebesgue integral is a limit of approximation by a partition of $y$-axis with preimage.
Partition of $x$-axis is sensitive to fluctuation and restricted to Euclidean space, while partition of $y$-axis is not.
For example, $f(x) = \mathbb{1}_\mathbb{Q}(x)$ is Lebesgue integrable, but it is not Riemann integrable since it is sensitive to fluctuation.
\end{exmp}

\begin{defn}[Almost everywhere, 1.1.5 in Textbook]
$P(x)$ is a property at $x \in \mathbb{R}$.
We say $P$ holds \textbf{almost everywhere} (or a.e.) in $\mathbb{R}$ if and only if
$\mathcal{L}(\{x:P(x)$ does not hold $\} = 0$.
\end{defn}

\begin{exmp}
$f(x) = [x]$ is continuous almost everywhere.
\end{exmp}


\begin{thm}
$f$ is Riemann integrable if and only if $f$ is continuous a.e.
\end{thm}

\textit{Exercises.} $f = g$ a.e. $\Rightarrow \int fd\mathcal{L} = \int g d\mathcal{L}$.

\begin{defn}[Almost surely]
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space.
The event $A (\in \mathcal{F})$ occurs \textbf{almost surely} (a.s.) if $\mathbb{P}(A) = 1$.
\end{defn}

\begin{exmp}
Let $X$ be a uniform random variable in $(0,1)$.
Let $A = \{ X(\omega) \neq \frac{1}{2}\}$; $\mathbb{P}(A) = 1$.
\end{exmp}



\begin{defn}[Expectation, 1.3.3. in Textbook]
\textbf{Expectation} of $X : \Omega \rightarrow \mathbb{R}$ is defined by
$$\mathbb{E}(X) := \int_\Omega X d\mathbb{P} \quad \text{if} \quad \int_{\Omega}|X|d\mathbb{P} < \infty$$ 
\end{defn}

\begin{thm}[1.3.4 in Textbook]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $X$ takes finite number of values $\{x_1,x_2, \cdots, x_n\} \Rightarrow \mathbb{E}(X) = \sum_{i=1}^n x_i\mathbb{P}(X = x_i)$
\item $X, Y$: random variables, $E(|X|), E(|Y|) < \infty$,
\begin{enumerate}[label = (\roman*)]
\item $X \leq Y$ a.s. (i.e. $\mathbb{P}[\{X(\omega) \leq Y(\omega)\}]=1$), then $\mathbb{E}(X) \leq \mathbb{E}(Y)$
\item $X = Y$ a.s. $\Rightarrow \mathbb{E}(X) = \mathbb{E}(Y)$
\end{enumerate}
\item $X, Y$: random variables, $\mathbb{E}(|X|), \mathbb{E}(|Y|) < \infty \Rightarrow \mathbb{E}(\alpha X + \beta Y) = \alpha \mathbb{E}(X) + \beta \mathbb{E}(Y)$.
\item Jensen's Inequality: $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is a convex function $\Rightarrow \phi(\mathbb{E}(X)) \leq \mathbb{E}(\phi(X))$\\
(\textit{cf.} $\phi(t) = t^2$)
\end{enumerate}

\begin{proof}[Proof of 4.]
Define $S_\phi = \{(a,b) \in \mathbb{R}^2 : a + bt \leq \phi(t) \enspace \forall t\}$.
Then $\forall t \in \mathbb{R}, \phi(t) = \sup_{(a,b) \in S_\phi} \{a+bt\}$.
In fact, it is a equivalent condition. Now,
$$
\begin{aligned}
\phi(\mathbb{E}[X]) &= \sup_{a,b\in S_\phi} \{a + b\mathbb{E}[X]\}\\
&= \sup_{a,b \in S_\phi} \mathbb{E}[a+bX]\\
&\leq \mathbb{E}[\sup_{a,b\in S_\phi}(a+bX)] = \mathbb{E}[\phi(X)] \quad (\text{Check!})
\end{aligned}
$$
\end{proof}
\end{thm}


\begin{exmp}[Dirac Measure in $\mathbb{R}$]
$(\mathbb{R}, \mathcal{B}(\mathbb{R}), \delta_y) \enspace (y \in \mathbb{R})$ is a probability space with $\delta_y(A) = 1$ if $y \in A$, and $0$ otherwise.
Then, $\int_\mathbb{R} fd\delta_y = f(y)$ (Check!)
% The standard expression is 'a measure "on" a space', not "in"

\vspace{6mm}
Consider modeling: $X$: random variable such that probability of $x_i = p_i$ with $\sum_{i=1}^n p_i = 1$.
Then, $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mu)$ with $\mu = \sum_{i=1}^n p_i\delta_{x_i}$ is a probability space, and $P(X = x_i) = p_i$ for $X : \mathbb{R} \ni \omega \mapsto \omega \in \mathbb{R}$: Example of thm 1.3.4.
\end{exmp}


\textit{Summary:}
\begin{itemize}
\item Probability space: $(\Omega, \mathcal{F}, \mathbb{P})$
\item Random variables: $X: \Omega \rightarrow \mathbb{R}$
\item Expectation: $E(X) = \int Xd\mathbb{P}$
\end{itemize}





\section{Convention of Integrals}
We will use this section when we define the Brownian motion.
\begin{defn}
\leavevmode
\vspace{-6mm}
\begin{enumerate}[label = (\arabic*)]
\item
Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, and
$f, f_1, f_2, \cdots$ be measurable ($\Omega \rightarrow \mathbb{R}$).
Then, $f_n \rightarrow f$ \textbf{almost everywhere} (a.e.) if
$$
\mu[\{\omega:(f_n(\omega))_{n=1}^\infty \text{ does not converge to } f(\omega)\}] = 0
$$

\item
Let ($\Omega, \mathcal{F}, \mathbb{P}$) be a probability space, $X, X_1, X_2, \cdots$ be random variables.
Then, $X_n \rightarrow X$ \textbf{almost surely} (a.s.) if
$$\mathbb{P}[\{\omega: (X_n(\omega))_{n=1}^\infty \text{ does not converge to } X(\omega)\}] = 0$$
\end{enumerate}
\end{defn}


\textit{Question:}
$f_n \rightarrow $ a.e. Then, $\int f_nd\mu \rightarrow \int fd\mu$?
$X_n \rightarrow X$ a.s. Then, $\mathbb{E}[X_n] \rightarrow \mathbb{E}[X]$?


\begin{thm}[Monotone Convergence Theorem. 1.4.5 in Textbook]
$0 \leq f_1 \leq f_2 \leq \cdots$(or decreasing), and $f_n \rightarrow f$ a.e.
Then, $\int f_nd\mu \rightarrow \int fd\mu$.
\end{thm}

\begin{thm}[Dominated Convergence Theorem. 1.4.9 in Textbook]
$\exists g \in L^1(\mu)$ such that $|f_n| \leq g$ for all $n$, and $f_n \rightarrow f$ a.e.
Then, $\int f_n d\mu \rightarrow \int f d\mu$.
\end{thm}

\begin{cor}
$\exists Y \in L^1(\mathbb{P})$ such that $|X_n| \leq Y$ for all $n$, and $X_n \rightarrow X$ a.s.
Then, $\mathbb{E}[X_n] \rightarrow \mathbb{E}[X]$.
\end{cor}

\begin{exmp}
Let $f_n(x) = 
\begin{cases}
n^2x & \text{if } 0 \leq x \leq \frac{1}{n},\\
-n^2x + n & \text{if } \frac{1}{n} < x \leq \frac{2}{n},\\
0 & \text{otherwise}.
\end{cases}
$
Then, $f_n \rightarrow 0$ a.e. and $\int f_n dx = 1$.
\end{exmp}

\section{Computation of Expectations}

\textbf{Notation:} ($X : \Omega \ni \omega \mapsto X(\omega) \in \mathbb{R}$)
\begin{itemize}
\item $\mathbb{E}[X] = \int X d\mathbb{P} = \int_\Omega X(\omega) d\mathbb{P}(\omega)$
\item $\int_B X(\omega)d\mathbb{P}(\omega) := \int \mathbb{1}_B(\omega) X(\omega) d\mathbb{P}(\omega)$
\end{itemize}

\textbf{Recall:} $X$: random variable, $\mu_X$: distribution measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$,
$\mu_X(B) = \mathbb{P}(X \in B)$.


\begin{thm}
$g \in L^1(\mu_X)$.
Then, 
$\mathbb{E}[g(X)] = \int_\mathbb{R} g(x)d\mu_X(x) (:= \int g d\mu_X)$.

\begin{exmp}
$g(x) = x$.
$\int |x| d\mu_X(x) < \infty \Rightarrow \mathbb{E}[X] = \int xd\mu_X(x)$.
\end{exmp}

\begin{proof}
First, prove the thm holds for $g \geq 0$, then prove for general $g$ by $g = g^+ - g^-$.
\begin{enumerate}[label = (\arabic*)]
\item $g = \mathbb{1}_B$\\
By thm 1.3.4. (1), $E[\mathbb{1}_B(X)] = 1 \cdot \mathbb{P}[\mathbb{1}_B(X) = 1] = \mathbb{P}(X\in B) = \mu_X(B)
= \int \mathbb{1}_B(x) d\mu_X(x)$.
\item $g = \sum_{k=1}^n \alpha_k \mathbb{1}_{B_k}$\\
Trivial by linearity.
\item $g \geq 0$\\
By MCT.
See \textit{Rudin} chapter 1 for details.
\end{enumerate}
\end{proof}
\end{thm}


\textbf{Recall:} $X:$ random variable, $X$ has density function $f_X$ if
$$
\begin{aligned}
\mu_X((a,b)) &= \int_a^b f_X(x)dx \enspace \forall a,b.\\
\mu_X(B) &= \int_B f_Xd\mathcal{L} = \int_B f_X(x) d\mathcal{L}(x) = \int_B f_X(x)dx.
\end{aligned}
$$

\begin{thm}
$g \in L^1(\mu_X)$.
Then, $E(g(X)) = \int_\mathbb{R} g(x)f_X(x)dx$.

\begin{exmp}
Let $X$ be standard normal. i.e., $f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ (regardless what $X$ be).
Then, $E(X^4) = \int_\mathbb{R} x^4 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} = 3$.
\end{exmp}
\end{thm}



\chapter{Information and Conditioning}
\section{Information and $\sigma$-algebras}

\begin{exmp}
Toss a coin Three times.
$\Omega = \{ HHH, HHT, \cdots, TTT\}$.\\
$A_H = \{ HHH, HHT, HTH, HTT\}$,
$A_T = \{ THT, THT, TTH, TTT\}$.\\
Let $\mathcal{F}(1) = \{ \phi, \Omega, A_H, A_T\}$ so that
it is a $\sigma$-algebra containing the randomness up to time 1.\\
Similarly, define $A_{HH}, A_{HT}, A_{TH}, A_{TT}$.\\
Let $\mathcal{F}(2) = \{ \phi, \Omega, A_{HH}, A_{HT}, A_{TH}, A_{TT}, A_{HH} \cup A_{HT}, \cdots, A_{TT}^C\}$
so that it is a $\sigma$-algebra containing the randomness up to time 2, and define $\mathcal{F}(0)$ similarly, and let $\mathcal{F}(0) = \{ \phi, \Omega\}$.\\
Then, $\mathcal{F}(0) \subset \mathcal{F}(1) \subset \mathcal{F}(2) \subset \mathcal{F}(3)$.
Let $X_t =$ \# of heads until time $t$.
Then, $X_t$ is $\mathcal{F}(t)$-measurable for each $t$.\\
Now, $\{ X_1 = 1\} = \{ \omega: X_1(\omega) = 1 \} = A_H$, and $\{X_1 = 0\} = \{ \omega : X_1(\omega) = 0\} = A_T$.
\end{exmp}

% 시간이 지나면서 random variable의 값이 계속 바뀌는데, 시점 t에서 주식 값이 evolute하느냐를 판단을 하고 싶다.
% 이 상황에서는 X_t와 함께 증가하는 \sigma-algebra가 필요하다.
% 시점 t에서 필요한 정보를 모아놓은 것이 sigma-algebra가 된다
% X_t는 시점 t까지의 information만 모아놓은 것.
% 우리는 X_t가 discrete한 경우가 아니라 주식처럼 X_t가 continuous한 경우를 다룬다
% 이를 확률미분방정식으로 나타내고 이걸 배운다.





\begin{defn}[$\sigma$-algebra generated by $X$]
$\Omega$ is a set, $X : \Omega \rightarrow \mathbb{R}$.
$\sigma(X) = \{ A \subset \Omega : A = X^{-1}(B)$ for some $B \in \mathcal{B}(\mathbb{R})\}$.
Then, $\sigma(X)$ is a $\sigma-$algebra(exercise) and it is called a \textbf{$\sigma$-algebra generated by} $X$.
\end{defn}

\begin{rem}
$X$ is a random variable in $(\Omega, \sigma(X))$.\\
$X$ is a random variable in $(\Omega, \mathcal{F})$, then $\sigma(X) \subset \mathcal{F}$ (exercise)
\end{rem}


\begin{defn}[$\mathcal{F}$-measurable]
$(\Omega, \mathcal{F})$: measure space.
$X : \Omega \rightarrow \mathbb{R}$.
$X$ is called \textbf{$\mathcal{F}$-measurable} if $\sigma(X) \subset \mathcal{F}$.
i.e., $X$: measurable with respect to $(\Omega, \mathcal{F}$).
\end{defn}


In example, $X(t)$ is $\mathcal{F}(t)$-measurable $\forall t$ (check!)\\
\textit{cf.} $X(t) : \Omega \rightarrow \mathbb{R}$.
$(X(t))^{-1}(B) \in \mathcal{F}(t) \enspace \forall B \in \mathcal{B}(\mathbb{R})$.\\
Enough to check $(X(t))^{-1}(\{0\}), (X(t))^{-1}(\{1\}), \cdots, (X(t))^{-1}(\{t\})$.\\
$\mathcal{F}(t)$ has enough information to determine $X(t)$ in the sense that
$\{\omega: (X(t))(\omega) \in B\} \in \mathcal{F}(t) \enspace \forall B \in \mathcal{B}(\mathbb{R})$.

\begin{defn}[Filtration, Stochastic Process]
$\Omega$: non-empty set, $T > 0$.
\begin{enumerate}
\item If $\mathcal{F}(t)$ is a $\sigma$-algebra $\forall t \in [0,T] \in T$ and $s < t \Rightarrow \mathcal{F}(s) \subset \mathcal{F}(t)$,
then $(\mathcal{F}(t) : t \in [0,T])$ is called a \textbf{filtration}
\item If $X(t) : \Omega \rightarrow \mathbb{R}$ is $\mathcal{F}(t)$-measurable $\forall t \in [0,T]$,
then $(X(t) : t \in [0,T])$ is called \textbf{Stochastic Process adopted to the filtration} $\mathcal{F}(t)$.
\end{enumerate}
\end{defn}


\section{Independence}

%\section{Conditional Expectation}
$X : \Omega \rightarrow \mathbb{R}$, $\mathcal{F}$: $\sigma$-algebra on $\Omega$.
\begin{enumerate}
\item $\mathcal{F}$ has full information to determine $X \Rightarrow X$ is $\mathcal{F}$-measurable. (2.1)
\item $\mathcal{F}$ has no information to determine $X \Rightarrow X$ is independent to $\mathcal{F}$. (2.2)
\item $\mathcal{F}$ has a partition information to determine $X \Rightarrow$ (2.3)
\end{enumerate}




\begin{defn}[independent]
$(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space.
$A, B \in \mathcal{F}$ is \textbf{independent} if $\mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)$.
\end{defn}

\textit{Question:} $X, Y$ are random variables in $(\Omega, \mathcal{F}, \mathbb{P})$.
If $X, Y$ are independent, then $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$, but the converse does not hold.


\begin{defn}
$(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space.
$\mathcal{G}, \mathcal{H} \subset \mathcal{F}$ are sub $\sigma$-algebras of $\mathcal{F}$.
$X, Y : \Omega \rightarrow \mathbb{R}$ are random variables in $(\Omega, \mathcal{F}, \mathbb{P})$.
\begin{enumerate}
\item $\mathcal{G}, \mathcal{H}$: independent iff $\P(A\cap B) = \P(A) \P(B) \enspace \forall A \in \mathcal{G}, B \in \mathcal{H}$.
\item $X, Y$: independent iff $\sigma(X), \sigma(Y)$ are independent.
\item $X, \mathcal{G}$: independent iff $\sigma(X), \mathcal{G}$ are independent.
\end{enumerate}
\end{defn}



\begin{defn}
$(\O, \F, \P)$ is a probability space.\\
$\mathcal{G}_1, \mathcal{G}_2, \cdots \mathcal{G}_n, \cdots$: sub $\sigma$-algebra of $\F$.
$X_1, X_2, \cdots, X_n, \cdots$: random variable in $(\O, \F, \P)$.
\begin{enumerate}
\item $\mathcal{G}_1, \cdots, \mathcal{G}_2$ are independent iff $\P(A_1 \cap \cdots \cap A_n) = \P(A_1) \cdots \P(A_n)$ for $A_1 \in \mathcal{G}_1, \cdots, A_n \in \mathcal{G}_n$.
\item $X_1, \cdots, X_n$ are independent iff $\sigma(X_1) \sim \sigma(X_n)$ are independent.
\item $\mathcal{G}_1, \mathcal{G}_2, \cdots$ are independent iff $\mathcal{G}_1 \sim \mathcal{G}_n$ are independent $\forall n$.
\item $X_1, X_2, \cdots$ are independent iff $X_1 \sim X_n$ are independent $\forall n$.
\end{enumerate}
\end{defn}

\begin{exmp}
Toss a coin three times.
\begin{enumerate}
\item $X(2), X(3)$ are not independent.\\
$\P(\{X(2) = 2\} \cap \{ X(3) = 1\}) \neq \P(X(2) = 2)\P(X(3)=1)$.
\item $X(2), X(3)-X(2)$ are independent.\\
Why: $X(2)$ is an information at tossing first, second times, and $X(3)$ is an information at tossing third time.
\end{enumerate}
\end{exmp}


\begin{defn}[Joint distribution]
$(\O, \F, \P)$ is a probability space. $X, Y$ are random variables in $\Omega$.
$(X, Y) : \O \ni \omega \mapsto (X(\omega), Y(\omega)) \in \R^2$
\begin{enumerate}
\item Joint Distribution Measure in $\R^2$\\
$\mu_{X,Y}(C) = \P((X,Y) \in C) $
for $C \in \mathcal{B}(\R^2)$.\\
(Note: We checked that $\{\omega : (X(\omega), Y(\omega)) \in C \} \in \mathcal{F}$ in real analysis.)

\item Joint Cumulative Distribution Function\\
$F_{X,Y}(a,b) = \P( X \leq a, Y \leq b) = \mu_{X,Y} ((-\infty, a] \times (-\infty, b])$ (check!)

\item Joint Probability Distribution Function\\
If $f_{X,Y} : \R^2 \r \R$ is Borel-measurable and satisfies
$\mu_{X,Y}(A\times B) = \int_B\int_A f_{X,Y}(x,y)dxdy$ for all $A, B \in \mathcal{B}(\R)$, then $f_{X,Y}$ is called a joint probability density function (jpdf)
\end{enumerate}
\end{defn}


\begin{thm}
$(\O,\F,\P)$ is a probability space, $X, Y$ are random variables in $\Omega$. Then, the followings are equivalent.
\begin{enumerate}[label=(\roman*)]
\item $X,Y$ are independent
\item $\mu_{X,Y}(A\times B) = \mu_X(A)\mu_Y(B) \enspace \forall A,B \in \mathcal{B}(\R)$
\item $F_{X,Y}(a,b) = F_X(a)F_Y(b) \enspace \forall a,b\in\R$
\item $\mathbb{E}[e^{uX+vY}] = \mathbb{E}[e^{uX}] \mathbb{E}[e^{vY}]$
\end{enumerate}
\end{thm}

\begin{rem}
If JPDF $f_{X,Y}$ exists, then (i) to (iv) $\Leftrightarrow f_{X,Y}(x,y) = f_X(x)f_Y(y)$ a.e.
\end{rem}

\begin{thm}
$X,Y$ are independent if and only if
$f, g : \R \r \R$ Borel-measurable, $\mathbb{E}[|f(X)g(Y)|] < \infty$ implies that
$\mathbb{E}[f(X)g(Y)] = \mathbb{E}[f(X)]\mathbb{E}[g(Y)]$.
\end{thm}

\begin{rem}
$f(x) = g(x) = x$ : $\mathbb{E}[|XY|] < \infty \Rightarrow \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$
\end{rem}


\begin{proof}
Details are exercises.
\begin{enumerate}[label = (\arabic*)]
\item $f = \mathbb{1}_A, g = \mathbb{1}_B$
\item $f, g$ are simple functions
\item $f, g \geq 0$
\item $f, g$ are general.
\end{enumerate}
\end{proof}






\underline{Review}\\
$\mathcal{G, H}$ are independent if
$\forall A \in \mathcal{G}, B \in \mathcal{H} \Rightarrow \P(A \cap B) = \P(A)\P(B).$\\
$X, Y$ are independent if $\sigma(X), \sigma(Y)$ are independent.\\
* $\sigma(X) = \{A \in \Omega : A = X^{-1}(B) \text{ for some } B \in \mathcal{B}(\R)\}$.\\
* $\mu_{X,Y}(C) = \P((X,Y) \in C) \enspace \forall C \in \mathcal{B}(\R^2)$.

\vspace{5mm}

Thm. T.F.A.E.C:
\begin{enumerate}
\item $X,Y$ are independent
\item $\mu_{X,Y}(A\times B) = \mu_X(A)\mu_Y(B)$
\item $\F_{X,Y}(x,y) = \F_X(x)\F_Y(y)$
\item (If JPDF $f_{X,Y}$ exists) $f_{X,Y}(x,y) = f_X(x)f_Y(y)$
\end{enumerate}




\begin{thm}
$(\O,\F,\P)$ is a probability space. $X,Y$ are independent random variables, $f, g : \R \r \R$ are Borel measurable.
Then, $f(X), g(Y)$ are independent.

\begin{proof}
$A \in \sigma(f(X))$;
$A = (f\circ X)^{-1}(B)$ for some $B \in \mathcal{\R}
= X^{-1}(f^{-1}(B)) \in \sigma(X)$.\\
$\therefore \sigma(f(X)) \subset \sigma(X)$, $\sigma(g(Y)) \subset \sigma(Y)
\Rightarrow \sigma(f(X)), \sigma(g(Y))$ are independent.
\end{proof}
\end{thm}

\begin{cor}
$\mathbb{E}[f(X)g(Y)] = \mathbb{E}[f(X))\mathbb{E}(g(Y)]$.
\end{cor}



\begin{defn}
$X,Y$ are random variables in $(\O,\F,\P)$.
\begin{enumerate}
\item $Var(X) = \E[(X-\E[X])^2] = \E[X^2] - \E[X]^2$
\item $std(X) = \sqrt{Var(X)}$
\item $Cov(X,Y) = \E[XY] - \E[X]\E[Y]$
\item $corr(X,Y) = cov(X,Y) / (std(X)std(Y))$
\end{enumerate}
\end{defn}

\begin{exmp}
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $X$: standard normal random variable $(N(0,1^2))$
\item $\P(Z = 1) = \P(Z = -1) = \frac{1}{2}$ ($X,Z$ are independent)
\item $Y = XZ $. Then
\begin{enumerate}[label = \arabic*)]
\item Y is standard normal,
\item $corr(X,Y) = 0$.
\item $X,Y$ are not independent.
\end{enumerate}
\end{itemize}
\end{exmp}



\begin{defn}[Jointly normal]
$X, Y$ are \textbf{jointly normal} with mean $m = (m_X, m_Y),\ Var(C) = 
\begin{bmatrix}
    C_{11} & C_{12} \\
    C_{21} & C_{22}
\end{bmatrix}
$ if
$$f_{X,Y}(z) = \frac{1}{\sqrt{(2\pi)^2\det C}}e^{-\frac{1}{2}(z-m)C^{-1}(z-m)^\mathsf{T}}$$
\end{defn}

\begin{thm}
$X,Y$ are jointly normal and uncorrelated $(C_{12} = C_{21} = 0)$.
Then, they are independent.
\end{thm}

\section{Conditional Expectation}
$(\O,\F,\P)$ is a probability space. $\int_A X d\P := \int \mathbb{1}_A X d\P = \int \mathbb{1}_A (\omega) X(\omega) d\P(\omega)$.
\begin{lem}
$\int_A Xd\P = \int_A Y d\P$ for all $A \in \F$ if and only if $X = Y$ a.s.
\begin{proof}
$A_n = \{ \omega : X(\omega) - Y(\omega) > \frac{1}{n}\},
B_n = \{ \omega : X(\omega) - Y(\omega) < -\frac{1}{n}\}$.
Then,
$$0 = \int_{A_n}(X-Y)d\P \geq \int_{A_n}\frac{1}{n}d\P = \frac{1}{n} \int \1_{A_n} d\P
= \frac{1}{n} \P(A_n)$$
Thus, $\P(A_n) = 0 \enspace \forall n$. Similarly, $\P(B_n) = 0$.
Now, $\{ \omega:X(\omega) \neq Y(\omega)\} = \left(\bigcup_{n=1}^\infty A_n\right) \cup \left(\bigcup_{n=1}^\infty B_n\right) \Rightarrow$ measure $0$.
\end{proof}
\end{lem}

\textit{Intuition}.
$(\O,\F,\P)$ is given, $X : \F$-measurable random variable, $\mathcal{G} \subset \F$ is a sub $\sigma$-algebra.
If we know nothing, then we expect $X$ as $\E[X]$.
If we know $\F$, then we expect $X$ as $X$.
Now, if we know $\mathcal{G}$, then we expect $X$ as $\E[X | \mathcal{G}]$ (what is it?)



\begin{defn}[Conditional Expectation]
$(\O,\F,\P)$ is a probability space.
$X \in L^1(\P)$ is a random variable.
$\mathcal{G}$ is a sub $\sigma$-algebra of $\F$.
We define $\E[X | \mathcal{G}]$ as
\begin{enumerate}
\item $\mathcal{G}$-measurable random variable
\item $\int_A \E[X|\mathcal{G}](\omega) d\P(\omega) = \int_A X(\omega) d\P(\omega)$.
\end{enumerate}
\end{defn}

\textit{Question.} $\E[X|\mathcal{G}]$ exists? (Yes! proof skip).
unique? (Yes! up to a.s.)

\begin{rem}
Lemma implies determine $X$ (a.s.) is equivalent to know $\int_A Xd\P \enspace \forall A \in \F$.\\
In this sense, conditional expectation $Y = \E[X|\mathcal{G}]$ is knowing $\int_A Yd\P = \int_A X d\P \enspace \forall A \in \mathcal{G}$.
\end{rem}


\begin{exmp}
Toss a coin three times.\\
$\F(0) \subset \F(1) \subset \F(2) \subset \F(3)$.
$X(t)$ is a number of heads until $t$ times; $X(t)$ is $\F(t)$-measurable.
If $\F(1) = \{ \phi, \Omega, A_H, A_T\}$, then
$\E[X(2) | \F(1)] = X(1) + \frac{1}{2}$, since we know the information of 1st flip.
\begin{proof}
Want: $\int_A (X(1) + \frac{1}{2})d\P = \int_A X(2) d\P$ for all $A \in \F(1)$
(\textit{c.f.} $\P(\omega) = \frac{1}{8} \enspace \forall \omega \in \Omega$).
For $A = A_H$, $\int \1_{A_H}(\omega) (X(1)(\omega) + \frac{1}{2})d\P(\omega) = \frac{3}{2}\P(A_H) = \frac{3}{4}$.

$\int \1_{A_H}(\omega)(X(2))(\omega) d\P(\omega)
= \sum_{\omega \in A_H} (X(2))(\omega)\P(\omega) = \frac{1}{8}(2+2+1+1) = \frac{3}{4}$.

\end{proof}
\end{exmp}

\begin{rem}
$\mathcal{G} = \sigma(Y)$;
$\E[X|\mathcal{G}] = \E[X | \sigma(Y)] := \E[X|Y]$
\end{rem}

\begin{thm}
$X, Y$ are independent random variable in $(\O,\F,\P)$, $\mathcal{G}$ is a sub $\sigma$-algebra of $\F$.
\begin{enumerate}
\item $\E[aX + bY | \mathcal{G}] = a\E[X|\mathcal{G}] + b\E[Y|\mathcal{G}]$
\item $X$ is $\mathcal{G}$-measurable.
Then, $\E[XY|\mathcal{G}] = X\E[Y|\mathcal{G}]$.
\item $\mathcal{H}$ is a sub $\sigma$-algebra of $\mathcal{G}$. Then,
$\E[\E[X|\mathcal{G}]|\mathcal{H}] = \E[X|\mathcal{H}]$.
\item $X, \mathcal{G}$ are independent, then $\E[X|\mathcal{G}] = \E[X]$.
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item Exercise

\item We only need to show that $X \geq 0, Y \geq 0$ implies 2.
\begin{enumerate}
\item $X = \1_B$\\
Want: $\E[\1_B Y | \mathcal{G}] = \1_B \E[Y | \mathcal{G}]$ for $B \in \mathcal{G}$.

\item $X = \sum_{i=1}^n \alpha_i \1_{B_i}$\\
Use linearity.

\item $X \geq 0$\\
Use MCT

\end{enumerate}

\item Want: $\E[\E[X|\mathcal{G}]|\mathcal{H}] = \E[X|\mathcal{H}]$\\
Let $A \in \mathcal{H}$.
Then, $\int_A \E[\E[X|\mathcal{G}]|\mathcal{H}](\omega) d\P(\omega)
= \int_A \E[X|\mathcal{G}] d\P
= \int_A Xd\P
= \int_A \E[X|\mathcal{H}]d\P$

\item Can be shown similarly as in 2.
Check $X = \1_B$ case. (Hint: $A \in \mathcal{G} \Rightarrow A, B$ are independent.)
\end{enumerate}
\end{proof}
\end{thm}


\begin{exmp}[Revisit]
$$
\begin{aligned}
\E[X(2) | \F(1)]
&= \E[X(2) - X(1) + X(1) | \F(1)]\\
&= \E[X(2) - X(1) | \F(1)] + X(1)\\
&= \E[X(2) - X(1)] + X(1)\\
&= \frac{1}{2} + X(1)
\end{aligned}
$$
\end{exmp}



\textbf{Review}
\begin{itemize}
\item $(\O,\F,\P)$: probability space, $\G \subset \F$.
\item $X : \F$-measurable random variable.
\item $Y = \E[X | \G]$ if $Y$ is $\G$-measurable.
\item $\int_A Y(\omega) d\P(\omega) = \int_A X(\omega) d\P(\omega) \enspace \forall A \in \G$.
\end{itemize}
\begin{rem}
$\E[X|\G]$ is an expectation of $X$ when we know $\G$.
\end{rem}

\begin{rem}
$Y = Z$ a.s. and $Z$ is $\G$-measurable, then $Z = \E[X|\G]$.
\end{rem}



\begin{rem}
$(X(t))_{t\in[0,T]}$ is stochastic process adapted to $(\F(t))_{t\in[0,T]}$.
In this, $(X(t))_{t\in[0,T]}$ is a random variable in $(\O,\F,\P)$ and $X(t)$ is $\F(t)$-measurable.
$\F(s) \subset \F(t)$ for all $s < t$ and $\F(0) = \{\phi,\O\}$.
\end{rem}

\begin{rem}
We can define $\F$ as $\F(t) = \bigcup_{s : s\leq t} \sigma(X(s))$
\end{rem}

\begin{defn}[Martingale, Markov Process]
\leavevmode
\begin{enumerate}
\item Martingale $X(t)$\\
$\E[X(t)|\F(s)] = X(s)$ for all $s < t$.

\item Markov Process $X(t)$\\
For any borel measurable $f$, there exists some borel measurable $g$ such that $\E[f(X(t)) | \F(s)] = g(X(s))$

\end{enumerate}
\end{defn}


\begin{rem}
In Martingale, if we know all the previous value, then the expectation of the future is as same as the expectation of the present.
\end{rem}

\begin{rem}
Markov process is a generalization of Markov chain.
We only have to know the present value.
\end{rem}



\chapter{Brownian Motions}
\section{Introduction}
To study Brownian Motions, we will study:
\begin{enumerate}
\item Random Walks
\item Definition of Brownian Motions and its basic property (We will change the textbook!)
\item Constuction of Brownian Motions
\end{enumerate}


\section{Scaled Random Walks}


\begin{defn}[Random Walk]
\leavevmode
\begin{itemize}
\item
Let
$
X_i = 
\begin{cases}
1 & \text{prob } \frac{1}{2}\\
-1 & \text{prob } \frac{1}{2}
\end{cases}
; X_1, X_2, \cdots
$
independent.

\item $M_n = X_1 + \cdots + X_n$ is called \textbf{random walk}

\item $W_n(t) = \frac{1}{\sqrt n} M_{nt} (:= \frac{1}{\sqrt n}M_{[nt]})^v, \enspace t \in \{ \frac{1}{n}k, k \in \mathbb{Z}_+\}$ is called \textbf{scaled random walk}
\end{itemize}
\end{defn}


\begin{pro}
Random walk holds the following properties:
\begin{enumerate}
\item Independent Increament
\item Martingale
\item Quadratic Variation
\end{enumerate}

\begin{proof}
\leavevmode
\begin{enumerate}
\item See Def 3.2.3
\item Let $\F(n) = \sigma(X_1, X_2, \cdots, X_n) = $ smallest $\sigma$-algebra making$X_1 \sim X_n$ measurable.
Then,
\begin{itemize}
\item $M_n = X_1 + \cdots X_n$ is $\F(n)$-measurable
\item $(M_n)_{n\in\mathbb{N}}$ is stochastic process adapted to $(\F(n))_{n=0}^\infty$
\item $k < l \Rightarrow
\E[M_l | \F(k)] = \E[M_l - M_k | \F(k)] + \E[M_k | \F(k)]
= \E[M_l - M_k] + M_k
= \E[X_{k+1} + \cdots + X_l] + M_k
= \E[X_{k+1}] + \cdots + \E[X_l] + M_k
= M_k.$
\end{itemize}

\item $\sum_{i=1}^n (M_i - M_{i-1})^2 = n$
\end{enumerate}
\end{proof}
\end{pro}

\begin{defn}[Independent increament]
$M_n$ is \textbf{independent increament} if
$M_{k_1}, M_{k_2}-M_{k_1}, \cdots, M_{k_m - k_{m-1}}$ are independent
for any $k_1 < k_2 < \cdots < k_m$.
Here, $M_{k_l} - M_{k_{l-1}}$ is called increament.
If $M_n$ is a random walk, then $M_{k_1} = \sum_{i=1}^{k_1} X_i, M_{k_2-k_1} = \sum_{i=k_1}^{k_2}X_i, \cdots$ are independent.
\end{defn}



\begin{rem}
Proposition 3.2.2 holds for scaled random variable $W_n(t) = \frac{1}{n} M_{nt}$ ($t \in \frac{1}{n} \mathbb{Z}_+$).
\begin{proof}
\leavevmode
\begin{enumerate}
\item Independent Increament\\
For $t_1 < t_2 < \cdots < t_m$,
$W_n(t_1) - W_n(0), W_n(t_2) - W_n(t_1), \cdots, W_n(t_m) - W_n(t_{m-1})$ are independent,
since its increaments $W_n(t_{n+1}) - W_n(t_l) = \frac{1}{n} (M_{nt_{l+1}} - M_{nt_l})$ are independent by independent increament property of $M_n$.

\item Martingale\\
Let $\F_n(t) = \sigma(X_1, X_2, \cdots, X_{nt})$.
Then, $W_n(t) = \frac{1}{n} (X_1+X_2+\cdots+X_{nt})$ is $\F_n(t)$-measurable.
Therefore, $(W_n(t))$ is stochastic process adapted to $(\F_n(t))$.
With some computations as before, $\E[W_n(t) | \F_n(s)] = \cdots = W_n(s)$ for $s < t$.

\item Quadratic Variation\\
$\sum_{i=1}^{nt} \left(W_n(\frac{i}{n}) - W_n(\frac{i-1}{n})\right)^2
= \sum_{i=1}^{nt} \left[ \frac{1}{\sqrt n} (M_i - M_{i-1})\right]^2
= \sum_{i=1}^{nt} \frac{1}{n} \cdot 1
= t$
\end{enumerate}
\end{proof}
\end{rem}



\begin{exmp}
Let $f \in C^1([0,t])$.
Then,
$$
\begin{aligned}
\sum_{i=1}^{nt} \left( f(\frac{i}{n}) - f(\frac{i-1}{n}) \right)^2
&= \sum_{i=1}^{nt} \left[ \frac{1}{n} f'(\frac{x_i}{n})\right]^2\\
&= \frac{1}{n} \frac{1}{n} \sum_{i=1}^{nt} \left( f'(\frac{x_i}{n}\right)^2) \quad ( \rightarrow \int_0^t [f'(x)]^2dx)\\
&\leq \frac{c}{n} \quad (\rightarrow 0)
\end{aligned}
$$
It is the most different property between random process and deterministic function:
Q.V. of random variable is constant but Q.V. of $C^1$ function is zero.
\end{exmp}


\begin{thm}[Central Limit Theorem]
Let $Y_1, Y_2, \cdots$ are independent and identically distributed (called i.i.d.) with mean $0$ and variation $1$
$(\E(Y_i) = 0, Var(Y_i) = \E(Y_i^2) = 1)$.
Then, 
\begin{equation}
\frac{1}{\sqrt n}\left[Y_1 + \cdots Y_n \right] \rightarrow N(0,1^2)\tag{$\bigstar$}
\end{equation}
\end{thm}

\begin{rem}
Meaning of $\bigstar$:
$$\P\left[\frac{1}{n} (Y_1 + \cdots + Y_n) \in [a,b]\right] \rightarrow \int_a^b \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx.$$
\end{rem}


$W_n(t) = \frac{1}{n} M_{nt} = \frac{1}{n} (X_1 + \cdots + X_{nt})
= \sqrt{t} \frac{1}{\sqrt{nt}} (X_1 + \cdots + X_{nt}) \sim N(0,t)$



\textit{cf.} $N(\mu, \sigma^2)$ is a normal random variable with mean $\mu$ and variation $\sigma^2$.
Using the above,
$$
\lim_{n\rightarrow\infty} \P\left[ W_n(t) \in [a,b]\right] = \int_a^b \frac{1}{\sqrt{2\pi t}} e^{-x^2/2t}dx.
$$

$
W_n(t) = \frac{1}{n^{\frac{1}{2} + \alpha}} M_{nt}
\begin{cases}
\alpha < 0 \quad |W_n(t)| \rightarrow \infty\\
\alpha > 0 \quad |W_n(t)| \rightarrow 0\\
\end{cases}
$


\begin{rem}
$\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}$ is a heat kernel in PDE.
\end{rem}

\vspace{5mm}

\textbf{Summary}
\begin{enumerate}
\item Independent Increament
\item Martingale
\item Markov Process
\item $W_n(t) \sim N(0,t)$\\
$W_n(t) - W_n(s) \sim N(0, t-s)$
\item Q.V. in $[0,t] = t$.
\end{enumerate}



\textbf{Review}\\
$X_1, X_2, \cdots$ are i.i.d. and
$X_i = \begin{cases} \pm 1 & 1/2 \\ -1 & 1/2\end{cases}$.\\
Random walk: $\mu_n = X_1 + \cdots + X_n$.\\
Scaled random walk: $W_n(t) = \frac{1}{\sqrt n}M_{nt}$. Then,
\begin{enumerate}
\item $W_n(0)=0)$
\item Independent Increament\\
$t_1 < t_2 < \cdots < t_n$, then
$W_n(t_1), W_n(t_2) - W_n(t_1), \cdots, W_n(t_n) - W_n(t_{n-1})$ are independent.
\item Asymptotic Normal\\
$W_n(t) - W_n(s) \sim N(0,t-s)$ as $n\rightarrow\infty$.
\end{enumerate}



% ========================================
% !New Book!
% ========================================
\part{Introduction to stochastic integral}



\setcounter{chapter}{1}

\chapter{Brownian Motion}
\section{Definition of Brownian Motion}

\begin{defn}[Stochastic Process]
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item Let $(\O, \F, \P)$ be a probability space.
\item $[0,\infty)$ with Borel $\sigma$-algebra
\item $X:[0,\infty) \times \O \rightarrow \R$, measurable.
\end{itemize}
Then, $X$ is a \textbf{stochastic process} if
\begin{enumerate}
\item $X(t,\cdot) : \O \rightarrow \R$ is random variable
\item $X(\cdot,\omega) : [0,\infty) \rightarrow \R$ is measurable.
\end{enumerate}
\end{defn}



\begin{rem}
$X(t,\cdot) \Rightarrow X(t)$: random variable in $\O$.
$X(t) : \omega \mapsto [X(t)](\omega) = X(t,\omega)$.\\
For each $t \in [0,\infty)$ there exists random variable $X(t) : \O\rightarrow \R$.
If we pick $\omega \in \O$, then each $X(t_i)$ is determined simultaneously by $X(t_i)(\omega)$.
\end{rem}


\begin{rem}
We can work in $[0,T]$ instead of $[0,\infty)$.
In fact, we can define in $[0,T]$ and extend to $[0,\infty)$, but it is extremly difficult.
\end{rem}


\begin{defn}[Brownian Motion in $[0,\infty)$]
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $t \in [0,\infty)$, $\omega \in \O((\O,\F,\P)$: probability space)
\item Stoch. Process $B(t,\omega)$
\end{itemize}
$B$ is called \textbf{Brownian Motion} if
\begin{enumerate}
\item $B(0,\omega) = 0$ a.s. (i.e.,$\P\left[\{\omega: B(0,\omega) = 0\}\right]=1$)
\item $B(\cdot, \omega) : [0,\infty) \rightarrow \R$ is a continuous function a.s.
\item $\forall 0 \leq s < t$, $B(t) - B(s) \sim N(0,t-s)$
\item Independent Increament
\end{enumerate}
\end{defn}

\begin{rem}
$B(0,\omega)$ is a measurable function.
\end{rem}

\begin{rem}
$B(t) \sim N(0,t)$ by 3 with $s=0$.
\end{rem}


\begin{rem}
$(B(t))_{t\geq 0} : \O \rightarrow \R$
\begin{itemize}
\item $B(t)$ itself is a normal distribution
\item $B(t) - B(s) : \O \rightarrow \R$ is normal distribution with variance $t-s$.
\end{itemize}
\end{rem}

\begin{rem}
Brownian motion is a continuous version of random walk: random walk has property 1,4 and has property 3 with $n\rightarrow\infty$.
\end{rem}



\begin{thm}
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $s < t$ : $\E[B(s)B(t)] = s$

\item $t_1 < t_2 < \cdots < t_n \Rightarrow (B(t_1), B(t_2), \cdots, B(t_n))$ is jointly normal with $\mu = (0,0,\cdots,0$ and $Var = C$.
$(C_{ij} = t_{\min(i,j)} \enspace \forall i,j)$.
\end{enumerate}

\begin{proof}
\leavevmode
\begin{enumerate}
\item
$\E[B(s)B(t)] = \E[B(s)(B(t)-B(s))] + \E[B(s)^2]
= \E(B(s))\E(B(t)-B(s)) + s = s$.

\item
Let $\vec{v} = (B(t_1), B(t_2)-B(t_1),\cdots,B(t_m)-B(t_{m-1}))$. Then,
$$
\text{PDF of } \vec{v}
= \frac{1}{\sqrt{2\pi t_1}}e^{-\frac{x_1^2}{2t_1}} \cdot \frac{1}{\sqrt{2\pi (t_2 - t_1)}}e^{-\frac{x_2^2}{2(t_2-t_1)}} \cdots \frac{1}{\sqrt{2\pi (t_m-t_{m-1})}}e^{-\frac{x_m^2}{2(t_m-t_{m-1})}}.
$$

Therefore, $\vec{v}$ is jointly normal with $\mu = 0$ and $Var = \text{diag}(t_1, t_2-t_1, \cdots, t_n-t_{n-1}) = D$, and,
$$
\vec{W}
= (B(t_1), \cdots, B(t_m))
= \vec{v}
\begin{bmatrix}
1 & 1 &\cdots &1\\
0 & 1 &\cdots &1\\
\vdots & \vdots & \vdots & \vdots\\
0 & 0 &\cdots &1\\
\end{bmatrix}
= \vec{v}E.
$$
Thus, $\vec{W}$ is jointly normal with $\mu = (0,0,\cdots,0)$ and $Var = EDE^T = C$.
\end{enumerate}
\end{proof}
\end{thm}


\begin{defn}[Filtration for Brownian Motion]
$$
\begin{aligned}
\F_t &= \sigma(B(s) : s \leq t)\\
&= \text{smallest } \sigma\text{-algebra containing } \{\omega: (B(s))(\omega) \in A\} \enspace \forall s \in [0,t], A: \text{Borel}\\
&= \text{smallest } \sigma\text{-algebra making } \forall B_s, s\in[0,t] \text{ measurable}
\end{aligned}
$$
\end{defn}


\begin{rem}
\leavevmode
\begin{enumerate}
\item $(B(t))_{t\geq0}$: Stochastic process adapted to the filtration $(\F_t)$.
\item $(B(t), \F(t))$: Martingale.
\end{enumerate}

\begin{lem}
$B(t) - B(s)$ is independent of $\F_s$ $(s < t)$.
\begin{proof}[Proof of lemma]
\leavevmode
\begin{enumerate}
\item $B(t) - B(s)$ is independent of $\sigma (B(s_1), B(s_2), \cdots, B(s_n))$ for $0 < s_1 < \cdots < s_n \leq s$, and check that  $\sigma (B(s_1), B(s_2), \cdots, B(s_n)) = \sigma(B(s_1), B(s_2)-B(s_1), \cdots, B(s_n)-B(s_{n-1}))$ 
\item Let $\mathcal{H} = \bigcup_{m=1}^\infty\bigcup_{0 < s_1 < \cdots < s_n \leq s} \sigma(B(s_1), \cdots, B(s_n))$.
Then, $\mathcal{H}$ is a closed under finite intersection.
i.e., $A_1, A_2, \cdots, A_n \in \mathcal{H} \Rightarrow A_1 \cap A_2 \cap \cdots \cap A_n \in \mathcal{H}$.
\item $B(t) - B(s)$ is independent of $\mathcal{H}$ by 1.
Then, $B(t) - B(s)$ is independent of $\overline{\mathcal{H}} = \F_t$ : smallest $\sigma$-algebra containing $\mathcal{H}$. (Midterm 1 problem 2)
\end{enumerate}
\end{proof}
\end{lem}

\begin{proof}
\begin{enumerate}
\item By construction
\item $s < t \Rightarrow
\E[B(t)|\F_s] = \E[B(t)-B(s)|\F_s] + \E[B(s)|\F_s] = \E[B(t) - B(s)] + B(s)$.
\end{enumerate}
\end{proof}
\end{rem}




\chapter{Constuction of Brownian Motion}

There are three ways to construct Brownian motion.
One is by Wiener, one is by Kolmogorov, and one is by Leby.
Wiener's method gives the existence of Brownian motion in natural way.
Kolmogorov's method gives the property of Brownian motion with sample path with awful $\omega$.
Levy's method gives an instruction for Brownian motion with wierd $\omega$.

\section{Wiener Space}
Let $C = C_0[0,1] = \{ f : f \text{ is continuous on } [0,1] \text{ and } f(0) = 0\}$.
We give a norm to $C$ by $\|f\| = \sup_{0 \leq x \leq 1} |f(x)| = \max_{0 \leq x \leq 1} |f(x)|$,
and distance $d(f,g) = \|f-g\|$.
Thus, there is an open ball $B_r(x) = \{ y : d(x,y) < r \}$ and topology(open set) of $C$.
Now, there is Borel $\sigma$-algebra = smallest $\sigma$-algebra containing all open sets.

\vspace{5mm}

\textbf{Notation}\\
From now, let $\mathcal{B}(C)$ be Borel $\sigma$-algebra in $C$.

\begin{defn}[Cylindrical Sets]
A cylindrical sets $\mathcal{R}$ is a collection of subsets of $C$ of the form
$$A = \left\{ f \in C : \left(f(t_1), f(t_2), \cdots, f(t_m)\right) \in U,
0 < t_1 < t_2 < \cdots < t_m \leq 1, U \in \mathcal{B}(\R^n)\right\},$$
and $A$ is called \textbf{cylindrical set}.
\end{defn}

\textit{cf.} For $m = 1$, $\{ f : f(t_1) \in U_k \} = A_k \in \mathcal{R}$,
then $\bigcup_{k=1}^\infty A_k = \{ f : f(t_1) \in \bigcup_{k=1}^\infty U_k\} \in \mathcal{R}$.

\begin{rem}
$\mathcal{R}$ is not a $\sigma$-algebra.
\end{rem}

\begin{exmp}
$\left\{ (f(t_1), f(t_2)) \in (-1,1) \times (2,3)\right\}
= \left\{ f : f(t_1) \in (-1,1), f(t_2) \in (2,3) \right\}$.
\end{exmp}



\begin{defn}
Let $\mu : \mathcal{R} \rightarrow [0,1]$ such that
$$
\mu(A) = \iint \cdots \int \frac{1}{\sqrt{2\pi t_1}} e^{-\frac{x_1^2}{2t_1}} \frac{1}{\sqrt{2\pi (t_2-t_1)}} e^{-\frac{(x_2-x_1)^2}{2(t_2-t_1)}} \cdots \frac{1}{\sqrt{2\pi (t_mt_{m-1})}} e^{-\frac{(x_m-x_{m-1})^2}{2(t_m - t_{m-1})}} dx_1 \cdots dx_m,
$$
and it is a natural definition since $B(t_i)$'s are jointly normal.
\end{defn}




\begin{thm}[Wiener]
$\mu$ is a countably additive($\sigma$-additive) function on $\mathcal{R}$.
In other words,
$A_1, A_2, \cdots$ are disjoint members of $\R$ and $\bigcup_{k=1}^\infty A_k \in \mathcal{R}$,
then $\mu(\bigcup_{k=1}^\infty A_k) = \sum_{k=1}^\infty \mu(A_k)$.
\end{thm}



\textbf{Summary}
\begin{itemize}
\item $C = C_0[0,1] \Rightarrow \mathcal{B}(C)$ is Borel $\sigma$-algebra
\item $\mathcal{R}$: collection of subsets of $C$
\item $\mu : \mathcal{R} \rightarrow [0,1]$ is $\sigma$-additive (Wiener)
\end{itemize}

\underline{Fact:} $\mathcal{R}$ is a \textbf{Ring} in the sense that
\begin{enumerate}
\item $\phi \in \mathcal{R}$
\item $A, B \in \mathcal{R} \Rightarrow A \cup B \in \mathcal{R}$
\item $A, B \in \mathcal{R} \Rightarrow A \backslash B \in \mathcal{R}$
\end{enumerate}

\begin{thm}[Caratheodory Extension Theorem]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $\mu : \mathcal{R} \rightarrow [0,1]$ is $\sigma$-additive
\item $\mathcal{R}$ is a ring
\item $\overline{\mathcal{R}}$ is a smallest $\sigma$-algebra containing $\mathcal{R}$.
\end{enumerate}
Then, there exists unique extension of $\mu$ to $\overline{\mathcal{R}}$ which is a measure.
\end{thm}


\begin{rem}
$\mu : \overline{\mathcal{R}}$ is a probability measure called Wiener measure.
\end{rem}
\begin{rem}
We can check that $\mathcal{B}(C) \subset \overline{\mathcal{R}}$. (it suffices to check that $B\in\overline{\mathcal{R}}$ for all open ball $B$), and $\mathcal{B}(C) = \overline{\mathcal{R}}$.
\end{rem}



\textbf{Conclusion:} $(C,\overline{\mathcal{R}}, \mu)$ is a probability space and says \textbf{Wiener space}.


\begin{thm}
$B: [0,1] \times C \rightarrow \R$, and let $B(t,\omega) = \omega(t)$.
Then, $B$ is a Brownian motion.

\begin{proof}
To prove that $B$ is a Brownian motion, we have to check
\begin{enumerate}
\item $B(0,\omega) = \omega(0) = 0$ (obvious $ \because \omega \in C = C_0[0,1])$.
\item $B(\cdot, \omega) = \omega(\cdot)$ is continuous (obvious by construction)
\item $s < t$: $B(t) - B(s) = N(0,t-s)$.
\item Independent increament i.e., $t_1 < \cdots < t_n \Rightarrow B(t_1), B(t_2) - B(t_1), \cdots, B(t_n) - B(t_{n-1})$ are independent.
\end{enumerate}

\begin{proof}[Proof of 3, 4]
$$
\begin{aligned}
&\mu\left(B(t_1) \in A, B(t_2 - B_1) \in A_2, \cdots, B(t_m) - B(t_{n-1}) \in A_m\right)\\
&= \mu \left[ \left\{\omega: (B(t_1))(\omega) \in A_1, \cdots, (B(t_m))(\omega) - (B(t_{m-1}))(\omega) \in A_m \right\}\right]\\
&= \mu\left[ \left\{ \omega : \omega(t_1) \in A_1, \omega(t_2) - \omega(t_1) \in A_2, \cdots, \omega(t_m) - \omega(t_{m-1}) \in A_m \right\}\right]\\
&= \mu\left[ A_{m,t_1,\cdots,t_m, U}\right | \text{By def } \mu] \quad U = \{(x_1, \cdots, x_m) \in \R^m : x_1 \in A_1, \cdots, x_m- x_{m-1} \in A_m\}\\
&= \iint\cdots\int \left[\prod_{i=1}^{m} \cdots \right] du_1 \cdots du_m\\
&= \iint\cdots\int \prod_{i=1}^m \frac{1}{\sqrt{2\pi(t_i-t_{i-1})}} e^{-\frac{y_i^2}{2(t_i-t_{i-1})}} dy_n\cdots dy_1\\
&= \prod_{i=1}^m \left[\int_{A_i} \frac{1}{\sqrt{2\pi(t_i-t_{i-1})}} e^{-\frac{y_i^2}{2(t_i-t_{i-1})}}dy_i\right].
\end{aligned}
$$
Therefore,
\begin{enumerate}[label = (\roman*)]
\item $A_1, A_3, \cdots, A_m = \R \Rightarrow \mu(B(t_2) - B(t_1) \in A_2) = \int_{A_2} \frac{1}{\sqrt{2\pi(t_2-t_{1})}} e^{-\frac{y_2^2}{2(t_2-t_1)}}dy_2$: Proof of 3.
\item By (i), $\mu(B(t_1) \in A_1, \cdots, B(t_m) - B(t_{m-1}) \in A_m ) = \mu(B(t_1) \in A_1) \cdots \mu(B(t_m) - B(t_{m-1}) \in A_m)$: Proof of 4.
\end{enumerate}
\end{proof}


\end{proof}
\end{thm}



\begin{rem}[Invariance Principle (Donsker, 1952)]
\leavevmode\\
In random walk, for $X_1, X_2, \cdots$ be i.i.d. and $\P(X_i = 1) = \P(X_1 = -1) = \frac{1}{2}$,
we defined $S_0 = X_1 \cdots + X_n$ as random walk, $W_n(t) = \frac{1}{\sqrt n}S_{nt}$ ($t = 0,\frac{1}{n},\cdots,\frac{n-1}{n},1$) as scaled random walk.\\
Define $\widehat{W_n}(t) = \frac{1}{\sqrt{n}}S_{[nt]} + \frac{1}{\sqrt n} (nt - [nt])(S_{[nt]+1} - S_{[nt]}) \in C[0,1]$.
Then, we can define probability measure $\mu_n$ in $(C,\mathcal{B}(c))$ by
$$\mu_n(A) := \P \left[ \widehat{W_n}(\cdot) \in A\right] \quad \forall A \in \mathcal{B}(C)$$
\end{rem}


\begin{thm}
$\widehat{W_n}(\cdot)$ converges to Brownian motion in the sense that $\mu_n \rightarrow \mu$ weakly.
\end{thm}

\begin{defn}[Weakly Convergence]
Let $\O$ be topological space, $\mathcal{B}(\O)$ be Borel $\sigma$-algebra.
For each $n \in \mathbb{N}$ let $\P_n, \P$ be probability measure in $(\O,\mathcal{B}(\O))$.
We say $\P_n \rightarrow \P$ \textbf{weakly} if $\int fd\P_n \rightarrow \int fd\P$ for every continuous and bounded function $f : \O \rightarrow \R$.
\end{defn}


\begin{rem}
We defined a Brownian motion $B$ in $B : [0,1] \times C \rightarrow \R$.
To extend to $[0,\infty) \times C \rightarrow \R$, define
$B(t) =
\begin{cases}
B_1(t) & t \in [0,1]\\
B_1(1) + B_2(t-1) & t \in [1,2]\\
B_1(1) + B_2(1)  + B_3(t-2)& t \in [2,3]\\
\cdots
\end{cases}
$
\end{rem}

\section{Borel-Cantelli Lemma and Chebyshev Inequality}
To Be Later...

\section{Kolmogorov's Extension and Continuity Theorems}

\begin{itemize}
\item $\O = \R^{[0,\infty)} = \{ f : \text{ s.t. } f : [0,\infty) \rightarrow \R, f(0) = 0\}$
\item $\mathcal{R} = \{ A_{m, t_1, t_2, \cdots, t_m, U} : m \in \mathbb{N}, 0 \leq t_1 < \cdots < t_m, U \in B(\R^m)\}$
\item $\F = \overline{\mathcal{R}}$ (Cylindrical $\sigma$-algebra)
\item $\mu_{t_1,t_2,\cdots,t_m}$: probability measure in $\mathcal{B}(\R^m)$ by
$$\mu_{t_1,t_2,\cdots,t_m}(A) = \iint \cdots \int_A \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi(t_i-t_{i-1})}} e^{-\frac{(u_i-u_{i-1})^2}{2(t_i-t_{i-1})}} du_1\cdots du_m$$
\end{itemize}
(Note: In section 3.1 we define $0 < t_1 < t_2 < \cdots$ since Brownian motion is fixed at $0$, but in this section we define $0 \leq t_1 < t_2 < \cdots$.)

\begin{defn}[Marginal distribution of $0\leq t_1 < \cdots < t_n$]
$\mu_{t_1,t_2,\cdots,t_n} = \P((X(t_1), \cdots, X(t_n)) \in U)$ $(:= \P[\{\omega: (X(t_1,\omega),\cdots,X(t_n,\omega))\in U\}])$is called \textbf{marginal distribution of $0 \leq t_1 < \cdots < t_n$}.
\end{defn}

Question: Can we construct a probability measure $\P$ on $(\O,\F)$ such that
$$\P[A_{n,t_1,\cdots,t_n,U}]=\mu_{t_1,\cdots,t_n}(U)\text{ ?}$$
If it is possible, then
$Y : [0,\infty)\times \O \rightarrow \R$
has marginal $\mu_{t_1,\cdots,t_n}$.

(Why?)
$$
\begin{aligned}
&\P[\{\omega: (Y(t_1,\omega), Y(t_2,\omega),\cdots,Y(t_n,\omega) )\in U\}]\\
&= \P[\{\omega : (\omega(t_1), \cdots, \omega(t_n)) \in U\}]\\
&= \P[A_{n,t_1,\cdots,t_n,U}] = \mu_{t_1,\cdots,t_n}(U)
\end{aligned}
$$


% 상황
% 1. (X(t))_{t\geq0}를 정의하고 식을
% 2. (X(t_1),\cdots,X(t_n))의 분포는 어떻게 되어야 하는지 알고 있음
% => We have to construct \P in \mathbb{Q}
% Brownian motion뿐만 아니라 모든 stochastic process를 construct하는 black box가 된다!


\underline{Observation} (Consistency condition)
$$
\begin{aligned}
&\mu_{t_1,\cdots,t_n}: \text{marginal distribution of } X(t,\omega)\\
&\Rightarrow \mu_{t_1,\cdots,t_n}(A_1\times\cdots\times A_{i-1}\times\R\times A_{i+1}\times\cdots\times A_n)\\
&=\mu_{t_1,\cdots,t_{i-1},t_{i+1},\cdots,t_n}(A_1\times\cdots\times A_{i-1}\times A_{i+1}\times\cdots\times A_n)\\
\end{aligned}
$$

\begin{thm}[Kolmogorov's Extension Theorem]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $\mu_{t_1,\cdots,t_n}$: Borel probability measure in $\R^2$
\item $\{ \mu_{t_1},\cdots,t_n: 0\leq t_1<\cdots<t_n\}$ satisfies the consistency condition
\end{enumerate}
Then, $\exists \P$ on $(\O,\F)$ such that
$$\P[A_{n,t_1,\cdots,t_n,U}]=\mu_{t_1,\cdots,t_n}(U)$$
\end{thm}

Kolmogorov's method is easy to construct Brownian motion with definition 1,3,4, but it is hard to probe definition 2: $B(\cdot,\omega)$ is continuous a.s.

Kolmogorov's extension theorem can be applied to every stochastic process.

% Question. $\P[\{\omega : \omega \in C[0,\infty)\}] = 1?$ (Even it is not in $\F$)


\begin{exmp}
$0\leq t_1 < t_2 < \cdots t_n$.
$$\mu_{t_1,t_2,\cdots,t_n}(U) =
\begin{cases}
\iint \cdots \int_U \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi(t_i-t_{i-1})}} e^{-\frac{(u_i-u_{i-1})^2}{2(t_i-t_{i-1})}} du_1\cdots du_n & \text{if } t_1 = 0\\
\iint \cdots \int_U \prod_{i=2}^{n} \frac{1}{\sqrt{2\pi(t_i-t_{i-1})}} e^{-\frac{(u_i-u_{i-1})^2}{2(t_i-t_{i-1})}} \delta_0(u_1)du_1\cdots du_n & \text{if } t_1 \neq 0,
\end{cases}
$$
and it is easy to show that $\mu_{t_1,\cdots,t_n}$ satisfies consistency condition.
By Kolmogorov's extension theorem, we have $(\O,\F,\P)$ associates to the above.
Let $B : [0,t] \times \O \ni (t,\omega) \mapsto \omega(t) \in \R$ (coordinate mapping process).

Question: Is $B$ the Brownian motion? We have to check
\begin{enumerate}
\item $B(0) = 0$
\item $B(\cdots, \omega)$ is continuous a.s.
\item $B(t)-B(s): N(0,t-s)$
\item Independent increament,
\end{enumerate}
and 1, 3, 4 are easy.
To prove 2, we need the following theorem
\end{exmp}

\begin{thm}[Kolmogorov's Continuity Theorem]
$X: [0,1]\times\O\rightarrow\R$ is a stochastic process satisfying KET.
We say $(X(t))$ is separable if
\begin{enumerate}
\item $\forall t \in [0,1], \epsilon > 0 \enspace \lim_{s\rightarrow t} \P[|X(s)-X(t)| > \epsilon]=0$
\item $\exists \alpha,\beta,C > 0$ such that $\E[|X(t)-X(s)|^{1+\alpha}] \leq C|t-s|^\beta$ for all $s,t\in[0,1]$.
\end{enumerate}
Then, there exists stochastic process $\widetilde{X}(t,\omega)$ such that
\begin{enumerate}
\item $\widetilde{X}(\cdot,\omega)$ is continuous $\forall \omega \in \O$
\item $\O_0 = \{ \omega : X(t,\omega) = \widetilde{X}(t,\omega) \enspace \forall t\in[0,1] \enspace, \P(\O)=1$
\end{enumerate}
\end{thm}


Return to example, by KCT, there exists $\widetilde{B}(t,\omega)$ such that $\widetilde{B}$ satisfies condition 2 of Brownian motion.
Condition 3 and 4 are obvious, and condition 1 is also true since $\widetilde{B}(0,\omega) = 0$ on $\O_0$, hence a.s.



We will skip the proof of KET and KCT (see Google)


Note:
Kolmogorov's method is more general than Wiener's method since Kolmogorov's mtehod can be applied to any stochastic process.
However, the space in Wiener's method (Banach space) and Kolmogorov's method are different, hence Kolmogorov's construction does not contain Wiener's construction.





\section{Levy's Interpolation Method}

Let $D_n = \{\frac{k}{2^{n-1}} : k = 0,1,\cdots,2^{n-1}\}$.
We will construct $X_1(t), X_2(t), \cdots, X_n(t), \cdots$ such that $X_n(t) \rightarrow$ BM.

\begin{defn}
Let $\phi_1,\cdots$ be i.i.d of $N(0,1)$. Define $X_n(0) = 0, X_1(1) = \phi_1$ and
$$
X_{n+1}(t) =
\begin{cases}
    X_n(t) & \text{if } t \in D_n\\
    X_n(t) + \frac{1}{2^{\frac{n+1}{2}}} \phi_i & \text{if } t \in D_{n+1} - D_n,
\end{cases}
$$
and interpolate by the line.
Then, for each $t \in [0,1], (X_n(t))_{n=1}^\infty$ is a Cauchy sequence in $L^2(\O) = \{ X: \O \rightarrow \R ; \int X^2 d\P < \infty \}$ with distance $d(X_m(t), X_n(t)) = \sqrt{\E[|X_m(t) - X_n(t)|^2]}$.
Since $L^2(\O)$ is complete, $X_n(t) \rightarrow X(t)$ for some random variable $X(t)$.
\end{defn}








\setcounter{chapter}{2}
\setcounter{section}{2}
\section{Wiener Integral}


\textbf{Review}: $L^2$ space\\
$(\O,\F,\P) = ([a,b], \mathcal{B}([a,b]), m)$.
\begin{itemize}
\item $L^2(\O) := \{ X : \text{ random variable in } \O, \int X^2 d\P < \infty \}$
\item $L^2([a,b]) := \{ f : [a,b] \rightarrow \mathbb{R}, \int f^2 dx < \infty \}$
\end{itemize}

\begin{enumerate}[wide]
\item Convergence in $L^2$
\begin{itemize}
\item $(X_n)_{n=1}^\infty \in L^2(\O); X_n \rightarrow X$ in $L^2(\O)$ if $\int|X_n-X|^2 d\P \rightarrow 0$
\item $(f_n)_{n=1}^\infty \in L^2([a,b]); f_n \rightarrow f$ in $L^2([a,b])$ if $\int|f_n-f|^2 dx \rightarrow 0$
\item $d(X,Y) = \sqrt{\int |X-Y|^2 d\P}$: distance $\Rightarrow$ metric space
\end{itemize}

\item $L^2$: complete:
\begin{itemize}
\item $(X_n)_{n=1}^\infty$: Cauchy in $L^2(\O) \Longrightarrow \exists X \in L^2(\O)$ such that $X_n \rightarrow X$ in $L^2(\O)$
\item $(f_n)_{n=1}^\infty$: Cauchy in $L^2([a,b]) \Longrightarrow \exists f \in L^2([a,b])$ such that $f_n \rightarrow f$ in $L^2([a,b])$
\end{itemize}

\item Simple functions are dense in $L^2(\O)$
\begin{itemize}
\item $X \in L^2(\O) \Rightarrow \exists$ simple functions $(X_n)_{n=1}^\infty$ such that $X_n \rightarrow X$ in $L^2(\O)$
\item $f \in L^2([a,b]) \Rightarrow \exists$ simple functions $(f_n)_{n=1}^\infty$ such that $f_n \rightarrow f$ in $L^2([a,b])$
\end{itemize}

\item $X_n \rightarrow X$ in $L^2(\O)$ $\cdots (\bigstar)$
\begin{itemize}
\item $X_n \rightarrow X$ a.s. if $\P\{\omega:\lim_{n\rightarrow\infty} X_n(\omega) = X(\omega)\} = 1$ $\cdots 	(\spadesuit)$
\end{itemize}
$(\bigstar) \not\Rightarrow (\spadesuit)$ and $(\spadesuit) \not\Rightarrow (\bigstar)$.
But if $(\bigstar)$ there exists a subsequence such that $(\spadesuit)$.
\end{enumerate}


\textit{Question 1.} What is $\int_a^b f(t) dB(t)$?\\
\textit{Question 2.} What is $\int_a^b f(B(t)) dB(t)$?

\vspace{5mm}

\textbf{Review}: Stieltjes Integral


Let $a = t_0 < t_1 < \cdots < t_n = b$, $t_i = a + \frac{b-a}{n}i$.\\
$\lim_{n\rightarrow}\sum_{k=1}^n f(t_k)(g(t_k)-g(t_{k-1})) =: \int_a^b fdg$ if exists.

\begin{rem}
\leavevmode
\begin{enumerate}
\item $g \in C^1 \Rightarrow \int_a^b fdg = \int_a^b fg'dx$
\item Well-defined if $g$ is absolutely continuous
\item (Summation by parts) $\sum_{k=1}^n f(t_k)(g(t_k)-g(t_{k-1})) = \sum_{k=0}^{n-1} g(t_k)(f(t_k) - f(t_{k+1})) + f(b)g(b) - f(a)g(a)$.
If $f$ is absolutely continuous, then it converges to $-\int gdf + f(b)g(b) - f(a)g(a)$.
Therefore, if $f$ is absolutely continuous, then $\int fdg = f(b)g(b) - f(a)g(a) - \int gdf$
\end{enumerate}
\end{rem}

\begin{exmp}[Example to Questions]
\leavevmode
\begin{itemize}
\item If $f$ is absolutely continous, then $\int_0^1 fdf = \int_0^1 ff'dx = \frac{f(1)^2 - f(0)^2}{2}$
\item However, $\int_0^1 B(t) dB(t) = \frac{B(1)^2 - B(0)^2}{2} - \frac{1}{2}$ (by quadratic variation)
\end{itemize}
\end{exmp}


Consider a random variable $B(t) : \omega \mapsto B(t,\omega)$.
Then, $$ \int_a^b f(t)dB(t) : \Omega \ni \omega \mapsto \int_a^b f(t)dB(t,\omega) \in \mathbb{R} $$
is well-defined if $f$ is absolutely continuous.
However, what if $f \in L^2([a,b])$?

\begin{enumerate}[label=\textbf{Step \arabic*}]
\item $f$ is simple, $f(x) = \sum_{i=1}^k \alpha_i \1_{[a_i,b_i]}(x) \in L^2([a,b])$\\
Let $I(f) = \sum_{i=1}^k \alpha_i(B(b_i) - B(a_i)) \in L^2(\O)$. W.L.O.G. let $[a_i,b_i]$ be disjoint. Note that
\begin{enumerate}
\item $\int \1_{[a_i,b_i]}dB(t) = \int_{a_i}^{b_i} dB(t) = B(b_i) - B(a_i)$, hence $I(f) = \int_a^b fdB(t)$
\item $I(f) : N(0, \int_a^b f^2 dx)$, since $\E[I(f)^2] = \sum \alpha_i^2 (b_i-a_i) = \int_a^b f^2 dx$
\item $I$ is linear map. i.e. $f, g$ are simple $\Rightarrow I(f+g) = I(f) + I(g)$ and $I(cf) = cI(f)$
\end{enumerate}

\item Recall (3: simple functions are dense in $L^2$)
\begin{itemize}[wide]
\item $f \in L^2([a,b]) \Rightarrow \exists$ simple functions $(f_n)_{n=1}^\infty$ such that $f_n \rightarrow f$ in $L^2$.
\item If $I(f_m), I(f_n) \in L^2(\O)$, then
$$
\int_a^b |I(f_m) - I(f_n)|^2 d\P = \int I(f_m-f_n)^2 d\P = \E[I(f_m-f_n)^2] = \int_a^b (f_m-f_n)^2 dx.
$$
Therefore $(f_n)$ is a Cauchy sequence in $L^2([a,b]) \Rightarrow (I(f_n))_{n=1}^\infty$ is a Cauchy sequence in $L^2(\O)$.
Let $Z \in L^2(\O)$ be $I(f_n) \rightarrow Z$, and define $I(f) := Z$.
\end{itemize}
\end{enumerate}


\textbf{Exercise:}  Well-definedness\\
i.e., $f_n, g_n \rightarrow f$, $I(f_n) \rightarrow Z, I(g_n) \rightarrow Z'$, then $Z = Z'$ a.s.?

\textbf{Exercise:} $(Z_n: N(\mu_n, \sigma_n^2), Z_n \rightarrow Z$ in $L^2$)
Then, 1) $\mu_n \rightarrow mu, \sigma_n \rightarrow sigam$, $Z : N (\mu, \sigma^2)$.

\begin{thm}
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $I(f) : N(0, \int_a^b f^2dx) \quad (f \in L^2([a,b])$
\item $\E[I(f)I(g)] = \int_a^b fgdx \quad (f,g \in L^2([a,b])$
\item If $f$ is absolutely continous (or $f \in C^1([a,b])$), then $I(f) = \int_a^b f(t)dB(t) (= f(b)B(b) - f(a)B(a) - \int_a^b B(t)df)$
\end{enumerate}
\end{thm}


\begin{defn}[$\int_a^b f(t)dB(t)$]
Let $f \in L^2([a,b])$. $\int_a^b f(t)dB(t) := I(f)\in L^2(\O)$. Then,
\begin{itemize}
\item $I(f) : \omega \mapsto \int_a^b f(t)dB(t,\omega)$
\item $I: L^2([a,b]) \rightarrow L^2(\O)$ isometry
\end{itemize}
\end{defn}

\begin{proof}[Proof of thm]
\leavevmode
\begin{enumerate}
\item
Let $f_n \rightarrow f$ in $L^2([a,b])$ ($f_n: simple)$ and $I(f_n) \rightarrow I(f)$ in $L^2(\O)$.
Since $I(f_n) \sim N(0, \int_a^b f_n^2 dx)$, it converges to $N(0,\int_a^b f^2(x)dx)$.
Therefore, $\int_a^b f_n^2dx \rightarrow \int_a^b f^2dx$.

\item By 1. $\E[I(f)^2] = \int_a^b f^2dx$. Moreover,
$\E[I(f+g)^2] = \int_a^b (f+g)^2 dx$,
$\E[I(f)^2] = \int_a^b f^2 dx$,
$\E[I(g)^2] = \int_a^b g^2 dx$.
Check that $I(f+g) = I(f)+I(g)$.

\item Let $a = t_0 < t_1 < \cdots < t_n = b$ and $t_i = a + \frac{b-a}{n}i$.
Let $f_n(t) = \sum_{i=1}^n \1_{[t_{i-1}, t_i)}(t) f(t_{i-1})$.
Then, $f_n \rightarrow f$ in $L^2$ if $f_n$'s are absolutely continuous.
Therefore, $I(f) = \lim_{n\rightarrow\infty} I(f_n) = \lim_{n\rightarrow\infty} \sum_{k=1}^n f(t_{i-1})(B(t_i)-B(t_{i-1}))$ ( = Definition of Stieltjes integral $\int_a^b dB(t)$)
\end{enumerate}
\end{proof}


\begin{exmp}
\leavevmode
\begin{enumerate}
\item $\int_0^1 B(t)dt$?\\
Let $f(x) = x-1, df(x) = dx$. Then,
$$
\begin{aligned}
\int_0^1 B(t)dt &= B(t)df(t)\\
&= B(1)f(1) - B(0)f(0) - \int_0^1 f(t)dB(t)\\
&= -\int_0^1 f(t)dB(t)\\
&= \int_0^1 (1-t)dB(t)\\
&\Rightarrow N(0, \int_0^1 (1-t)^2 dt)
= N(0,\frac{1}{3})
\end{aligned}
$$

\item
By theorem 1 of 2.3.1,
$X = \int_0^1 tdB(t) = N(0,\frac{1}{3})$ and
$Y = \int_0^1 t^2dB(t) = N(0,\frac{1}{5})$.
Moreover, by theorem 2 of 2.3.1, $Cov(X,Y) = \E[XY] - \E[X]\E[Y]
= \int_0^1 tt^2 dt = \frac{1}{4}$.

\end{enumerate}
\end{exmp}



\textbf{Recall} (Martingale)\\
$(M_t, \F_t)_{t\geq 0}$ is a stochastic process in $(\O,\F,\P)$ with
\begin{enumerate}
\item
$\F_t$: filtration; $\F_s \subset \F_t$ if $s \leq t$;
$M_t$: $\F_t$-measurable (adapted) $\forall t$.

\item
$\E[|M_t|] < \infty$

\item
$\E[M_t|\F_s] = M_s \quad \forall s < t$.
\end{enumerate}

To show that Wiener integral is Martingale, let
$M_t = \int_0^t f(u)dB(u)$, and
$\F_t = \sigma(B_u: u \in [0,s]$.
Then, $\E[M_t | \F_s]
= \E[M_t - M_s | \F_s] + \E[M_s | \F_s]
= \E[\int_s^t f(u)dB(u)|\F_s] + M_s
= M_s$?

\begin{thm}
$(M_t,\F_t)$ is Martingale if $f \in L^2([0,\infty)$.
\begin{proof}
\begin{enumerate}[wide]
\item
By constuction

\item
$\E[M_t^2] = \int_0^t f(u)^2 du < \infty
\Rightarrow \E[|M_t|] \leq \E[M_t^2]^{1/2} < \infty$ by Jensen's inequality.

\item WTS: $\E[\int_s^t f(u)dB(u) | \F_s] = 0$.
\begin{proof}
Let $f \in L^2[s,t]$.
Then, $\exists$ simple functions $f_n \in L^2([s,t])$ such that
$f_n \rightarrow f$ in $L^2([s,t])$. Then,
$$
\begin{aligned}
f_n(x) &= \sum_{k=1}^m \alpha_k \1_{[t_k-1, t_k)}(x)\\
&\Rightarrow \int_s^t f_n(u)dB(u) = \sum_{k=1}^m \alpha_k (B(t_k)-B(t_{k-1}))\\
&\Rightarrow \E[\int_s^t f_n(u)dB(u) | \F_s] = 0
\end{aligned}
$$
since $\E[B(t_i) - B(t_{i-1}) | \F_s] = 0.
$
Now,
$$
\begin{aligned}
X^2 &:= \E\left[\int_0^t f(u)dB(u) | \F_s\right]^2\\
&= \E\left[\int_0^t (f(u) - f_n(u))dB(u) | \F_s\right]^2\\
&\leq \E\left[\left(\int_s^t (f(u) - f_n(u))\right)^2 | \F_s\right]
\end{aligned}
$$
Next, we take $\E$ both sides
$$
\begin{aligned}
\E[X^2] &\leq \E\left[\E\left[\left(\int_s^t (f(u) - f_n(u)) dB(u) \right)^2 | \F_s \right]\right]\\
&= \E\left[\left(\int_s^t (f(u) - f_n(u) dB(u)\right)^2\right]\\
&= \int_s^t \left(f(u) - f_n(u)\right)^2 \rightarrow 0
\end{aligned}
$$
\end{proof}
\end{enumerate}
\end{proof}
\end{thm}


A key idea is that $\int_0^t f(u) dB(u) = M_t$ is a Martingale.



\setcounter{chapter}{3}
\chapter{Stochastic Integrals}
\section{Background and Motivation}

\textbf{Motivation}
\begin{itemize}
\item What is $\int_a^b f(B(u)dB(u))$?
\item What is $\int_a^b f(u,B(u))dB(u)$?
\end{itemize}

\begin{exmp}
$\int_0^T B(t)dB(t) = \lim_{n\rightarrow\infty} \sum_{i=1}^n B(s_i) \left[B(t_i) - B(t_{i-1})\right] = $?\\
Let $L_n = \sum_{i=1}^n B(t_{i-1})(B(t_{i}) - B(t_{i-1}))$, and
$R_n = \sum_{i=1}^n B(t_i) (B(t_i) - B(t_{i-1}))$.
Then,
\begin{enumerate}%[wide]
\item $L_n + R_n = \sum_{i=1}^n (B(t_i)^2 - B(t_{i-1})^2) = B(t_n)^2 - B(t_0)^2 = B(T)^2$.
\item $R_n - L_n = \sum_{i=1}^n [(B(t_i) - B(t_{i-1}))^2 - (t_i-t_{i-1})] + \sum_{i=1}^n (t_i - t_{i-1}) = (\sum_{i=1}^n X_i) + T$.
Note that $\E[(\sum_{i=1}^n X_i)^2] = \sum_{i=1}^n \E[X_i^2] + 2\sum_{i<j} \E[X_iX_j]$,
\begin{enumerate}[wide]
\item
$i < j \Rightarrow \E[X_iX_j] = \E[B(t_i)-B(t_{i-1}), B(t_j) - B(t_{j-1}) ] = \E[X_i]\E[X_j] = 0$

\item
$\E[X_i]^2 = \E\left[ (B(t_i)-B(t_{i-1}))^2 - (t_i - t_{i-1})^2 \right] = 2(t_i-t_{i-1})^2 = \frac{T^2}{n^2}$ (exercise).
\end{enumerate}
Therefore, $\E((\sum_{i=1}^n X_i)^2) = \frac{T^2}{n}$, and
$\sum_{i=1}^n X_i \rightarrow 0$ in $L^2(\O) .$%\Rightarrow R_n - L_n \rightarrow T$ in $L^2(\O)$.
\end{enumerate}
Therefore, $R_n + L_n \rightarrow B(T)^2$ and $R_n - L_n \rightarrow T$\\
$\Rightarrow R_n \rightarrow \frac{B(T)^2 + T}{2}, L_n \rightarrow \frac{B(T)^2 - T}{2}$.

\begin{rem}
In Riemann integral, $\lim_{n\rightarrow\infty} L_n = \lim_{n\rightarrow\infty} R_n$.
\end{rem}

\begin{rem}
$L_n$ is called \textbf{It\^o Integral}, and
$M_n = \frac{L_n + R_n}{2}$ is called \textbf{Stratonovich Integral}
\end{rem}
\begin{rem}
We use $L_n$ in many cases since $\E[L_n] = 0$ and $M_t = \int_0^t B(s) dB(s)$ is Martingale in the sense of It\^o.
\end{rem}
\end{exmp}

\begin{defn}[It\^o Integral]
$
\int_a^b f(t,B(t))dB(t) \rightarrow
\lim_{n\rightarrow\infty}\sum_{i=1}^n f(t_{i-1}, B(t_{i-1}))(B(t_i) - B(t_{i-1}))
$
\end{defn}

\vspace{5mm}

\textbf{Review}\\
We'll check the facts that the textbook assumes but not proves.

$f \in L^2([a,b]) \rightarrow I(f) := \int_a^b f(s)dB(s) \in L^2(\O)$ and we've showed that
\begin{enumerate}
\item
$I(f) \sim N(0,\int_a^b f^2(t)dt)$

\item
$\E[I(f)I(g)] = \int_a^b f(t)g(t)Dt$

% \item
\end{enumerate}
\begin{rem}
$I(f), I(g)$ are jointly normal.
\end{rem}

Let $X$ be random variable on $(\O,\F,\P)$, and let $\phi_X(t) = \E[e^{itX}] = \int_\O e^{itX(\omega)}d\P(\omega)$.


\begin{exmp}
Let $X \sim N(\mu, \sigma^2)$. Then,
$$
\phi_X(t) = \int e^{itx} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} = e^{it\mu - \frac{1}{2}t^2\sigma^2}dx.
$$
Conversely, $\phi_X(t) = e^{it\mu - \frac{1}{2}t^2\sigma^2} \ \forall t$, then $X \sim N(\mu,\sigma^2)$.
\begin{proof}[Sketch of proof]
Let $f_X$ be a pdf of $X$.
Then, $\int_{-\infty}^\infty e^{itx}f_X(x)dx = \phi_X(t)$, and by inverse Fourier transform, $f_X(x) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{-itx}\phi_X(t)dt$.
\end{proof}
\end{exmp}

\begin{exmp}
Let $\vec{X} = (X_1, X_2, \cdots, X_n)$ be jointly normal with mean $\vec{\mu} = (\mu_1,\cdots,\mu_n)$ and variance $\Sigma = (\Sigma_{ij})_{i,j=1}^n$.
Then,
$$
f_{\vec{X}}(\vec{x}) = \frac{1}{(2\pi)^n\det \Sigma} e^{-\frac{1}{2} (\vec{x}-\vec{\mu})\Sigma^{-1}(\vec{x}-\vec{\mu})^\mathsf{T}},
$$
$$
\phi_{\vec{X}}(\vec{t}) = \E\left[e^{i(\vec{X}\cdot\vec{t})}\right] = e^{i\vec{\mu}\vec{t} - \frac{1}{2}\vec{t} \Sigma \vec{t}^\mathsf{T}}.
$$
Conversely,
$$
\phi_{\vec{X}}(\vec{t}) = \E\left[e^{i(\vec{X}\cdot\vec{t})}\right] = e^{i\vec{\mu}\vec{t} - \frac{1}{2}\vec{t} \Sigma \vec{t}^\mathsf{T}},
$$
then $\vec{X} \sim N(\vec{\mu}, \Sigma)$.
\end{exmp}

\begin{thm}
$$(I(f), I(g)) \sim N\left((0,0), 
\begin{bmatrix}
\int_a^b f(t)^2 dt & \int_a^b f(t)g(t)dt\\
\int_a^b f(t)g(t)dt & \int_a^b g(t)^2 dt  \\
\end{bmatrix}
\right)
$$
\begin{proof}
Consider $uI(f) + vI(g) = I(uf+vg) \sim N\left(0, \int_a^b (uf(t)+vg(t))^2dt\right)$.
Then,
$$
\begin{aligned}
\E\left[e^{i(uI(f)+vI(g))}\right]
&= \exp\left[-\frac{1}{2}\left(  u^2\int_a^b f^2 dt + v^2 \int_a^b g^2 dt + 2uv \int_a^b fgdt \right)   \right]\\
&= \exp\left[ -\frac{1}{2} (u,v) \Sigma {u \choose v} \right].
\end{aligned}
$$
Therefore, $(I(f), I(g)) \sim N(\vec{0}, \Sigma)$.
\end{proof}
\end{thm}

\begin{rem}
The same holds for $n$ functions:
$$f_1, \cdots, f_n \in L^2([a,b]) \Rightarrow \left( I(f_1), \cdots, I(f_n)\right) \sim N(\vec{0}, \Sigma), \quad \Sigma_{ij} = \int_a^b f_i(t)f_j(t) dt.$$
\end{rem}


Let $f\in L^2([0,t])$ for all $t \geq 0$. (\textit{cf}. $f(t) = t \not\in L^2([0,\infty])$).
Set $X(t) = \int_0^t f(s)dB(S)$ ($X(t) : \omega \mapsto \int_0^t f(s)dB(s,\omega)$).
We've proved that $X$ is a Martingale.

\begin{thm}
$X(t)$ is independent increament.
\begin{proof}
Let $s < t$ and we will show that $X(s), X(t) - X(s)$ are independent.
(proof for $X(t_1), X(t_2) - X(t_1), \cdots, X(t_m) - X(t_{m-1})$ is identical and exercise)

$$
\begin{aligned}
X(s) &= \int_0^s f(u) dB(u) = \int_0^t g(u)dB(u), \quad
&g(u) = 
\begin{cases}
    f(u) & u \leq s\\
    0 & u > s
\end{cases}
\\
X(t) - X(s) &= \int_s^t f(u) dB(u) = \int_0^t h(u)dB(u), \quad
&h(u) = 
\begin{cases}
0 & u \leq s\\
f(u) & u > s
\end{cases}
\end{aligned}
$$
In other words, $X(s) = I(g), X(t) - X(s) = I(h)$ (jointly normal).
Therefore, $\E[I(g)I(h)] = \int_0^t g(u)h(u)du = 0$.
Since  $I(g), I(h)$ are jointly normal and uncorrelated (supports of $g, h$ are disjoint), they are independent.

\end{proof}
\end{thm}


\begin{exmp}
$\E[B(1)^2B(2)^2] = $ ?

Method 1.
using $(B(1), B(2)) \sim N\left((0,0), 
\begin{pmatrix}
1&1\\1&2
\end{pmatrix}
\right)
$, calculate
$$
\int_{-\infty}^\infty \int_{-\infty}^\infty
x^2y^2 \frac{1}{2\pi} e^{-\frac{1}{2} \left(2x^2 - 2xy + y^2\right)}dxdy.
$$

Method 2.
$B(t) - B(s)$ are independent of $\F_s$ and $\F_u$ for all $(u \leq s)$.
In other words, give $\F_s$, $B(t) - B(s) \sim N(0,t-s)$.
Therefore,
$$
\begin{aligned}
\E[B(1)^2B(2)^2]
&= \E[\E[B(1)^2B(2)^2 | \F_1]]\\
&= \E[B(1)^2 \E[B(2)^2 | \F_1]]\\
&= \E\left[B(1)^2\E[(B(2) - B(1))^2 + 2(B(2) - B(1))B(1) + B(1)^2 | \F_1]\right]\\
&= \E\left[B(1)^2\left(\E[(B(2) - B(1))^2|\F_1] + 2B(1)\E[(B(2) - B(1))] + B(1)^2 \right)\right]\\
&= \E\left[B(1)^2\left( 1 + B(1)^2 \right)\right]\\
&= 1 + 3 = 4
\end{aligned}
$$
(Fact: $\E[X^{2n}] = (2n-1)!!$ for standard normal random variable $X$)
\end{exmp}

\begin{exmp}
Let $X(t) = \int_0^t (a+b\frac{u}{t}) dB(u)$.
Then, for which $a,b$, $X(t)$ be Brownian motion?

To say that $X$ is a Brownian motion, we have to show that
\begin{enumerate}
\item $X(t) - X(s) \sim N(0,t-s)$
\item Independent increament
\item $X(0) = 0$
\item continuity
\end{enumerate}

\begin{enumerate}[wide]
\item Suppose $s < t$. Then,
$$
\begin{aligned}
X(t) - X(s)
&= \int_0^t (a + b\frac{u}{t}) dB(u) - \int_0^s (a+b\frac{u}{s}) dB(u)\\
&= \int_0^s bu(\frac{1}{t} - \frac{1}{s}) dB(u) + \int_s^t (a + b\frac{u}{t})dB(u)\\
\Rightarrow \E[(X(t)-X(s))^2] &= \int_0^s (bu)^2(\frac{1}{t}-\frac{1}{s})^2 du + \int_s^t (a+b\frac{u}{t})^2 du\\
&= \frac{b^3}{3}(\frac{1}{t}-\frac{1}{s})^2 + \frac{1}{3}\frac{t}{b}\left((a+b)^3 - (a+b\frac{s}{t})^3\right)\\
&= \frac{b^2}{3}\frac{s^3}{t^2} - \frac{2b^2}{3}\frac{s^2}{t} + \frac{b^2}{3}s + \frac{t}{3b}\left(3a^2b(1-\frac{s}{t}) + 3ab^2(1-\frac{s^2}{t^2}) - \frac{s^3}{t^3}b^3 + b^3\right)\\
&= (\frac{b^2}{3} - a^2)s + t(a^2 + 3ab + \frac{b^2}{3}) \quad \left(-\frac{2b^2}{3} - ab = 0\right)
\end{aligned}
$$
Therefore, $\frac{b^2}{3}-a^2 = -1, a^2 + ab + \frac{b^2}{3} = 1 \Rightarrow (a,b) = (1,0), (2,-3)$

\item Suppose $t_0 = 0 < t_1 < t_2 < \cdots < t_m$. Then,
$X(t_1) - X(t_0), X(t_2) - X(t_1), \cdots, X(t_m) - X(t_{m-1})$ are jointly normal ($\because X(t_{i+1})- X(t_i) = I(f_i)$ for some $f_i$).
Moreover, $\E[I(f_i)I(f_j)] = 0$ for $(a,b) = (1,0), (2,-3)$.
Therefore, they are independent.
\end{enumerate}
\end{exmp}

% 여기부터 기말
% It\^o integral and It\^o' Formula (2 weeks)
% SDE (3 weeks)

\section{Filtration for Brownian Motion}
Let $(X_t, \F_t)$ be stochastic process and filtration.
\begin{pro}
If $X_t - X_s$ is independent of $\F_s \ \forall s < t$, then $(X_t)$ is independent increament.
\begin{proof}
$Z_1, Z_2, \cdots Z_n$ are independent if and only if (by inverse transform)
$$
\E\left[\prod_{k=1}^n e^{i\theta_k Z_k}\right] = \prod_{i=1}^n \E\left[e^{i\theta_k Z_k}\right] \quad \forall \theta_1 \sim \theta_n \in \mathbb{R}.
$$
Claim: $X(s) - X(t) - X(s)$ are independent.

$$
\begin{aligned}
\E\left[e^{i\theta_1 X(s) + i\theta_2 (X(t) - X(s))}\right]
&= \E \left[ \E\left[e^{i\theta_1 X(s) + i\theta_2 (X(t) - X(s))}\right] | \F_s \right]\\
&= \E\left[ e^{i\theta_1 X(s)} \E\left[ e^{i\theta_2 (X(t)-X(s))} | \F_s \right] \right]\\
&= \E\left[ e^{i\theta_1 X(s)} \right] \E\left[ e^{i\theta_2 (X(t)-X(s))} \right].
\end{aligned}
$$
\end{proof}
\end{pro}

\section{Stochastic Integrals}
Let $B(t)$ be Brownian motion.
$\F_t$ is a \textbf{Brownian Filtration} if
\begin{enumerate}
\item $B(t)$ is $\F_t$-measurable $\forall t$
\item $B(t) - B(s)$ is independent of $\F_s \ \forall s < t$.
\end{enumerate}

\begin{exmp}
$\F_t = \sigma(B(s) : s \leq t )$ works.
\end{exmp}


Brownian filtration is essential in It\^o's integral since
in Wiener's integral, we concern
$$\int_a^b f(s) dB(s) : \omega \mapsto \int_a^b f(s) dB(s,\omega),$$
while
in It\^o's integral, we concern
$$\int_a^b f(s) dB(s) : \omega \mapsto \int_a^b f(s,\omega) dB(s, \omega),$$
and $f : [a,b] \times \Omega \rightarrow \mathbb{R}$.

\begin{exmp}
$\int_a^b g(B(s)) dB(s)$ and $\int_a^b g(s,B(s))dB(s)$.
\end{exmp}


\begin{defn}
$f \in L^2_{\text{ad}}([a,b]\times \Omega)$ (admissible and adapted) if
\begin{enumerate}
\item $f(t)$ (random variable such that $(f(t))(\omega) = f(t,\omega))$ is $\F_t$-measurable $\forall t$.
(i.e. $(f(t))$: adapted to $(\F_t)$)
\item $f \in L^2([a,b] \times \Omega)$ i.e.
$$
\int_a^b\int_\Omega f^2(t,\omega) d\P dt < \infty
(\Leftrightarrow \int_a^b \E[f(t)^2] dt < \infty).
$$
\end{enumerate}
\end{defn}


Purpose of today: Define $\int_a^b f(t) dB(t)$ for $f \in L^2_{\text{ad}}$.



\begin{enumerate}[label=\textbf{Step \arabic*)}, wide]
\item Step Stochastic Process $f(t)$

Let $f(t) = \sum_{i=1}^n X(t_{i-1}) \1_{[t_{i-1}, t_i)}(t)$, where $a = t_0 < t_1 < \cdots < t_n = b$, and $X(t_i)$: $\F_t$-measurable $\forall t$.
Define
$$
I(f) := \sum_{i=1}^n X(t_{i-1}) (B(t_i) - B(t_{i-1}))
$$

\begin{lem}
$\E\left[I(f)^2\right] = \int_a^b \E\left[f(t)^2\right] dt (= \|f\|^2_{L^2([a,b]\times\Omega)})$
\begin{proof}
$$
\begin{aligned}
\E\left[I(f)^2\right]
&= \sum_{i=1}^n \E\left[ X(t_{i-1})^2 (B(t(_i) - B(t_{i-1}))^2)\right]\\
&\phantom{=}+ 2\sum_{i < j} \E\left[ X(t_{i-1})X(t_{j-1}) (B(t_i) - B(t_{i-1}))(B(t_{j} - B(t_{j-1}))) \right],
\end{aligned}
$$ and
$$
\begin{aligned}
\sum_{i=1}^n \E\left[ X(t_{i-1})^2 (B(t(_i) - B(t_{i-1}))^2)\right]
&= \E\left[\E\left[ X(t_{i-1})^2 (B(t(_i) - B(t_{i-1}))^2) \right] | \F_{t_{i-1}} \right]\\
&= \E\left[ X_{t_{i-1}}^2 \E\left[ (B(t_i) - B(t_{i-1}))^2 \right] | \F_{t_{i-1}} \right]\\
&= (t_i - t_{i-1}) \E\left[X_{t_{i-1}}^2\right],
\end{aligned}
$$ and
$$
2\sum_{i < j} \E\left[ X(t_{i-1})X(t_{j-1}) (B(t_i) - B(t_{i-1}))(B(t_{j} - B(t_{j-1}))) \right] = 0.
$$
Therefore, $\E\left[I(f)^2\right] = \sum_{i=1}^n (t_i - t_{i-1}) \E\left[X_{t_{i-1}}^2\right] = \int_a^b \E\left[ f(t)^2\right] dt$.
\end{proof}
\end{lem}

\begin{rem}
$\E\left[ I(f) \right] = 0$ but it is possible that $I(f) \not\sim N\left(0, \int_a^b \E\left[ f(t)^2\right] dt \right)$.
\end{rem}

\item Approximation: $f \in L^2_{\text{ad}} \Rightarrow \exists$ step stochastic process $f_n \in L^2_{\text{ad}}$ such that
$$
\int_a^b \E\left[ (f(t) - f_n(t))^2\right]dt \rightarrow 0 \quad \text{as } n\rightarrow\infty.
$$
Proofs: later

\item $f\in L^2_{\text{ad}} \Rightarrow \exists f_n$ steps such that $\|f_n - f\|_{L^2([a,b]\times\Omega)} \rightarrow 0.$
Therefore, $(f_n)$: convergent. i.e., Cauchy in $L^2([a,b] \times \Omega)$, and then by Lemma, $(I(f_n))$ is Cauchy in $L^2(\Omega)$.
($\because \E\left[ (I(f_m) - I(f_n))^2 \right] = \E\left[ I(f_m - f_n)^2 \right] = \|f_m - f_n\|^2_{L^2([a,b]\times\Omega)}$).
Therefore, $I(f_n)$ is convergent in $L^2(\Omega)$. (By the completeness of $L^2(\Omega)$)
We denote by $I(f)$ this limit.

\end{enumerate}

\begin{defn}[It\^o's integral]
$\int_a^b f(t) dB(t) := I(f)$
(Note: We have to check well-definedness)
\end{defn}

\begin{thm}
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $I(\alpha f + \beta g) = \alpha I(f) + \beta I(g)$ (linear)
\item $\E[I(f)] = 0$
\item $\E[I(f)^2] = \int_a^b \E[f(t)^2]dt$
\item $\E[I(f)I(g)] = \int_a^b \E[f(t)g(t)]dt$
\end{enumerate}
\begin{proof}[Sketch of proof]
\leavevmode
\begin{itemize}
\item 1, 3 $\Rightarrow$ 4 (insert $f+t$ to $f$ at 3)
\item 1,2,3: For step stochastic process $f_n \rightarrow f$ in $L^2([a,b]\times\Omega)$ show that
$\E[I(f_n)] \rightarrow \E[I(f)]$ and $\E[I(f_n)^2] \rightarrow \E[I(f)^2]$.
\end{itemize}
\end{proof}
\end{thm}




Comment on Step 2)
Let $\alpha(s,t) = \E[f(t)f(t)]$, $\alpha: [a,b]^2 \rightarrow \R$.
\begin{enumerate}
\item $\alpha$ is continuous in $[a,b]^2$
\item $f$: bounded
\item $f \in L^2_{\text{ad}}$
\end{enumerate}


$X_t = \int_0^t f(s)dB(s); f \in L^2_{\text{ad}}([0,t] \times \O) \ \forall t > 0$.


\begin{thm}
$(X_t, \F_t)$: Martingale
\end{thm}

\begin{thm}
$X_t$ is continuous a.s.
i.e., $\exists \O_0 \subset \O$ such that $\P(\O_0) = 1$, and $\omega \in \O_0 \Rightarrow h(t) = X_t(\omega)$ is continuous in $t$.
\begin{proof}
Approximations via step stochastic process.
\end{proof}
\end{thm}


\begin{thm}
Let $f \in L^2_{\text{ad}}$ and $\alpha(s,t) = \E[f(s)f(t)]$ is continous in $[0,\infty)^2$.
Define $\Delta_n = \{ a = t_0 < t_1 < \cdots < t_n = b \}, \|\Delta_n\| = \max\limits_{i = 0 \sim n-1} (t_{i+1} - t_i)$.
Then, $lim_{\|\Delta_n\|\rightarrow 0} \sum_{i=1}^{n} f(t_{i-1}, \omega)(B(t_i,\omega) - B(t_{i-1},\omega)) \rightarrow \int_a^b f(t,\omega) dB(t,\omega)$ (a.s.).
\end{thm}



% ========================================
% 시험에는 안 낸대요
% ========================================


\chapter{An Extension of Stochastic Integral}
\textbf{Motivation}

In chapter 4, we assumed that $\int_a^b \E[f(t)^2]dt < \infty$ and play on $L^2_\text{ad}$.
However it is too much strong condition.
Instead, in this chapter, we assume that $\int_a^b f(t,\omega)^2 dt < \infty$ a.s and play on $\widetilde L^2_{\text{ad}}$.

Fact: $X \geq 0, \E[X] < \infty \Rightarrow X < \infty$ a.s.

$\E[\int_a^b f(t,\omega)^2 dt] = \int_\O\int_a^b f(t,\omega)^2 dt d\P$ (By Fubini)
$= \int_a^b \int_\O f(t,\omega)^2 d\P dt = \int_a^b \E[f(t)^2] dt$.
By Fact, condition on chapter 4 implies chdition on chapter 5, hence $L^2_{\text{ad}} \subset \widetilde L^2_{\text{ad}}$.
In this chapter, we will define It\^o's integral with that condition.

\vspace{5mm}

\textbf{Goal}

Define $I(f) = \int_a^b f(t)dB(t)$ for $f \in \widetilde{L}^2_{\text{ad}}$.

Idea: $f \in \widetilde L^2_{ad}$ then exists step stochastic process $f_n$ such that $\int_a^b |f_n(t,\omega) - f(t,\omega)| dt \rightarrow 0$ in probability.
% Then, $$ f_n(t,\omega) = \sum_{i=1}^n X_{t_{i-1}}(\omega) \1_{[t_{i-1}, t_i)}(t) = \sum_{i=1}^n f(t_{i-1}, \omega) \1_{[t_{i-1}, t_i)}(t) $$

\underline{Note}: $X_n \rightarrow X$ means
\begin{itemize}
\item $L^1$: $\E[|X_n - X|] \rightarrow 0$
\item $L^2$: $\E[|X_n - X|^2] \rightarrow 0$
\item a.s.: $\{ \omega : X_n(\omega) \not\rightarrow X(\omega)\}$ measure zero.
\item in probability: $\forall \delta > 0 \P[|X_n - X| > \delta] \rightarrow 0$.
\end{itemize}

\underline{Note}

$\P[|X_n - X| > \delta] < \frac{\E[|X_n - X|^2]}{\delta^2}$.
Therefore, $L^2$ convergence implies in probability convergence.
Then, we can follow the previous strategy.
(Warning: $(I(f_n))_{n=1}^\infty$ is no longer a Cauchy sequence in $L^2$)

\begin{thm}
$X_t = \int_0^t f(t)dB(t)$ and $f \in \widetilde L^2_{\text{ad}}$.
Then,
\begin{enumerate}
\item $X_t$ is a local martingale.
\item $X_t$ has continous realization.
\end{enumerate}
\end{thm}


\begin{exmp}
$f(t) = e^{B(t)^2}$. Then,
$$
\E[f(t)^2] = 
\begin{cases}
\frac{1}{\sqrt{1-4t}} & t < \frac{1}{4}\\
\infty & t \geq \frac{1}{4}
\end{cases}
$$
Hence calculating $\int_0^1 f(t)dB(t)$ is impossible in the sense of chapter 4,
but it is possible in the sense of chapter 5.
($\because \int_0^1 f(t)^2 dt = \int_0^1 e^{2B^2(t)} dt < \infty$ a.s.)
\end{exmp}

Question: $X_t = \int_0^t e^{B(s)}dB(s)$. Then is it Martingale? Continuous?


\underline{Note}: In local martingale, $\E[I(f)] \neq 0$.
% Local Martingale은 박형빈 교수님이 잘 알고 계시니 박형빈 교수님께 여쭈어보기.




% \chapter{Stochastic Integrals for Martingale}

\setcounter{chapter}{6}
\chapter{It\^o Formula}


\textbf{Motivation}

$\int f(B(t)) dB(t) = ?$


\begin{exmp}
For $f, g$ differentiable,
$\int f'(g(t)) dg(t) = \int_a^b f'(g(t))g'(t)dt = f(g(b)) - f(g(a))$.
\end{exmp}

However, $\int_a^b B(t)dB(t) = \frac{B(b)^2 - B(a)^2}{2} - \frac{b-a}{2}$.



Idea:
Let $a = t_0 < t_1 < \cdots < t_n = b, f \in C^2$.
Then,
$$
\begin{aligned}
f(B(b)) - f(B(a))
&= \sum_{i=1}^n [f(B(t_i)) - f(B(t_{i-1}))]\\
&\simeq \sum_{i=1}^n f'(B(t_{i-1}))(B(t_i) - B(t_{i-1})) + \sum_{i=1}^n \frac{1}{2} f'' (B(t_{i-1}))(B(t_i)-B(t_{i-1}))^2\\
&\simeq \sum_{i=1}^n f'(B_{t_{i-1}})(B_{t_i} - B_{t_{i-1}})) + \frac{1}{2} \sum_{i=1}^n f''(B_{t_{i-1}})(t_i - t_{i-1}) \quad \text{(by lemma)}\\
&\longrightarrow \int_a^b f'(B(t)) dB(t) + \frac{1}{2} \int_a^b f''(B(t)) dt \quad \text{as } \|\Delta_n\|\rightarrow 0.
\end{aligned}
$$

\section{It\^o's Formula in the Simplest Form}


\begin{lem}
$\sum_{i=1}^n g(B(t_{i-1})) \left[ (B(t_i) - B(t_{i-1}))^2 - (t_i-t_{i-1}) \right]
\rightarrow 0 
$ in $L^2$ as $\|\Delta_n\| \rightarrow 0$.
\end{lem}


\begin{thm}
$f \in C^2(\R)$. Then,
$$
f(B(b)) - f(B(a)) = \int_a^b f'(B(t))dB(t) + \frac{1}{2} \int_a^b f'(B(t))dt.
$$
\end{thm}



\begin{proof}[Proof of Lemma]
$$
\begin{aligned}
\E[()^2]
&= \sum_{i=1}^n \E\left[g(B(t_{i-1}))^2 \left[ (B(t_i) - B(t_{i-1}))^2 - (t_i-t_{i-1})\right]^2\right]\\
&\phantom{=} + 2\sum_{i<j} \E\left[ g(B(t_{i-1})) g(B(t_{j-1})) \left((B(t_i) - B(t_{i-1}))^2 - (t_i - t_{i-1})\right)\right.\\
&\phantom{===}\left.\times \left((B(t_j) - B(t_{j-1}))^2 - (t_j - t_{j-1})\right) \right]\\
&= \E[g(B(t_{i-1}))^2 \E[()^2 | \F_{t_{i-1}}]]\\
&= \frac{2(b-a)^2}{n^2} \E[g(B(t_{i-1}))^2]
\end{aligned}
$$
$\therefore \E[()^2] \leq \frac{2(b-a)^2}{n^2} \sum \frac{2(b-a)^2}{n^2} \sum_{i=1}^n \E[g(B(t_{i-1}))^2] \leq cn$
\end{proof}


\begin{rem}[Theorem 4.7.1]
Suppose $f \in L^2_{\text{ad}}$, $h(t,s) = \E[f(t)f(s)]$ is continuous in $\R^2$.
Then,
$$
\int_a^b f(t)dB(t) = \lim_{n\rightarrow\infty} f(t_{i-1})[B(t_i) - B(t_{i-1})]
$$
\end{rem}

\begin{rem}
In the above, $h$ is continuous if
\begin{enumerate*}[label=(\arabic*)]
\item $f(t) = g(B(t))$
\item $f(t) = g(t,B(t))$
\end{enumerate*}
for continuous $g$.
\end{rem}



\begin{thm}
$f \in C^2(\R)$.
Then,
$$
f(B(b)) - f(B(a)) = \int_a^b f'(B(t))dB(t) + \frac{1}{2} \int_a^b f''(B(t))dt
$$

\begin{proof}
Idea: If $g \in C^2$, then by intermediate theorem $g(y) - g(x) = (y-x)g'(x) + \frac{1}{2}(y-x)^2 g''(\alpha)$ for some $\alpha x + \lambda(y-x), \lambda \in [0,1]$.
Then,
\begin{align}
f(B(b)) - f(B(a))
&= \sum_{i=1}^n \left( f(B(t_i)) - f(B(t_{i-1})) \right)\tag*{}\\
&= \sum_{i=1}^n f'(B(t_{i-1}))(B(t_i) - B(t_{i-1}))\tag{1}\\
&\phantom{= }+ \frac{1}{2} \sum_{i=1}^n f''\left(B(t_{i-1}) + \lambda_i(B(t_i) - B(t_{i-1}))\right) (B(t_i) - B(t_{i-1}))^2\tag{2}
\end{align}
and (1) converges to $\int_a^b f'(B(t))dB(t)$ by theorem 4.7.1.
Thus, it suffices to show that (2) converges to $\frac{1}{2} \int_a^b f''(B(t))dt$ for some subsequence $(n_k)_{k=1}^\infty \rightarrow \infty$.

\begin{enumerate}[label={\bfseries STEP \arabic*}, wide]
\item
$\sum_{i=1}^n \left[ f''(B_{t_{i-1}}) + \lambda_i (B(t_i) - B(t_{i-1})) - f''(B(t_{i-1})) \right] (B(t_i) - B(t_{i-1})) \rightarrow 0$
% a.s. along some subsequence

Let $Y_{i,n} = \sum_{i=1}^n \left[ f''(B_{t_{i-1}}) + \lambda_i (B(t_i) - B(t_{i-1})) - f''(B(t_{i-1})) \right]$.
Then,
$$\sum |Y_{i,n} (B(t_i) - B(t_{i-1}))^2| \leq Y_n \sum(B(t_i) - B(t_{i-1}))^2,$$
where $Y_n = \max\{|Y_{1,n}|, \cdots ,|Y_{n,n}|\}$ and note that
$\sum (B(t_i) - B(t_{i-1}))^2$ converges to $b-a$.
Moreover, $Y_n \rightarrow 0$ a.s.

($\because$
let $M = \max\{B(t): t\in[a,b]\}$ and $m = \min\{B(t): t\in[a,b]\}$.
Then
$$
Z(\delta) = \sup_{x,y\in[m,M], |x-y|<\delta} |f''(y)-f''(x)| \rightarrow 0
$$
as $\delta \searrow 0$ by uniform continuity.
Moreover $D_n := \sup_{i=1\sim n} |B(t_i) - B(t_{i-1})| \rightarrow 0$ as $n\rightarrow\infty$ since Brownian motion is a.s. continous.
Then, $Y_n \leq Z(D_n) \rightarrow 0$ a.s. as $n\rightarrow 0$.)

\item
$\sum_{i=1}^n f''(B(t_{i-1}))\left[ (B(t_i) - B(t_{i-1}))^2 - (t_i-t_{i-1}) \right] \rightarrow 0$
% in probability

We skip the detailed case.
If $f''$ bounded then
$\E[\sum_{i=1}^n()]^2 = \sum_{i=1}^n \E[()^2] + 2\sum_{i<j}\E[()()]$.
The first term is $\E[\E[()^2|\F_{t_{i-1}}]] = \E[f''(B(t_{i-1}))^2 2(t_i-t_{i-1})^2] \leq c/n^2$,
and the second term is $\E[\E()()|\F_{t_{j-1}}] = 0$

For the non-bounded case, take a bound $L$ so that $E_L = [-L, L]$ and $\1_{E_L}f''$ be bounded.
Then, $f'' = \1_{E_L}f''$ for sufficient large $L$.

\item
$\sum_{i=1}^n f''(B(t_{i-1}))(t_i-t_{i-1}) \rightarrow \int_a^b f''(B(t))dt$
% a.s.

By definition of Riemann integral.
\end{enumerate}
The proof ends by adding the three steps.
\end{proof}
\end{thm}

\begin{rem}
In the right side of the equation, the first term is defined as It\^o's integral (but we can interpret it as Riemann sense), and the second term is defined in the sense of Riemann.
\end{rem}


\begin{exmp}
$e^B(t) - 1= \int_0^t e^{B(s)}dB(s) + \frac{1}{2}\int_0^t e^{B(s)}ds.$

The first term is Martingale and the second term increases in $t$.
Set $e^{B(t)} = X(t)$.
Then,

$$
X(t) - X(0) = \int_0^t X(s) dB(s) + \frac{1}{2} \int_0^t X(s)ds,
$$
and
\begin{equation}
dX(t) = X(t)dB(t) + \frac{1}{2}X(t)dt\tag{SDE}
\end{equation}
\end{exmp}

\begin{rem}
The first term is noise in stock price, and the second term is increaing in long-term.
\end{rem}



\setcounter{section}{2}
\section{It\^o's Formula Slightly Generalized}
Let $f(t,x) \in C^{1,2}$ (i.e. $f_t, f_x, f_{xx}$ are continuous)
Then,
$$
f(b,B(b)) - f(a,B(a))
= \int_a^b f_x(s,B(s))dB(s) + \int_a^b (f_t + \frac{1}{2} f_{xx})(s,B(s))ds
$$

\begin{proof}
$f(b,B(b)) - f(a,B(a)) = 
\sum_{i=1}^n (f(t_i B_{t_i}) - f(t_{i-1}, B_{t_{i-1}}))$, and
$$
\begin{aligned}
f(t_i, B_{t_i} - f(t_{i-1}, B_{t_{i-1}})
&= \left[f(t_i, B_{t_i}) - f(t_{i-1}, B_{t_i})\right]
+ \left[f(t_{i-1}, B_{t_{i}}) - f(t_{i-1}, B_{t_{i-1}})\right]\\
&= f(s_i, B_{t_i})(t_i - t_{i-1}) + \text{ same as before}\\
&\phantom{=}\rightarrow \int_a^b f_t(s,B(s))ds
\end{aligned}
$$
\end{proof}

\begin{exmp}
$f(t,x) = e^{x-\frac{1}{2}t}$ (or $e^{cx - \frac{c^2}{2}t}$)

Then, $f(t,B(t)) - f(0,B(0))
= \int_0^t f(s,B(s))dB(s)$ and
\begin{enumerate}
\item $X(t) = e^{B(t) - \frac{1}{2}t}$ is Martingale
\item $dX(t) = X(t)dB(t)$
\item $\E[X(t)] = \E[X(0)] = 1$
\end{enumerate}
\end{exmp}


\section{It\^o's Formula in the General Form}
$$
X(t) = X(0) + \int_0^t f(s)dB(s) + \int_0^t g(s)ds,
$$
where $f \in L^2_{\text{ad}}$; $\int_0^t \E[f(s)^2]ds < \infty \ \forall t$ and
$g \in L^2_{\text{ad}}$; $g$ is Riemann integrable a.s.


\begin{thm}
$\theta \in C^{1,2}(\R\times\R)$. Then,
$$
\begin{aligned}
\theta(b,X(b)) - \theta(a,X(a))
&= \int_a^b \theta_x(s,X(s))f(s)dB(s)\\
&\phantom{==}+ \int_a^b \left(\theta_t + g(s)\theta_x(s,X(s)) + \frac{1}{2}f^2 \theta_{xx}\right)(s,X(s))ds
\end{aligned}
$$
Why? $d\theta(t,B(t)) = \theta_t dt + \theta_x dB_t + \frac{1}{2} \theta_{xx}(dB_t)^2$ and $(dB_t)^2 = dt$ (It\^o formula), $(dB_t)^3 = 0$.
Thus, $d\theta(t,X(t)) = \theta_tdt + \theta_x dX(t) + \frac{1}{2} \theta_{xx}(dX(t))^2
= \theta_tdt + fdB(t) + gdt + f^2dt$.
\end{thm}


\textbf{Review}

$g$: differentiable, $f \in C^{1,1} \Rightarrow df(t,g(t)) = f_t(t,g(t))dt + f_x(t,g(t))dg(t)$
It\^o's formula is a Brownian version of the above ($f,(t,B(t))$).


Version 1.
$df(B(t)) = f'(B(t))dB(t) + \frac{1}{2} f''(B(t))dt$

Version 2.
$df(t,B(t)) = f_tdt + f_xdB(t) + \frac12f_{xx}dt = f_xdB(t) + (f_t+\frac12f_{xx})dt$


\begin{exmp}[Heat Equation]
$$
f(x) = 
\begin{cases}
f_t = \frac12 f_{xx}\\
f(0,x) = h(x)
\end{cases}
$$
Question. What is $f(T,x_0) = $?

Answer: $g(t,x) = f(T-t, x+x_0) \Rightarrow g_t = -f_t(T-t, x+x_0), g_{xx} = f_{xx}(T-t,x+x_0)$
$\therefore f_t = \frac12 f_{xx} \Rightarrow g_t + \frac12 g_{xx} = 0$

$\therefore g(T,B(T)) = -g(0,B(0)) = \int_0^T g_x(t,B(t))dB(t)$

\begin{rem}
$X_t = \int_0^t f(s)dB(s) \Rightarrow X_t$ is Martingale and continuous path $\Rightarrow \E[X_t] = \E[X_t | \F_0] = X_0 = 0$.
\end{rem}
\end{exmp}

$\therefore \E[g(T,B(T))] = -g(0,0) = f(T,x_0) = 0$.

$\therefore f(T,x_0) = \E[f(0,x_0+B(T))] = \E[h(x_0+B(T))] = \int_{-\infty}^\infty h(x_0 + y) \frac{1}{\sqrt{2\pi t}} e^{-\frac{y^2}{2T}}dy$

$\therefore f(T,x_0) = \int_{-\infty}^\infty h(x_0 - y) \frac{1}{\sqrt{2\pi T}} e^{-\frac{y^2}{2T}}dy
= (h * P_T)(x_0)$. ($P_t$: Heat Kernel)

\begin{rem}
It is a canonical relation of heat equation(PDE) and Brownian motion(probability).
\end{rem}


Version 3.
$\theta(t,x) \in C^{1,2}$. A \textbf{integral form} is
$$
X_t = X_0 + \int_0^t f(s)dB(s) + \int_0^t g(s)ds,
$$
where $f,g$ are stochastic process: $f \in L^2_{\text{ad}}, g \in L^1_\text{ad}$.
A \textbf{stochastic differential} is
$$
dX_t = f(t)dB(t) + g(t)dt
$$



Formal computation:
$
\begin{cases}
dtdt = 0\\
dB(t)dt = 0\\
dB(t)dB(t) = dt
\end{cases}
$


\begin{thm}
Version 1.
$$df(B_t) = f'(B_t)dB_t + \frac12 f''(B_t)(dB_t)^2$$
%$(= f'(B_t)dB_t + \frac12 \sum f''(B_t)(B_{t_i}-B_{t_{i-1}})^2$

Version 2.
$$df(t,B_t) = f_tdt + f_xdB_t + \frac12 f_{xx} (dB_t)^2$$

Version 3.
$$d\theta(t,X_t) = \theta_t dt + \theta_x dX_t + \frac12 \theta_{xx}(dX_t)^2$$

\hspace{4mm}(Note: $dX_t = fdB_t + gdt \Rightarrow (dX_t)^2 = (fdB_t + gdt)^2 = f^2dt$)

Version 3*.
$$d\theta(t,X_t) = [\theta_t + g\theta_x + \frac12 f^2 \theta_{xx}]dt + f\theta_x dB_t,$$
and its integral form is
$$
\theta(t,X_0) - \theta(0,X_0)
= \int_0^t \theta_t + g\theta_x + \frac12 f^2 \theta_{xx} dt + \int_0^t f\theta_x dB_t
$$

% Learn functional analysis to prove more elegant and intuitive way

% \begin{proof}[Proof of sketch]
% $$
% \sum_{i=1}^n \frac12 \theta_{xx}(t_{i-1}, X_{t_{i-1}})(X_{t_i}-X_{t_{i-1}})^2
% = \sum_{i=1}^n \frac12 \theta_{xx}(t_{i-1}, X_{t_{i-1}})
% \left[(t_i-t_{i-1})g(t_{i-1}) + (B_{t_i}-B_{t_{i-1}})f(t_{i-1})\right]^2
% $$
% \end{proof}
\end{thm}



\begin{exmp}[Geometric Brownian Motion]
$$dS(t) = \mu S(t)dt + \sigma S(t) dB(t).$$
% 장기적 우상향과 변동폭
Set $\theta(t,x) = \log x$.

$$
\begin{aligned}
d\theta(t,S(t))
&= \theta_x(t,S(t))dS(t) + \frac12 \theta_{xx}(t,S(t))(dS(t))^2\\
&= \frac{1}{S(t)} \left[\mu S(t)dt + \sigma S(t)dB(t)\right] + \frac12(-\frac{1}{S(t)^2})\sigma^2 S(t)^2 (dB(t))^2\\
&= (\mu-\frac12\sigma^2)dt + \sigma dB(t).
\end{aligned}
$$
It's integral form is
$$\theta(t,S(t)) - \theta(0,S(0)) = (\mu - \frac12\sigma^2)t + \sigma B(t).$$
Therefore $\log S(t) - \log S(0) = (\mu-\frac12 \sigma^2)t + \sigma B(t)$ and
$S(t) = S(0) e^{(\mu-\frac12\sigma^2)t + \sigma B(t)}$
\end{exmp}

\vspace{5mm}

\begin{exmp}[Vasicek's Model for interest rate]
$$
dr_t = (\alpha - \beta r_t)dt + \sigma dB(t).
$$
(Mean reverting to $\alpha/\beta$, and $\sigma dB(t)$ is noise)

Set $\theta(t,x) = e^{\beta t}x$.

$$
\begin{aligned}
d\theta(t,r_t)
&= \theta_t(t,r_t)dt + \theta(t,r_t)\left[ (\alpha-\beta r_t)dt + \sigma dB_t \right]
+ \frac12 \theta_{xx}(t,r_t)\sigma^2 dt\\
&= \beta e^{\beta t} r_tdt + e^{\beta t}x[(\alpha-\beta r_t)dt + \sigma dB_t]\\
&= \alpha e^{\beta t}dt + \sigma e^{\beta t}dB_t .
\end{aligned}
$$
It's integral form is
$$
\theta(t,r_t) - \theta(0,r_0)
= \int_0^t \alpha e^{\beta s}ds + \int_0^t \sigma e^{\beta s} dB(s).
$$
Therefore, $e^{\beta t}r_t - r_0 = \alpha \frac{e^{\beta t -1}}{\beta} + N(0, \sigma^2 \frac{e^{2\beta t}-1}{2\beta})$ and
$$
r_t = e^{-\beta t}r_0 + \frac\alpha\beta (1-e^{-\beta t}) + N\left(0, \frac{\sigma^2}{2\beta}(1-e^{-2\beta t})\right).
$$
Furthermore, $r_t \rightarrow \frac\alpha\beta + N(0, \frac{\sigma^2}{2\beta})$ as $t\rightarrow0$.
\end{exmp}




\newpage


\textbf{Multidimensional Brownian Motion}

Brownian motion in $\mathbb{R}^d$ is a $d$-independent Brownian motion $B(t) = (B_1(t), B_2(t), \cdots, B_d(t))$.

$$
\begin{aligned}
B(t) : \ &\O \longrightarrow \R^d\\
&\omega \longmapsto (B_1(t,\omega), \cdots, B_d(t,\omega))
\end{aligned}
$$

\begin{rem}
In $d$-dimensional Brownian motion $B(t)$, consider a $d$-dimensional ball $A$.
If $d = 1$, then obvious $\phi(d) = \P[\exists t \text{ s.t. }B(t) \in A] = 1$.
If $d = 2$, then $\phi(d) = 1$, and $\phi(d) < 1$ for $d > 2$ (not zero)
\end{rem}


\vspace{5mm}


Goal of the today: Multidimensional It\^o's formula (chapter 7) and SDE (chapter 10). 


Let $B_1(t), B_2(t), \cdots, B_n(t)$ be independent Brownian motions.
\begin{equation}
\begin{aligned}
dX_t^{(1)} = f_{11}(t)dB_1(t) + f_{12}(t) dB_2(t) + g_1(t)dt\\
dX_t^{(2)} = f_{21}(t)dB_1(t) + f_{22}(t) dB_2(t) + g_2(t)dt,
\end{aligned}
\tag{$\bigstar$}
\end{equation}
$f_{11}, f_{12}, f_{21}, f_{22} \in L^2_{\text{ad}}, g_1, g_2 \in L^1[a,b]$ a.s.
$(\bigstar)$ can be written as

$$
d
\begin{pmatrix}
X_t^{(1)}\\
X_t^{(2)}
\end{pmatrix}
=
\begin{pmatrix}
f_{11} & f_{12}\\
f_{21} & f_{22}\\
\end{pmatrix}
d
\begin{pmatrix}
B_1(t)\\
B_2(t)
\end{pmatrix}
+
\begin{pmatrix}
g_1\\
g_2
\end{pmatrix}
dt
$$
or

$$
dY_t = F(t) dB(t) + G dt.
$$
$Y_t$ is diffusivity, $B(t)$ is 2-dim Brownian motion, $G$ is drift.

\underline{Note}
\begin{itemize}
\item $dt dt = dB_1 dt = dB_2 dt = dB_1 dB_2 = 0$
\item $(dB_1)^2 = (dB_2)^2 = dt$
\end{itemize}


\begin{thm}
$\theta(t,x,y)$, $\theta: [a,b] \times \R^2 \rightarrow \R \in C^{1,2}([a,b]\times\R^2)$.
Then,
$$
d\theta(t, X_t^{(1)}, X_t^{(2)})
= \theta_t dt + \theta_x dX_t^{(1)} + \theta_y dX_t^{(2)}
+ \frac12 \theta_{xx}\left(dX_t^{(1)}\right)^2 + 
+ \frac12 \theta_{yy}\left(dX_t^{(2)}\right)^2 + 
\theta_{xy} dX_t^{(1)}dX_t^{(2)}
$$
\end{thm}


\begin{exmp}
\leavevmode
\begin{enumerate}[wide]
\item
$f(t,B_1(t),B_2(t))$

$$
\begin{aligned}
&\phantom{==}df(t,B_1(t), B_2(t)) = f_tdt + f_x dB_1 + f_y dB_2 + \frac12 f_{xx} dt + \frac12 f_{yy} dt\\
&= \left(f_t + \frac12(f_{xx}+f_{yy})\right)dt + f_x dB_1 + f_y dB_2
\end{aligned}
$$
Using this, we can solve the heat equation $g_t = \frac12(g_{xx} + g_{yy}) = \frac12 \Delta g$
by putting $f(t,x,y) = g(T-t, x+x_0, y+y_0)$.

\item
$X_t, Y_t ; \theta(t,x,y) = xy$.

\begin{equation}
d(X_tY_t)
= Y_t dX_t + X_t dY_t + dX_t dY_t
\tag{product rule}
\end{equation}

\item
$\theta(t,x,y) = \frac{x}{y}$. Find $d(\frac{X_t}{Y_t})$ (exercise!)

\item
$dX_t = aX_t dB_1(t)+ bX_t dB_2(t) + \alpha X_t dt$,
$dY_t = cT_y dB_1(t) + dY_t dB_2(t) + \beta Y_t dt$.
(Two noise in the stock price)

$$
\begin{aligned}
d(X_tY_t)
&= X_t dY_t + Y_t dX_t + dX_t dY_t\\
&= X_tY_t((a+c)dB_1(t) + (b+d)dB_2(t))
+ X_tY_t(\alpha + \beta + (ac+bd))dt
\end{aligned}
$$
Put $Z_t = X_tY_t$. Then,
$$
\begin{aligned}
dZ_t &= Z_t\left((a+c)dB_1(t)+(b+d)dB_2(t) \right)
+ (\alpha+\beta+(ac+bd))Z_tdt\\
&= \sqrt{(a+c)^2 + (b+d)^2}dB(t) + + (\alpha+\beta+(ac+bd))Z_tdt\\
&= \sigma Z_t dB_t + \mu Z_t dt\\
&\Rightarrow Z_t \text{ is a generalized Brownian motion.}
\end{aligned}
$$

\underline{{Note}}:
$\frac{\alpha B_1 + \beta B_2}{\sqrt{\alpha^2+\beta^2}}$ is a Brownian motion (exercise)

\end{enumerate}
\end{exmp}






\setcounter{chapter}{9}
\chapter{Stochastic Differential Equations}

\begin{itemize}
\item
$\sigma(t,x), f(t,x): [a,b]\times\R \rightarrow \R$ (measurable)

\item
$B(t)$: Brownian motion

\item
$\F_t$: Brownian Filtration (i.e., $B(t) - B(s)$ is independent of $\F_t$ for all $s < t$)

\item
$\xi$: $\F_a$-measurable random variable.
\end{itemize}


\vspace{5mm}

\textbf{SDE} (Stochastic Differential Euqation)
$$
dX_t = \sigma(t,X_t) dB(t) + f(t,X_t)dt; X_a = \xi.
$$

is equivalent to \textbf{SIE} (Stochastic Integral Equation)
$$
X_t = \int_a^t \sigma(s, X_s) dB(s) + \int_a^t f(s,X_s)ds + \xi.
$$

\section{Some Examples}


\begin{defn}[Solution of SDE]
$X_t$: Stochastic Process on $t \in [a,b]$.
$X_t$ is a solution of SDE if
\begin{enumerate}
\item
$\sigma(t,X_t) \in L^2_{\text{ad}}([a,b], L^2(\O)) \ \forall t$ (\textit{cf.} Ch 5.)

\item
$f(t,X_t) \in L^1([a,b])$ a.s.

\item
SIE holds $\forall t \in [a,b]$ a.s.
\end{enumerate}

\begin{rem}
1 implies that the first term of SIE is defined, and 2 implies that the second term of SIE is defined.
Thus, the important condition is 3.
\end{rem}
\end{defn}



Question.
1. Exist? 2. Unique?


\begin{exmp}
\leavevmode
\begin{enumerate}
\item
(Non-Existence)
$dX_t = X_t^2 dB_t + X_t^3 dt, X_0 = 1$.

$Y_t = \frac{1}{X_t} = f(X_t)$, $f(x) = \frac{1}{x}$.
Then,
$$
\begin{aligned}
dY_t
&= f'(X_t)dX_t + \frac12 f''(X_t)(dX_t)^2\\
&= \left(-\frac{1}{X_t^2}\right) (X_t^2 dB_t + X_t^3 dt) + \frac{1}{2} \left(\frac{2}{X_t^3}\right)X_t^4 dt\\
&= -dB_t
\end{aligned}
$$
$\therefore Y_t = Y_0 - B = 1-B_t$ and $X_t = \frac{1}{1-B_t}$, which makes an explosion on $B_t = 1$

\item
(Non-Uniqueness)
$dX_t = 3X_t^{\frac{2}{3}}dB_t + 3X_t^{\frac{1}{3}}dt, X_0 = 0$.

$f_a(x) :=
\begin{cases}
(x-a)^3 & x \geq a\\
0 & x < a
\end{cases}
(a > 0)
\in C^2(\R)
$.
Let $Y_t = f_a(B_t)$. Then,
$$
\begin{aligned}
dY_t
&= f_a'(B_t)dB_t + \frac{1}{2}f_a''(B_t)dt\\
&= 3f_a(x)^{2/3}(B_t)dB_t + 3 f_a(x)^{1/3}(B_t)dt\\
&= 3Y_t^{2/3}dB_t + 3Y_t^{1/3}dt.
\end{aligned}
$$
Therefore, $Y_t$ is invariant of $a$, and the solution of $Y_t$ is not unique.
\end{enumerate}
\end{exmp}


% \section{Bellman-Gronwall Inequality}
\setcounter{section}{2}
\section{Existence and Uniqueness Theorem}



\begin{thm}
Let $dX_t = \sigma(t,X_t)dB_t + f(t,X_t)dt$.
\begin{enumerate}
\item
If $\sigma, f$ are Lipschitz (uniformly in time), then the solution is unique

\item
If $\sigma, f$ are Lipschitz and Linear Growth, then the solution uniquely exists.
\end{enumerate}
\end{thm}

\underline{Note}
\begin{itemize}
\item
Lipschitz (uniformly in time)
$$
|g(t,x) - g(t,y)| \leq c|x-y| \text{ for some } c > 0
$$

\item
Linear Growth
$$
|g(t,x)| \leq c(|x|+1) \text{ for some } c > 0
$$
\end{itemize}


\underline{Next Classes}\\
1,2,3 $\longrightarrow$ prove uniqueness and existence\\
4 $\longrightarrow$ Markov Property of $(X_t)$\\
5 $\longrightarrow$ Martingale (12/12)

% \section{System of Stochastic Differential Equations}



























% ========================================
% TA Session
% ========================================


\begin{appendices}
\chapter{TA Session}
\begin{exmp}[1.2.2]
Let $(\Omega_\infty, \mathcal{F}_\infty, \mathbb{P})$ be the independent, infinite coin-toss space.
Define stock price by
$$
\begin{aligned}
S_0(\omega) &= 4 \quad \text{for all } \omega \in \Omega_\infty\\
S_1(\omega) &=
\begin{cases}
8 & \text{if } \omega_1 = H\\
2 & \text{if } \omega_1 = T
\end{cases}
\\
S_2(\omega) &=
\begin{cases}
16 & \text{if } \omega_1 = \omega_2 = H\\
4 & \text{if } \omega_1 \neq \omega_2\\
1 & \text{if } \omega_1 = \omega_2 = T
\end{cases}
\\
\text{and in general}
\\
S_{n+1}(\omega) &=
\begin{cases}
2S_n(\omega) & \text{if } \omega_{n+1} = H\\
\frac{1}{2}S_n(\omega) & \text{if } \omega_{n+1} = T
\end{cases}
\end{aligned}
$$

Then, $S_0, S_1, \cdots, $ are random variable.\\
For example, $\mathbb{P}(S_2 = 4) = \mathbb{P}(A_{HT} \cup A_{TH}) = 2pq$

\end{exmp}



\vspace{5mm}

\begin{exmp}[2.2.2]
Let $\Omega$ be a three independent coin-toss space.
Stock price random variables $S_0, S_1, \cdots,$ are the same as the previous example.
Let the probability measure $\mathbb{P}$ be given by
$$\mathbb{P}(HHH) = p^3, \mathbb{P}(HHT) = p^2q, \cdots, \mathbb{P}(TTT) = q^3.$$
Assume $0 < p < 1$.
Then, the random variables $S_2$ and $S_3$ are not independent.\\
$\because$ Consider the sets $\{S_3 = 32\} = \{ HHH \}$ and $\{S_2 = 16\} = \{HHH, HHT \}$ whose probabilities are
$\mathbb{P}(S_3 = 32) = p^3$ and $\mathbb{P}(S_2 = 16) = p^2$.
In order to have Independence, $p^3 = \mathbb{P}(S_3 = 32) = \mathbb{P}(S_2 = 16 \text{ and } S_3 = 32) = \mathbb{P}(S_2 = 16)\mathbb{P}(S_3 = 32) = p^5\Rightarrow\!\Leftarrow$.

The random variables $S_2$ and $S_3/S_2$ are independent.
The $\sigma$-algebra generated by $S_2$ comprises $\phi, \Omega$, the atoms\\
$\{S_2 = 16\} = \{ HHH, HHT\}
, \{S_2=  4\} = \{ HTH, HTT, THH, THT\}
, \{S_2 = 1\} = \{TTH, TTH\}$, and their unions.\\

The $\sigma$-algebra generated by $S_3/S_2$ comprises $\phi, \Omega$ and\\
$\{S_3/S_2 = 2\} = \{HHH, HTH, THH, TTH\}
, \{S_3/S_2 = \frac{1}{2} \} = \{ HHT, HTT, THT, TTT\}$\\

For $A \in \sigma(S_2), B \in \sigma(S_3/S_2)$,
$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.

ex) $p^3 = \mathbb{P}(S_2 = 16 \text{ and } S_3/S_2 = 2\} = \mathbb{P}(S_2 = 16)\mathbb{P}(S_3/S_2 = 2)) = p^2 p = p^3$

\end{exmp}

\vspace{5mm}

\begin{exmp}[2.2.10 Uncorrelated, dependent normal random variables]
\leavevmode\\
Let $X, Z$ be random variable satisfying
$$
\begin{aligned}
X&: \text{ standard normal random variable}\\
Z&: \text{ independent of } X,\ \mathbb{P}(Z = 1) = \frac{1}{2},\ \mathbb{P}(Z = -1) = \frac{1}{2}
\end{aligned}
$$

Define $Y = ZX$.
Show
\begin{enumerate}
\item $Y$ is standard normal random variable
\item $X$ and $Y$ are uncorrelated but they are dependent.
\end{enumerate}


\begin{proof}
\leavevmode
\begin{enumerate}
\item
$$
\begin{aligned}
F_Y(b) &= \mathbb{P}(Y \leq b)\\
&= \mathbb{P}(Y \leq b \text{ and } Z = 1)
+ \mathbb{P}(Y \leq b \text{ and } Z = -1)\\
&= \mathbb{P}(X \leq b \text{ and } Z = 1)
+ \mathbb{P}(X \geq -b \text{ and } Z = -1)\\
&= \mathbb{P}(X \leq b)\mathbb{P}(Z=1)
+ \mathbb{P}(X \geq -b)\mathbb{P}(Z=-1)\\
&= \frac{1}{2}N(b) + \frac{1}{2}N(b)\\
&=N(b)
\end{aligned}
$$

\item
Since $\mathbb{E}X = \mathbb{E}Y = 0$,
$$Cov(X,Y) = \mathbb{E}[XY] = \mathbb{E}[ZX^2] = \mathbb{E}[Z]\mathbb{E}[X^2] = 0$$
$\therefore X$ and $Y$ are uncorrelated.\\
If $X$ and $Y$ are independent, $|X|$ and $|Y|$ are independent.
But $\mathbb{P}(|X| \leq 1, |Y| \leq 1) = \mathbb{P}(|X| \leq 1) = N(1) - N(-1)$, and $\mathbb{P}(|X| \leq 1, |Y| \leq 1) = \mathbb{P}(|X| \leq 1) \mathbb{P}(|Y| \leq 1) = (N(1)-N(-1))^2
\Rightarrow\!\Leftarrow$

\end{enumerate}
\end{proof}

Let $\mu_{X,Y}$ be a joint distribution measure of $(X,Y)$.
Since $|X| = |Y|$, $(X,Y)$ takes values only in the set
$C = \{(x,y): x = \pm y\}$.\\
It follows that for any measurable function $f$,
$$
\int_{-\infty}^\infty \int_{-\infty}^\infty \mathbb{1}_C(x,y) f_{X,Y}(x,y) dydx = 0
$$
$\therefore$ There is no joint density $f_{X,Y}$ for $(X,Y)$.


$$
\begin{aligned}
F_{X,Y}(a,b) &= \mathbb{P}(X \leq a, Y \leq b)\\
&= \mathbb{P}(X \leq a, X \leq b, Z = 1) + \mathbb{P}(X \leq a, -X \leq b, Z = -1)\\
&= \frac{1}{2} \mathbb{P}(X \leq a \wedge b) + \frac{1}{2}\mathbb{P}(-b \leq X \leq a)\\
&= \frac{1}{2} N(a \wedge b) + \frac{1}{2} ((N(a)-N(-b)) \vee 0)
\end{aligned}
$$
\end{exmp}

\begin{exmp}[2.2.12]
Let $(X,Y)$ be jointly normal with the density
$$
f_{X,Y}(x,y) = \frac{1}{2\pi \sigma_1\sigma_2 \sqrt{1-\rho^2}}
\exp \left(-\frac{1}{2(1-\rho^2)} \left[\frac{(x-\mu_1)^2}{\sigma_1^2} - \frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2}\right]\right)$$
Define $W = Y - \frac{\rho\sigma_2}{\sigma_1} X$.
Then, $X$ and $W$ are independent.

Note that linear combination of jointly normal random variables are jointly normal
(i.e., $(X,W)$ is jointly normal).

Thus it suffices to show that $Cov(X,W) = 0$ (by Thm 2.2.9)
$$
\begin{aligned}
Cov(X,W) &= \mathbb{E}[(X-\mathbb{E}X)(W-\mathbb{E}W)]\\
&= \mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] - \mathbb{E}[\frac{\rho\sigma_2}{\sigma_1}(X-\mathbb{E}X)^2]\\
&= Cov(X,Y) - \frac{\rho\sigma_2}{\sigma_1}\sigma_1^2\\
&= 0
\end{aligned}
$$

Let $f_{X,W}$ be joint density of $X$ and $W$.

$$
\begin{aligned}
\mathbb{E}[W] &= \mu_2 - \frac{\rho\sigma_2\mu_1}{\sigma_1} =: \mu_3\\
\mathbb{E}[(W-\mathbb{E}W)^2] &= \mathbb{E}[(Y-\mathbb{E}Y)^2] - \frac{2\rho\sigma_2}{\sigma_1}\mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] + \frac{\rho^2\sigma_2^2}{\sigma_1^2}\mathbb{E}[(X-\mathbb{E}X)^2]\\
&= \sigma^2 - \frac{2\rho\sigma_2}{\sigma_1}\rho\sigma_1\sigma_2 + \frac{\rho^2\sigma_2^2}{\sigma_1^2}\sigma_1^2\\
&=(1-\rho^2)\sigma_2^2 =: \sigma_3^2
\end{aligned}
$$

$\therefore f_{X,W}(x,w) = \frac{1}{2\pi\sigma_1\sigma_3} \exp\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2} - \frac{(w-\mu_3)^2}{2\sigma_3^2}\right)$.

Note that we have decomposed $Y$ into the linear combination $Y = \frac{\rho\sigma_2}{\sigma_1}X + W$ of a pair of independent normal random variables $X$ and $W$.

\end{exmp}

\vspace{5mm}

\begin{exmp}[2.3.3]
Let $\mathcal{G} = \sigma(X)$. Observe estimate $Y$ based on $X$ and error.
$$
\begin{aligned}
\mathbb{E}[Y|X] &= \frac{\rho\sigma_2}{\sigma_1} + \mathbb{E}[W] = \frac{\rho\sigma_2}{\sigma_1}(X-\mu_1) + \mu_2.\\
Y-\mathbb{E}[Y|X] &= W - \mathbb{E}[W]
\end{aligned}
$$
Note that the error is random variable with expected value zero and independent of the estimation $\mathbb{E}[Y|X]$.
\end{exmp}

\end{appendices}
\end{document}
