\documentclass[12pt]{report}
\usepackage[left=2cm,right=2.6cm,top=2.6cm,bottom=3cm,a4paper]{geometry}
\renewcommand{\baselinestretch}{1.3}

\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[inline]{enumitem}
\usepackage{linegoal}
\usepackage{amsmath,amssymb,latexsym,amsfonts, amsthm}
%\usepackage{verbatim}
\usepackage{xcolor}
%\usepackage{listings}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{kotex}

%\usepackage[math]{iwona}
\usepackage[tracking]{microtype}
\usepackage[sc,osf]{mathpazo}
%\usepackage{kpfonts}

\usepackage[all]{xy}

%\setlist[itemize]{label=\tiny\textbullet}
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}

% Modify Chapter Headings
% Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, Bjornstrup
% \usepackage[Lenny]{fncychap}

% Modify Section Style
\usepackage[explicit]{titlesec}
\usepackage{soul}
%\definecolor{titleblue}{HTML}{4a7aa4}
\newbox\TitleUnderlineTestBox
\newcommand*\TitleUnderline[1]
    {%
        \bgroup
        \setbox\TitleUnderlineTestBox\hbox{\colorbox{black}\strut}%
        \setul{\dimexpr\dp\TitleUnderlineTestBox-.3ex\relax}{.3ex}%
        \ul{#1}%
        \egroup
    }
\newcommand*\SectionNumberBox[1]
    {%
        \colorbox{black}
            {%
                \makebox[1.5em][c]
                {%
                    \vspace{7mm}
                    \color{white}%
                    \strut
                    \csname the#1\endcsname
                }
            }%
        \TitleUnderline{\ \ \ }%
    }
\titleformat{\section}
    {\Large\bfseries\sffamily\color{black}}
    {\SectionNumberBox{section}}
    {0pt}
    {\TitleUnderline{#1}}

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}



% tableofcontents with clickable
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, % make the links colored
    linkcolor=blue, % color TOC links in blue
    urlcolor=red, % color URLs in red
    linktoc=all % 'all' will create links for everything in the TOC
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New command
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\1}{\mathbb{1}}
\renewcommand{\O}{\Omega}
%\newcommand{\RP}{\mathbb{RP}}
%\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\r}{\rightarrow}
%\newcommand{\g}{\gamma}
%\newcommand{\E}{\mathcal{E}}
%\renewcommand{\S}{\mathcal{S}}
%\newcommand{\T}{\mathcal{T}}
%\newcommand{\M}{\mathfrak{M}}

%\newcommand{\inft}{\int_{-\infty}`^\infty}
%\newcommand{\rk}{\text{rank }}

\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\newcommand{\ri}{\Rightarrow}
\newcommand{\bigslant}[2]{{\raisebox{.1em}{$#1$}\left/\raisebox{-.1em}{$#2$}\right.}}

\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem 지정해주는 곳

\newtheoremstyle{break}
{\topsep}{\topsep}%
{\itshape}{}%
{\bfseries}{}%
{\newline}{}%
\theoremstyle{break}
\newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter

\newtheoremstyle{newdef}% name
{}%         Space above, empty = `usual value'
{}%         Space below
{}%         Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}%Thm head font
{}%        Punctuation after thm head
{\newline}% Space after thm head: \newline = linebreak
{}%         Thm head spec
\theoremstyle{newdef}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{remark}
\newtheorem*{exmp}{Example} % no numbering example
\newtheorem*{lem}{Lemma}
\newtheorem*{rem}{Remark} % no numbering remark



\usepackage[toc]{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{titlepage}
\centering
{\scshape\LARGE Seoul National University \par}
\vspace{1cm}
{\scshape\Large Lecture Note\par}
\vspace{5.5cm}
{\huge\bfseries Introduction to Stochastic \\Differential Equations\par}
\vspace{1.5cm}
\large Lecture by Seo Insuk \\
Notes taken by Lee Youngjae


\vfill
\vspace{1cm}\par
{\large \today\par}
\end{titlepage}

\setlength{\parindent}{0cm}

\tableofcontents

\setcounter{chapter}{-1}

\chapter{Introduction}
E-mail: \textit{insuk.seo@snu.ac.kr, 27-212}\\
Office Hour: Tuesday 15:00 - 16:00

Grading
\begin{itemize}
\item Mid-terms 1 (15\%, 10/10 or 17)
\item Mid-terms 2 (15\%, 11/7)
\item Fianl-term (40\%)
\item Assignment (20\%, 8-10 times)
\item Attendance (10\%, absent: -2\%, late: -1\%)
\end{itemize}


Let $X$ be a standard normal random variable in $\R$.
i.e., $\mathbb{P}[X \in [a,b]] = \int_a^b \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx$.
(Central Limit Theorem) If $x_1, x_2, \cdots, x_n \in X, E(x_i) = m, Var(x_i) = \sigma^2$, then
$$
\frac{\frac{x_1-m}{\sigma}+\frac{x_2-m}{\sigma}+\cdots+\frac{x_n-m}{\sigma}}{\sqrt n} \rightarrow X
$$

In this class, we study dynamic version of this theorem.
If $(W_t)_{t\geq 0}$ be a fluctuation, then $(W_t)_{t\geq 0}$ be a random variable in $C[0,T]$

\begin{exmp}
$\frac{dX_t}{dt} = rX_t; dX_1 = rX_tdt$. Then, $X_t = X_0 e^{rt}$ (unrisky assets, bank)\\
$dX_t = rX_tdt + \sigma X_tdW_t$, $\sigma:$ volatility (risky assets, stock)
\end{exmp}

We will study:
\begin{enumerate}
\item Probability Space
\item Random Variable
\item Expectation
\end{enumerate}

Textbooks:
\begin{enumerate}
\item Stochastic Calculus for Finance \RNum{2} (Shreve), covering chapter 1-3 or 4
\item Introduction to Stochastic Integration (Hui-Hsiung Kuo)
\end{enumerate}


\part{Stochastic calculus for finance}


\chapter{General Probability Theory}
\section{Infinite Probability Spaces}
There are three elements consisting probability space:
\begin{itemize}
\item $S$: Sample space
\item $\mathcal{E}$: Family of events $\mathcal{E} \subset 2^S$ ($\sigma$-algebra in measure theory)
\item $\mathbb{P}$: probability $\Rightarrow \mathbb{P}(E)$ is defined for all $E \in \mathcal{E}$ ($\mu$ with $\mu(S)=1$)
\end{itemize}

\begin{exmp}
\begin{minipage}[t]{\linegoal}
\begin{enumerate}
\item Toss a coin twice (H for Head, T for Tail)\\
Then, $S = \{HH, HT, TT, TT\}$
\item Uniform random variable in $[0,1]^3$\\
Then, $S = [0,1]^3$.
If $E = [0,\frac{1}{2}]^3$, then $\mathbb{P}(E) = Vol(E) = \frac{1}{8}$\\
\end{enumerate}
\end{minipage}

How to define $\mathcal{E}$?\\
In example 2, let $\mathcal{E} = $ family of all subsets of $[0,1]^3$ naively.
But Banach-Tarski Paradox says there are disjoint sets $E,F$ with $\mathbb{P}(E\cup F) \neq \mathbb{P}(E) + \mathbb{P}(F)$ in this $\mathcal{E}$.
Therefore we cannot naively set $\mathcal{E}$ (Use measure theory)\\

In example 1, suppose that we cannot see the second flip.
If $\{HH\} \not\in \mathcal{E}$ and $\{HT, HH\}\in\mathcal{E}$, then $\mathcal{E} = \{\phi, \{HH,HT\}, \{TH,TT\}, \{HH,HT,TH,TT\}\}$
\end{exmp}


\begin{defn}[Measure]
Let $\Omega$ be a non-empty set and $\mathcal{F}$ be family of subsets of $\Omega$ with
\begin{enumerate}
\item $\phi \in \mathcal{F}$
\item $A \in \mathcal{F} \Rightarrow A^C \in \mathcal{F}$
\item $A_1, A_2, \cdots \in \mathcal{F} \Rightarrow \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
\end{enumerate}
We say $\mathcal{F}$ as \textbf{$\sigma$-algebra} or \textbf{$\sigma$-field}, $A \in \mathcal{F}$ as \textbf{measurable}, and $\Omega$ as \textbf{measurable space}.
\end{defn}


\textit{Exercises.}
\leavevmode
\begin{enumerate}[label = \arabic*)]
\item $\Omega \in \mathcal{F}$
\item $A_1, A_2, \cdots \in \mathcal{F}$, then $A_1\cap A_2 \cdots \in \mathcal{F}$
\item $A_1, A_2, \cdots \in A_n \in \mathcal{F}$, then $A_1 \cup \cdots \cup A_n, A_1 \cap \cdots \cap A_n \in \mathcal{F}$.
\item $A, B \in \mathcal{F}$, then $A-B \in \mathcal{F}$
\end{enumerate}


\begin{defn}[Topological Space]
(See Rudin: \textit{Real and Complex Analysis, Chapter 1.})
Let $\Theta$ be non-empty set and $\tau$ be family of subsets of $\Theta$ with
\begin{enumerate}
\item $\phi, \Theta \in \tau$
\item $V_1, \cdots V_n \in \tau \Rightarrow V_1 \cap \cdots \cap V_n \in \tau$
\item $V_\alpha \in \tau \enspace \forall \alpha \in I \Rightarrow \bigcup_{\alpha\in I}V_\alpha \in \tau$.
\end{enumerate}
We say $V\in\tau$ be an \textbf{open set}, and $(\Theta,\tau)$ be a \textbf{topological space}.
\end{defn}


\begin{defn}[Measurable Function]
$f : (\Omega, \mathcal{F}) \rightarrow (\Theta, \tau)$ is \textbf{measurable} if
$f^{-1}(V) \in \mathcal{F} \enspace \forall V \in \tau$
\end{defn}


\begin{defn}[Positive Measure]
Let $\Omega$ be non-empty set and $\mathcal{F}$ be $\sigma$-algebra.
Then $\mu: \mathcal{F} \rightarrow [0,\infty]$ is called \textbf{measurable} if
\begin{enumerate}
\item $A_1, A_2, \cdots$: disjoint members of $\mathcal{F} \Rightarrow \mu(A_1\cup A_2\cup \cdots) = \sum_{i=1}^\infty \mu(A_i)$
\item $\mu(A) < \infty$ for some $A \in \mathcal{F}$,
\end{enumerate}
and $(\Omega, \mathcal{F}, \mu)$ is called a \textbf{measure space}.
\end{defn}


\begin{defn}[probability space, random variable]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $(\Omega, \mathcal{F}, \mathbb{P})$ is called a \textbf{probability space} if $\mathbb{P}(\Omega) = 1$.
\item $X$ is called a \textbf{random variable} if it is a function from $(\Omega, \mathcal{F}, \mathbb{P})$ to $\mathbb{R}$
\end{enumerate}
\end{defn}
% Note: Here the usage of the expression "be called as" is incorrect. 
% See: https://www.quora.com/What-is-the-difference-between-called-and-called-as


\underline{Next Class}
\begin{itemize}
\item Borel sets on $\mathbb{R}$ or $\mathbb{R}^d$
\item Lebesgue Measure
\item Lebesgue Integral (Define Expectation of random variable)
\end{itemize}

\vspace{5mm}

Last class, we define a sample space $\Omega$, a $\sigma$-algebra $\mathcal{F}$, and a (positive) measure $\mu : \mathcal{F} \rightarrow [0,\infty]$.
\vspace{5mm}

\textit{Exercises.}
\begin{itemize}
\item $A_1 \subset A_2 \subset \cdots \Rightarrow \mu(\bigcup_{i=1}^\infty A_i) = \lim_{n\rightarrow\infty}\mu(A_n)$
\item $A_1 \supset A_2 \supset \cdots, \mu(A_1) < \infty \Rightarrow \mu(\bigcap_{i=1}^\infty A_i) = \lim_{n\rightarrow\infty}\mu(A_n)$
\end{itemize}
% These are called continuity from below and continuity from above, resp.

\begin{thm}[Rudin 1.10]
Let $\mathcal{F}_0$ be a collection of subset of $\Omega$.
Then, $\exists! \mathcal{F}^*$ minimal $\sigma$-algebra containing $\mathcal F_0$.

\begin{proof}
Let $\{\mathcal{F}_\alpha, \alpha \in I\}$ be a family of $\sigma$-algebra containing $\mathcal{F}_0$.
Then, $\mathcal{F}^* = \bigcap_{\alpha\in I} F_\alpha$ satisfies the three condition:
1) contain $\mathcal{F}_0$
2) $\sigma$-algebra
3) minimal (trivial, $\mathcal{F}^* \subset \mathcal{F}_\alpha)$
\end{proof}
\end{thm}


\begin{defn}[Borel measurable]
$\mathcal{B}$ is a \textbf{Borel $\sigma$-algebra} on the topological space $(\Theta,\tau)$ if
$\mathcal{B}$ is a minimal $\sigma$-algebra containing $\tau$, and $B$ is a \textbf{Borel measurable} if $B \in \mathcal{B}$.
\end{defn}

\begin{rem}[Completion of measure space, Rudin 1.15]
\leavevmode\\
Consider an extension $(\Omega, \mathcal{F}, \mu) \rightarrow (\Omega, \overline{\mathcal{F}}, \mu)$ where
\begin{enumerate}
\item $\overline{\mathcal{F}} = \{ A \cup N : A \in \mathcal{F}, N \subset A_0 \in \mathcal{F}, \mu(A_0) = 0 \}$
\item $\mu(A \cup N) = \mu(A)$
\end{enumerate}
Then, (Check!)
\begin{enumerate}
\item (well-definedness) $A_1 \cup N_1 = A_2 \cup N_2 \Rightarrow \mu(A_1) = \mu(A_2)$
\item $\mu: \overline{\mathcal{F}}$ is $\sigma$-algebra.
\item $\mu : \overline{\mathcal{F}} \rightarrow [0,\infty]$ is a measure
\end{enumerate}
\end{rem}

\begin{exmp}
\leavevmode
\begin{enumerate}[label = \arabic*)]
\item $\mathbb{R}$
$$
\xymatrix@R+0.25em@C+2em{
\ar @{} [dr] |{}
\mathcal{F}_0 = \tau \ar[r]^{1.10} & \mathcal{B} \ar[r]^{\text{completion}} & \overline{\mathcal{B}}\\
\mathcal{L} \ar[r]^{\text{Rudin CH 2}} & \mathcal{L} \ar[r]^{\text{completion}} \ar[r] & \mathcal{L}
}
$$



\item $C[0,T] = \Omega = \{f ; f : [0,T] \rightarrow \mathbb{R}, \text{continuous}\}$.\\
Define $\mathcal{F}_0 = \{\bigcup_{t_1,t_2,\cdots,t_k}(A_1,A_2,\cdots,A_k)
: 0 \leq t_1 < t_2 < \cdots < t_k \leq T; A_1, \cdots A_k \in \overline{\mathcal{B}}
\}$.
We call $\{f \in C[0,T]: f(t_1) \in A_1, f(t_2) \in A_2, \cdots, f(t_k) \in A_k\}$ as \textbf{cylindrical set}.
Consider
% 깔끔하게좀 바꾸자
$$
\begin{aligned}
\mathcal{F}_0 &\overset{1.10}{\longrightarrow} &\mathcal{B} &\overset{\text{completion}}{\longrightarrow} &\overline{\mathcal{B}}\\
\mathbb{P}_{\text{BM}} &\overset{\text{KET}}{\longrightarrow} &\mathbb{P}_{\text{BM}} &\overset{\text{completion}}{\longrightarrow} &\mathbb{P}^*_{\text{BM}}
\end{aligned}
$$
(KET refers Kolmogorov's Extension Thm)
\end{enumerate}
\end{exmp}

\section{Random Variables and Distributions}
\begin{defn}
$f : \Omega \rightarrow \mathbb{R}$ is measurable if $f^{-1}(V) \in \mathcal{F}$ for any open set $V \subset \mathbb{R}$.
\end{defn}

\begin{rem}
$\mathcal{B}(\mathbb{R})$ = Borel $\sigma$-algebra in $\mathbb{R}$.
\end{rem}

\begin{rem}
If $f$: measurable, then $f^{-1}(B) \in \mathcal{F}$ for any $B \in \mathcal{B}(\mathbb{R})$.
\begin{proof}
Let $G = \{ A \subset \mathbb{R} : f^{-1}(A) \in \mathcal{F} \}$.
Then, $\tau \subset G$, $G: \sigma$-algebra (check!), hence $\mathcal{B}(\mathbb{R}) \subset G$.
\end{proof}
\end{rem}


\begin{defn}
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $(\Omega, \mathcal{F}, \mathbb{P})$ is a \textbf{probability space} if $(\mathbb{P}(\Omega) = 1$.
\item $X$ is a \textbf{random variable} if $X : \Omega \rightarrow \mathbb{R}$ is measurable.
\end{itemize}
\end{defn}

\begin{exmp}
\leavevmode
\begin{enumerate}
\item Toss a coin Twice.\\
$\Omega = \{HH,HT,TH,TT\}$,
$\mathcal{F} = 2^\Omega = \{$all subsets of $\Omega\}$,
$\mathbb{P}(A) = \frac{1}{4}|A|, \enspace A \in \mathcal{F}$.\\
Then, $X = $ the number of $H$'s is a random variable with $X(HH) = 2, X(HT)=X(TH)=1, X(TT)=1$.

\item Uniform random variable in $[0,1]$\\
$\Omega=  [0,1]$,
$\mathcal{F} = \{B \in \mathcal{B}(\mathbb{R}) : B \subset [0,1]\}$,
$\mathbb{P}(B) = \mathcal{L}(B)$ ($\mathbb{P}([0,1]) = \mathcal{L}([0,1]) = 1$).\\
Then, $X : [0,1] \rightarrow \mathbb{R}$ with $X(x) = 1$ is a (uniform) random variable in $[0,1]$.
\end{enumerate}
\end{exmp}


\begin{rem}
$\mathcal{L}$: Lebesgue measure on $\mathbb{R}$. i.e.,$\mathcal{L}(a,b) = b-a$.
Then, $\mathcal{L}(\{a\}) = 0$\\
$(\because
\{a\} = \bigcap_{i=1}^\infty (a-\frac{1}{n}, a+\frac{1}{n})
\Rightarrow \mathcal{L}(\{a\}) = \lim_{n\rightarrow\infty} \mathcal{L}((a-\frac{1}{n}, a+\frac{1}{n})) = 0
)$\\
Similarly, $\mathcal{L}([a,b]) = \mathcal{L}([a,b)) = \mathcal{L}((a,b]) = b-a$,
$\mathcal{L}(\mathbb{Q}) = \sum_{q \in \mathbb{Q}}\mathcal{L}(\{q\}) = 0$.
\end{rem}

Return to uniform random variable,
$$\mathbb{P}[X \in (a,b)] = \mathbb{P}[\{x : X(x) \in (a,b)\}] = \mathbb{P}[(a,b)] = b-a.$$

\begin{defn}[Distribution measure on $X$]
$X$ is a random variable in $(\Omega, \mathcal{F}, \mathbb{P})$.
$\mu_X$ is a \textbf{distribution measure} on $X$ if $\mu_X$ is a probability measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ such that 
$$
\mu_X(B) = \mathbb{P}[X \in B]
= \mathbb{P}[\{\omega : X(\omega) \in B\}]
= \mathbb{P}[X^{-1}(B)] \ \forall B \in \mathcal{B}(\mathbb{R})
$$
\end{defn}


\begin{defn}[Probability density function]
$f$ is a \textbf{probability density function} of $X$ if $\mu_X((a,b)) = \int_a^b f(x)dx$
\end{defn}

\begin{rem}
There is a measure with no pdf: Dirac measure
\end{rem}

\begin{rem}
Lebesgue-Radon-Nikodym decomposition implies that any measure can be decomposed as density part and singular part.
\end{rem}

\begin{exmp}[Standard Normal random variable]
\leavevmode\\
Let $\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$.
Define $F : (0,1) \rightarrow \mathbb{R}$ by $F(x) = N^{-1}(x)$ for $N(X) = \int_{-\infty}^x \phi(y)dy$.\\
Let $\Omega = (0,1), \mathcal{F} = \{B \in \mathcal{B}(\mathbb{R}) : B \subset (0,1)\}, \mathbb{P}(A) = \mathcal{L}(A) : A \in \mathcal{B}(\mathbb{R})$.\\
Then, $Y : \Omega \ni x \mapsto F(x) \in \mathbb{R}$ is a random variable with
$$
\begin{aligned}
\mathbb{P}[Y \in (a,b)]
&= \mathbb{P}[\{x : Y(x) \in (a,b)\}]\\
&= \mathbb{P}[\{x \in (N(a),N(b))\}]\\
&= N(b)-N(a) = \int_a^b \phi(x)dx,
\end{aligned}
$$
and a density function is $\phi$.
\end{exmp}


\vspace{5mm}
Previous Question: In the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and random variable $X : \Omega \rightarrow \mathbb{R}$, the random element or random realization $\omega \in \Omega$ is a element of events in sample space.
For example, $\omega = HHTTH$ is a random element in tossing a coin five times, and $X(\omega) = 3$. ($X(\omega)$ = \# of Heads)\\
In the previous example(Standard Normal random variable), define $(\Omega, \mathcal{F}, \mathbb{P}) = ((0,1), \mathcal{B}(0,1), \mathbb{P})$, $\mathbb{P}((a,b)) = b-a$, $F(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy$, $X : (0,1) \ni \omega \mapsto F^{-1}(\omega) \in \mathbb{R}$.
Then, $X$ is called a standard normal random variable.







\section{Expectations}
In the following, let $\Omega = (\Omega, \mathcal{F}, \mathbb{P})$.
Let $X : \Omega \rightarrow \mathbb{R}$.
Then the expection $\mathbb{E}(X)$ is a mean of $X(\omega)$ with respect to the randomness of $\omega$ (given by $\mathbb{P}$)


\begin{defn}[Lebesgue Integration]
$(\Omega, \mathcal{F}, \mu)$ is a measure space, and $f : \Omega \rightarrow \mathbb{R}$ is a measurable function.
\begin{enumerate}[label = (\arabic*)]
\item $f : \Omega \rightarrow [0,\infty)$\\
Let $0 = y_0 < y_1 < y_2 < \cdots \rightarrow \mathbb{R}$ be a partition of $[0,\infty)$,\\
$\Pi = \{y_0, y_1, y_2, \cdots\}$ : $\|\Pi\| = \sup_{i\geq 1}|y_i-y_{i-1}|$, and\\
$\text{LS}_\pi = \sum_{i=0}^\infty y_i \mu[f^{-1}([y_i,y_{i+1}))]$.\\
In Rudin's book, $\lim_{\|\Pi\|\rightarrow 0} \text{LS}_\Pi$ converges to an element belonging to $[0,\infty]$.\\
Now, $\int fd\mu := \lim_{\|\Pi\|\rightarrow 0}\text{LS}_{\Pi}$ is called a \textbf{Lebesgue Integral}.

\item $f : \Omega \rightarrow \mathbb{R}$\\
Let $f^+ = \max\{f,0\} \geq 0$, and $f^- = -\min\{f,0\} \geq 0$.
Then, $f = f^+ - f^-$, and $|f| = f^+ + f^-$.
If $\int f^+d\mu < \infty$ and $\int f^- d\mu < \infty$, then we say $f$ is Lebesgue integrable and $f \in L^1(\mu)$.
The Lebesgue integral of $f = \int fd\mu$ is defined as $\int f^+ d\mu - \int f^- d\mu$
\end{enumerate}
\end{defn}

\begin{rem}
\leavevmode
\begin{enumerate}
\item $\int f^+ d\mu < \infty$ and $\int f^- d\mu = \infty$, then $\int fd\mu = -\infty$.
The others are defined similarly.
\item $f \in L^1(\mu) \Leftrightarrow \int |f|d\mu < \infty$.
\end{enumerate}
\end{rem}


\begin{exmp}[Riemann vs Lebesgue Integral (p19-22)]
\leavevmode
\begin{itemize}
\item $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathcal{L})$ Lebesgue measure where $\mathcal{L}((a,b)) = b-a$.
\item $f : \mathbb{R} \rightarrow \mathbb{R} \in L^1(\mathcal{L})$
\item (Def) $A \subset \mathbb{R}$, $\int_A fd\mu := \int f \mathbb{1}_A d\mu$, where $\mathbb{1}_A(x) = 1$ if $x \in A$, and $0$ otherwise.
\end{itemize}
If $f$ is Riemann integrable, then $\int_{[a,b]} fd\mathcal{L} = \int_a^b fdx$.

Riemann integral is a limit of approximation by a partition of $x$-axis.
On the other hand, Lebesgue integral is a limit of approximation by a partition of $y$-axis with preimage.
Partition of $x$-axis is sensitive to fluctuation and restricted to Euclidean space, while partition of $y$-axis is not.
For example, $f(x) = \mathbb{1}_\mathbb{Q}(x)$ is Lebesgue integrable, but it is not Riemann integrable since it is sensitive to fluctuation.
\end{exmp}

\begin{defn}[Almost everywhere, 1.1.5 in Textbook]
$P(x)$ is a property at $x \in \mathbb{R}$.
We say $P$ holds \textbf{almost everywhere} (or a.e.) in $\mathbb{R}$ if and only if
$\mathcal{L}(\{x:P(x)$ does not hold $\} = 0$.
\end{defn}

\begin{exmp}
$f(x) = [x]$ is continuous almost everywhere.
\end{exmp}


\begin{thm}
$f$ is Riemann integrable if and only if $f$ is continuous a.e.
\end{thm}

\textit{Exercises.} $f = g$ a.e. $\Rightarrow \int fd\mathcal{L} = \int g d\mathcal{L}$.

\begin{defn}[Almost surely]
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space.
The event $A (\in \mathcal{F})$ occurs \textbf{almost surely} (a.s.) if $\mathbb{P}(A) = 1$.
\end{defn}

\begin{exmp}
Let $X$ be a uniform random variable in $(0,1)$.
Let $A = \{ X(\omega) \neq \frac{1}{2}\}$; $\mathbb{P}(A) = 1$.
\end{exmp}



\begin{defn}[Expectation, 1.3.3. in Textbook]
\textbf{Expectation} of $X : \Omega \rightarrow \mathbb{R}$ is defined by
$$\mathbb{E}(X) := \int_\Omega X d\mathbb{P} \quad \text{if} \quad \int_{\Omega}|X|d\mathbb{P} < \infty$$ 
\end{defn}

\begin{thm}[1.3.4 in Textbook]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $X$ takes finite number of values $\{x_1,x_2, \cdots, x_n\} \Rightarrow \mathbb{E}(X) = \sum_{i=1}^n x_i\mathbb{P}(X = x_i)$
\item $X, Y$: random variables, $E(|X|), E(|Y|) < \infty$,
\begin{enumerate}[label = (\roman*)]
\item $X \leq Y$ a.s. (i.e. $\mathbb{P}[\{X(\omega) \leq Y(\omega)\}]=1$), then $\mathbb{E}(X) \leq \mathbb{E}(Y)$
\item $X = Y$ a.s. $\Rightarrow \mathbb{E}(X) = \mathbb{E}(Y)$
\end{enumerate}
\item $X, Y$: random variables, $\mathbb{E}(|X|), \mathbb{E}(|Y|) < \infty \Rightarrow \mathbb{E}(\alpha X + \beta Y) = \alpha \mathbb{E}(X) + \beta \mathbb{E}(Y)$.
\item Jensen's Inequality: $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is a convex function $\Rightarrow \phi(\mathbb{E}(X)) \leq \mathbb{E}(\phi(X))$\\
(\textit{cf.} $\phi(t) = t^2$)
\end{enumerate}

\begin{proof}[Proof of 4.]
Define $S_\phi = \{(a,b) \in \mathbb{R}^2 : a + bt \leq \phi(t) \enspace \forall t\}$.
Then $\forall t \in \mathbb{R}, \phi(t) = \sup_{(a,b) \in S_\phi} \{a+bt\}$.
In fact, it is a equivalent condition. Now,
$$
\begin{aligned}
\phi(\mathbb{E}[X]) &= \sup_{a,b\in S_\phi} \{a + b\mathbb{E}[X]\}\\
&= \sup_{a,b \in S_\phi} \mathbb{E}[a+bX]\\
&\leq \mathbb{E}[\sup_{a,b\in S_\phi}(a+bX)] = \mathbb{E}[\phi(X)] \quad (\text{Check!})
\end{aligned}
$$
\end{proof}
\end{thm}


\begin{exmp}[Dirac Measure in $\mathbb{R}$]
$(\mathbb{R}, \mathcal{B}(\mathbb{R}), \delta_y) \enspace (y \in \mathbb{R})$ is a probability space with $\delta_y(A) = 1$ if $y \in A$, and $0$ otherwise.
Then, $\int_\mathbb{R} fd\delta_y = f(y)$ (Check!)
% The standard expression is 'a measure "on" a space', not "in"

\vspace{6mm}
Consider modeling: $X$: random variable such that probability of $x_i = p_i$ with $\sum_{i=1}^n p_i = 1$.
Then, $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mu)$ with $\mu = \sum_{i=1}^n p_i\delta_{x_i}$ is a probability space, and $P(X = x_i) = p_i$ for $X : \mathbb{R} \ni \omega \mapsto \omega \in \mathbb{R}$: Example of thm 1.3.4.
\end{exmp}


\textit{Summary:}
\begin{itemize}
\item Probability space: $(\Omega, \mathcal{F}, \mathbb{P})$
\item Random variables: $X: \Omega \rightarrow \mathbb{R}$
\item Expectation: $E(X) = \int Xd\mathbb{P}$
\end{itemize}





\section{Convention of Integrals}
We will use this section when we define the Brownian motion.
\begin{defn}
\leavevmode
\vspace{-6mm}
\begin{enumerate}[label = (\arabic*)]
\item
Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, and
$f, f_1, f_2, \cdots$ be measurable ($\Omega \rightarrow \mathbb{R}$).
Then, $f_n \rightarrow f$ \textbf{almost everywhere} (a.e.) if
$$
\mu[\{\omega:(f_n(\omega))_{n=1}^\infty \text{ does not converge to } f(\omega)\}] = 0
$$

\item
Let ($\Omega, \mathcal{F}, \mathbb{P}$) be a probability space, $X, X_1, X_2, \cdots$ be random variables.
Then, $X_n \rightarrow X$ \textbf{almost surely} (a.s.) if
$$\mathbb{P}[\{\omega: (X_n(\omega))_{n=1}^\infty \text{ does not converge to } X(\omega)\}] = 0$$
\end{enumerate}
\end{defn}


\textit{Question:}
$f_n \rightarrow $ a.e. Then, $\int f_nd\mu \rightarrow \int fd\mu$?
$X_n \rightarrow X$ a.s. Then, $\mathbb{E}[X_n] \rightarrow \mathbb{E}[X]$?


\begin{thm}[Monotone Convergence Theorem. 1.4.5 in Textbook]
$0 \leq f_1 \leq f_2 \leq \cdots$(or decreasing), and $f_n \rightarrow f$ a.e.
Then, $\int f_nd\mu \rightarrow \int fd\mu$.
\end{thm}

\begin{thm}[Dominated Convergence Theorem. 1.4.9 in Textbook]
$\exists g \in L^1(\mu)$ such that $|f_n| \leq g$ for all $n$, and $f_n \rightarrow f$ a.e.
Then, $\int f_n d\mu \rightarrow \int f d\mu$.
\end{thm}

\begin{cor}
$\exists Y \in L^1(\mathbb{P})$ such that $|X_n| \leq Y$ for all $n$, and $X_n \rightarrow X$ a.s.
Then, $\mathbb{E}[X_n] \rightarrow \mathbb{E}[X]$.
\end{cor}

\begin{exmp}
Let $f_n(x) = 
\begin{cases}
n^2x & \text{if } 0 \leq x \leq \frac{1}{n},\\
-n^2x + n & \text{if } \frac{1}{n} < x \leq \frac{2}{n},\\
0 & \text{otherwise}.
\end{cases}
$
Then, $f_n \rightarrow 0$ a.e. and $\int f_n dx = 1$.
\end{exmp}

\section{Computation of Expectations}

\textbf{Notation:} ($X : \Omega \ni \omega \mapsto X(\omega) \in \mathbb{R}$)
\begin{itemize}
\item $\mathbb{E}[X] = \int X d\mathbb{P} = \int_\Omega X(\omega) d\mathbb{P}(\omega)$
\item $\int_B X(\omega)d\mathbb{P}(\omega) := \int \mathbb{1}_B(\omega) X(\omega) d\mathbb{P}(\omega)$
\end{itemize}

\textbf{Recall:} $X$: random variable, $\mu_X$: distribution measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$,
$\mu_X(B) = \mathbb{P}(X \in B)$.


\begin{thm}
$g \in L^1(\mu_X)$.
Then, 
$\mathbb{E}[g(X)] = \int_\mathbb{R} g(x)d\mu_X(x) (:= \int g d\mu_X)$.

\begin{exmp}
$g(x) = x$.
$\int |x| d\mu_X(x) < \infty \Rightarrow \mathbb{E}[X] = \int xd\mu_X(x)$.
\end{exmp}

\begin{proof}
First, prove the thm holds for $g \geq 0$, then prove for general $g$ by $g = g^+ - g^-$.
\begin{enumerate}[label = (\arabic*)]
\item $g = \mathbb{1}_B$\\
By thm 1.3.4. (1), $E[\mathbb{1}_B(X)] = 1 \cdot \mathbb{P}[\mathbb{1}_B(X) = 1] = \mathbb{P}(X\in B) = \mu_X(B)
= \int \mathbb{1}_B(x) d\mu_X(x)$.
\item $g = \sum_{k=1}^n \alpha_k \mathbb{1}_{B_k}$\\
Trivial by linearity.
\item $g \geq 0$\\
By MCT.
See \textit{Rudin} chapter 1 for details.
\end{enumerate}
\end{proof}
\end{thm}


\textbf{Recall:} $X:$ random variable, $X$ has density function $f_X$ if
$$
\begin{aligned}
\mu_X((a,b)) &= \int_a^b f_X(x)dx \enspace \forall a,b.\\
\mu_X(B) &= \int_B f_Xd\mathcal{L} = \int_B f_X(x) d\mathcal{L}(x) = \int_B f_X(x)dx.
\end{aligned}
$$

\begin{thm}
$g \in L^1(\mu_X)$.
Then, $E(g(X)) = \int_\mathbb{R} g(x)f_X(x)dx$.

\begin{exmp}
Let $X$ be standard normal. i.e., $f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ (regardless what $X$ be).
Then, $E(X^4) = \int_\mathbb{R} x^4 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} = 3$.
\end{exmp}
\end{thm}



\chapter{Information and Conditioning}
\section{Information and $\sigma$-algebras}

\begin{exmp}
Toss a coin Three times.
$\Omega = \{ HHH, HHT, \cdots, TTT\}$.\\
$A_H = \{ HHH, HHT, HTH, HTT\}$,
$A_T = \{ THT, THT, TTH, TTT\}$.\\
Let $\mathcal{F}(1) = \{ \phi, \Omega, A_H, A_T\}$ so that
it is a $\sigma$-algebra containing the randomness up to time 1.\\
Similarly, define $A_{HH}, A_{HT}, A_{TH}, A_{TT}$.\\
Let $\mathcal{F}(2) = \{ \phi, \Omega, A_{HH}, A_{HT}, A_{TH}, A_{TT}, A_{HH} \cup A_{HT}, \cdots, A_{TT}^C\}$
so that it is a $\sigma$-algebra containing the randomness up to time 2, and define $\mathcal{F}(0)$ similarly, and let $\mathcal{F}(0) = \{ \phi, \Omega\}$.\\
Then, $\mathcal{F}(0) \subset \mathcal{F}(1) \subset \mathcal{F}(2) \subset \mathcal{F}(3)$.
Let $X_t =$ \# of heads until time $t$.
Then, $X_t$ is $\mathcal{F}(t)$-measurable for each $t$.\\
Now, $\{ X_1 = 1\} = \{ \omega: X_1(\omega) = 1 \} = A_H$, and $\{X_1 = 0\} = \{ \omega : X_1(\omega) = 0\} = A_T$.
\end{exmp}

% 시간이 지나면서 random variable의 값이 계속 바뀌는데, 시점 t에서 주식 값이 evolute하느냐를 판단을 하고 싶다.
% 이 상황에서는 X_t와 함께 증가하는 \sigma-algebra가 필요하다.
% 시점 t에서 필요한 정보를 모아놓은 것이 sigma-algebra가 된다
% X_t는 시점 t까지의 information만 모아놓은 것.
% 우리는 X_t가 discrete한 경우가 아니라 주식처럼 X_t가 continuous한 경우를 다룬다
% 이를 확률미분방정식으로 나타내고 이걸 배운다.





\begin{defn}[$\sigma$-algebra generated by $X$]
$\Omega$ is a set, $X : \Omega \rightarrow \mathbb{R}$.
$\sigma(X) = \{ A \subset \Omega : A = X^{-1}(B)$ for some $B \in \mathcal{B}(\mathbb{R})\}$.
Then, $\sigma(X)$ is a $\sigma-$algebra(exercise) and it is called a \textbf{$\sigma$-algebra generated by} $X$.
\end{defn}

\begin{rem}
$X$ is a random variable in $(\Omega, \sigma(X))$.\\
$X$ is a random variable in $(\Omega, \mathcal{F})$, then $\sigma(X) \subset \mathcal{F}$ (exercise)
\end{rem}


\begin{defn}[$\mathcal{F}$-measurable]
$(\Omega, \mathcal{F})$: measure space.
$X : \Omega \rightarrow \mathbb{R}$.
$X$ is called \textbf{$\mathcal{F}$-measurable} if $\sigma(X) \subset \mathcal{F}$.
i.e., $X$: measurable with respect to $(\Omega, \mathcal{F}$).
\end{defn}


In example, $X(t)$ is $\mathcal{F}(t)$-measurable $\forall t$ (check!)\\
\textit{cf.} $X(t) : \Omega \rightarrow \mathbb{R}$.
$(X(t))^{-1}(B) \in \mathcal{F}(t) \enspace \forall B \in \mathcal{B}(\mathbb{R})$.\\
Enough to check $(X(t))^{-1}(\{0\}), (X(t))^{-1}(\{1\}), \cdots, (X(t))^{-1}(\{t\})$.\\
$\mathcal{F}(t)$ has enough information to determine $X(t)$ in the sense that
$\{\omega: (X(t))(\omega) \in B\} \in \mathcal{F}(t) \enspace \forall B \in \mathcal{B}(\mathbb{R})$.

\begin{defn}[Filtration, Stochastic Process]
$\Omega$: non-empty set, $T > 0$.
\begin{enumerate}
\item If $\mathcal{F}(t)$ is a $\sigma$-algebra $\forall t \in [0,T] \in T$ and $s < t \Rightarrow \mathcal{F}(s) \subset \mathcal{F}(t)$,
then $(\mathcal{F}(t) : t \in [0,T])$ is called a \textbf{filtration}
\item If $X(t) : \Omega \rightarrow \mathbb{R}$ is $\mathcal{F}(t)$-measurable $\forall t \in [0,T]$,
then $(X(t) : t \in [0,T])$ is called \textbf{Stochastic Process adopted to the filtration} $\mathcal{F}(t)$.
\end{enumerate}
\end{defn}


\section{Independence}

%\section{Conditional Expectation}
$X : \Omega \rightarrow \mathbb{R}$, $\mathcal{F}$: $\sigma$-algebra on $\Omega$.
\begin{enumerate}
\item $\mathcal{F}$ has full information to determine $X \Rightarrow X$ is $\mathcal{F}$-measurable. (2.1)
\item $\mathcal{F}$ has no information to determine $X \Rightarrow X$ is independent to $\mathcal{F}$. (2.2)
\item $\mathcal{F}$ has a partition information to determine $X \Rightarrow$ (2.3)
\end{enumerate}




\begin{defn}[independent]
$(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space.
$A, B \in \mathcal{F}$ is \textbf{independent} if $\mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)$.
\end{defn}

\textit{Question:} $X, Y$ are random variables in $(\Omega, \mathcal{F}, \mathbb{P})$.
If $X, Y$ are independent, then $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$, but the converse does not hold.


\begin{defn}
$(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space.
$\mathcal{G}, \mathcal{H} \subset \mathcal{F}$ are sub $\sigma$-algebras of $\mathcal{F}$.
$X, Y : \Omega \rightarrow \mathbb{R}$ are random variables in $(\Omega, \mathcal{F}, \mathbb{P})$.
\begin{enumerate}
\item $\mathcal{G}, \mathcal{H}$: independent iff $\P(A\cap B) = \P(A) \P(B) \enspace \forall A \in \mathcal{G}, B \in \mathcal{H}$.
\item $X, Y$: independent iff $\sigma(X), \sigma(Y)$ are independent.
\item $X, \mathcal{G}$: independent iff $\sigma(X), \mathcal{G}$ are independent.
\end{enumerate}
\end{defn}



\begin{defn}
$(\O, \F, \P)$ is a probability space.\\
$\mathcal{G}_1, \mathcal{G}_2, \cdots \mathcal{G}_n, \cdots$: sub $\sigma$-algebra of $\F$.
$X_1, X_2, \cdots, X_n, \cdots$: random variable in $(\O, \F, \P)$.
\begin{enumerate}
\item $\mathcal{G}_1, \cdots, \mathcal{G}_2$ are independent iff $\P(A_1 \cap \cdots \cap A_n) = \P(A_1) \cdots \P(A_n)$ for $A_1 \in \mathcal{G}_1, \cdots, A_n \in \mathcal{G}_n$.
\item $X_1, \cdots, X_n$ are independent iff $\sigma(X_1) \sim \sigma(X_n)$ are independent.
\item $\mathcal{G}_1, \mathcal{G}_2, \cdots$ are independent iff $\mathcal{G}_1 \sim \mathcal{G}_n$ are independent $\forall n$.
\item $X_1, X_2, \cdots$ are independent iff $X_1 \sim X_n$ are independent $\forall n$.
\end{enumerate}
\end{defn}

\begin{exmp}
Toss a coin three times.
\begin{enumerate}
\item $X(2), X(3)$ are not independent.\\
$\P(\{X(2) = 2\} \cap \{ X(3) = 1\}) \neq \P(X(2) = 2)\P(X(3)=1)$.
\item $X(2), X(3)-X(2)$ are independent.\\
Why: $X(2)$ is an information at tossing first, second times, and $X(3)$ is an information at tossing third time.
\end{enumerate}
\end{exmp}


\begin{defn}[Joint distribution]
$(\O, \F, \P)$ is a probability space. $X, Y$ are random variables in $\Omega$.
$(X, Y) : \O \ni \omega \mapsto (X(\omega), Y(\omega)) \in \R^2$
\begin{enumerate}
\item Joint Distribution Measure in $\R^2$\\
$\mu_{X,Y}(C) = \P((X,Y) \in C) $
for $C \in \mathcal{B}(\R^2)$.\\
(Note: We checked that $\{\omega : (X(\omega), Y(\omega)) \in C \} \in \mathcal{F}$ in real analysis.)

\item Joint Cumulative Distribution Function\\
$F_{X,Y}(a,b) = \P( X \leq a, Y \leq b) = \mu_{X,Y} ((-\infty, a] \times (-\infty, b])$ (check!)

\item Joint Probability Distribution Function\\
If $f_{X,Y} : \R^2 \r \R$ is Borel-measurable and satisfies
$\mu_{X,Y}(A\times B) = \int_B\int_A f_{X,Y}(x,y)dxdy$ for all $A, B \in \mathcal{B}(\R)$, then $f_{X,Y}$ is called a joint probability density function (jpdf)
\end{enumerate}
\end{defn}


\begin{thm}
$(\O,\F,\P)$ is a probability space, $X, Y$ are random variables in $\Omega$. Then, the followings are equivalent.
\begin{enumerate}[label=(\roman*)]
\item $X,Y$ are independent
\item $\mu_{X,Y}(A\times B) = \mu_X(A)\mu_Y(B) \enspace \forall A,B \in \mathcal{B}(\R)$
\item $F_{X,Y}(a,b) = F_X(a)F_Y(b) \enspace \forall a,b\in\R$
\item $\mathbb{E}[e^{uX+vY}] = \mathbb{E}[e^{uX}] \mathbb{E}[e^{vY}]$
\end{enumerate}
\end{thm}

\begin{rem}
If JPDF $f_{X,Y}$ exists, then (i) to (iv) $\Leftrightarrow f_{X,Y}(x,y) = f_X(x)f_Y(y)$ a.e.
\end{rem}

\begin{thm}
$X,Y$ are independent if and only if
$f, g : \R \r \R$ Borel-measurable, $\mathbb{E}[|f(X)g(Y)|] < \infty$ implies that
$\mathbb{E}[f(X)g(Y)] = \mathbb{E}[f(X)]\mathbb{E}[g(Y)]$.
\end{thm}

\begin{rem}
$f(x) = g(x) = x$ : $\mathbb{E}[|XY|] < \infty \Rightarrow \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$
\end{rem}


\begin{proof}
Details are exercises.
\begin{enumerate}[label = (\arabic*)]
\item $f = \mathbb{1}_A, g = \mathbb{1}_B$
\item $f, g$ are simple functions
\item $f, g \geq 0$
\item $f, g$ are general.
\end{enumerate}
\end{proof}






\underline{Review}\\
$\mathcal{G, H}$ are independent if
$\forall A \in \mathcal{G}, B \in \mathcal{H} \Rightarrow \P(A \cap B) = \P(A)\P(B).$\\
$X, Y$ are independent if $\sigma(X), \sigma(Y)$ are independent.\\
* $\sigma(X) = \{A \in \Omega : A = X^{-1}(B) \text{ for some } B \in \mathcal{B}(\R)\}$.\\
* $\mu_{X,Y}(C) = \P((X,Y) \in C) \enspace \forall C \in \mathcal{B}(\R^2)$.

\vspace{5mm}

Thm. T.F.A.E.C:
\begin{enumerate}
\item $X,Y$ are independent
\item $\mu_{X,Y}(A\times B) = \mu_X(A)\mu_Y(B)$
\item $\F_{X,Y}(x,y) = \F_X(x)\F_Y(y)$
\item (If JPDF $f_{X,Y}$ exists) $f_{X,Y}(x,y) = f_X(x)f_Y(y)$
\end{enumerate}




\begin{thm}
$(\O,\F,\P)$ is a probability space. $X,Y$ are independent random variables, $f, g : \R \r \R$ are Borel measurable.
Then, $f(X), g(Y)$ are independent.

\begin{proof}
$A \in \sigma(f(X))$;
$A = (f\circ X)^{-1}(B)$ for some $B \in \mathcal{\R}
= X^{-1}(f^{-1}(B)) \in \sigma(X)$.\\
$\therefore \sigma(f(X)) \subset \sigma(X)$, $\sigma(g(Y)) \subset \sigma(Y)
\Rightarrow \sigma(f(X)), \sigma(g(Y))$ are independent.
\end{proof}
\end{thm}

\begin{cor}
$\mathbb{E}[f(X)g(Y)] = \mathbb{E}[f(X))\mathbb{E}(g(Y)]$.
\end{cor}



\begin{defn}
$X,Y$ are random variables in $(\O,\F,\P)$.
\begin{enumerate}
\item $Var(X) = \E[(X-\E[X])^2] = \E[X^2] - \E[X]^2$
\item $std(X) = \sqrt{Var(X)}$
\item $Cov(X,Y) = \E[XY] - \E[X]\E[Y]$
\item $corr(X,Y) = cov(X,Y) / (std(X)std(Y))$
\end{enumerate}
\end{defn}

\begin{exmp}
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $X$: standard normal random variable $(N(0,1^2))$
\item $\P(Z = 1) = \P(Z = -1) = \frac{1}{2}$ ($X,Z$ are independent)
\item $Y = XZ $. Then
\begin{enumerate}[label = \arabic*)]
\item Y is standard normal,
\item $corr(X,Y) = 0$.
\item $X,Y$ are not independent.
\end{enumerate}
\end{itemize}
\end{exmp}



\begin{defn}[Jointly normal]
$X, Y$ are \textbf{jointly normal} with mean $m = (m_X, m_Y),\ Var(C) = 
\begin{bmatrix}
    C_{11} & C_{12} \\
    C_{21} & C_{22}
\end{bmatrix}
$ if
$$f_{X,Y}(z) = \frac{1}{\sqrt{(2\pi)^2\det C}}e^{-\frac{1}{2}(z-m)C^{-1}(z-m)^\mathsf{T}}$$
\end{defn}

\begin{thm}
$X,Y$ are jointly normal and uncorrelated $(C_{12} = C_{21} = 0)$.
Then, they are independent.
\end{thm}

\section{Conditional Expectation}
$(\O,\F,\P)$ is a probability space. $\int_A X d\P := \int \mathbb{1}_A X d\P = \int \mathbb{1}_A (\omega) X(\omega) d\P(\omega)$.
\begin{lem}
$\int_A Xd\P = \int_A Y d\P$ for all $A \in \F$ if and only if $X = Y$ a.s.
\begin{proof}
$A_n = \{ \omega : X(\omega) - Y(\omega) > \frac{1}{n}\},
B_n = \{ \omega : X(\omega) - Y(\omega) < -\frac{1}{n}\}$.
Then,
$$0 = \int_{A_n}(X-Y)d\P \geq \int_{A_n}\frac{1}{n}d\P = \frac{1}{n} \int \1_{A_n} d\P
= \frac{1}{n} \P(A_n)$$
Thus, $\P(A_n) = 0 \enspace \forall n$. Similarly, $\P(B_n) = 0$.
Now, $\{ \omega:X(\omega) \neq Y(\omega)\} = \left(\bigcup_{n=1}^\infty A_n\right) \cup \left(\bigcup_{n=1}^\infty B_n\right) \Rightarrow$ measure $0$.
\end{proof}
\end{lem}

\textit{Intuition}.
$(\O,\F,\P)$ is given, $X : \F$-measurable random variable, $\mathcal{G} \subset \F$ is a sub $\sigma$-algebra.
If we know nothing, then we expect $X$ as $\E[X]$.
If we know $\F$, then we expect $X$ as $X$.
Now, if we know $\mathcal{G}$, then we expect $X$ as $\E[X | \mathcal{G}]$ (what is it?)



\begin{defn}[Conditional Expectation]
$(\O,\F,\P)$ is a probability space.
$X \in L^1(\P)$ is a random variable.
$\mathcal{G}$ is a sub $\sigma$-algebra of $\F$.
We define $\E[X | \mathcal{G}]$ as
\begin{enumerate}
\item $\mathcal{G}$-measurable random variable
\item $\int_A \E[X|\mathcal{G}](\omega) d\P(\omega) = \int_A X(\omega) d\P(\omega)$.
\end{enumerate}
\end{defn}

\textit{Question.} $\E[X|\mathcal{G}]$ exists? (Yes! proof skip).
unique? (Yes! up to a.s.)

\begin{rem}
Lemma implies determine $X$ (a.s.) is equivalent to know $\int_A Xd\P \enspace \forall A \in \F$.\\
In this sense, conditional expectation $Y = \E[X|\mathcal{G}]$ is knowing $\int_A Yd\P = \int_A X d\P \enspace \forall A \in \mathcal{G}$.
\end{rem}


\begin{exmp}
Toss a coin three times.\\
$\F(0) \subset \F(1) \subset \F(2) \subset \F(3)$.
$X(t)$ is a number of heads until $t$ times; $X(t)$ is $\F(t)$-measurable.
If $\F(1) = \{ \phi, \Omega, A_H, A_T\}$, then
$\E[X(2) | \F(1)] = X(1) + \frac{1}{2}$, since we know the information of 1st flip.
\begin{proof}
Want: $\int_A (X(1) + \frac{1}{2})d\P = \int_A X(2) d\P$ for all $A \in \F(1)$
(\textit{c.f.} $\P(\omega) = \frac{1}{8} \enspace \forall \omega \in \Omega$).
For $A = A_H$, $\int \1_{A_H}(\omega) (X(1)(\omega) + \frac{1}{2})d\P(\omega) = \frac{3}{2}\P(A_H) = \frac{3}{4}$.

$\int \1_{A_H}(\omega)(X(2))(\omega) d\P(\omega)
= \sum_{\omega \in A_H} (X(2))(\omega)\P(\omega) = \frac{1}{8}(2+2+1+1) = \frac{3}{4}$.

\end{proof}
\end{exmp}

\begin{rem}
$\mathcal{G} = \sigma(Y)$;
$\E[X|\mathcal{G}] = \E[X | \sigma(Y)] := \E[X|Y]$
\end{rem}

\begin{thm}
$X, Y$ are independent random variable in $(\O,\F,\P)$, $\mathcal{G}$ is a sub $\sigma$-algebra of $\F$.
\begin{enumerate}
\item $\E[aX + bY | \mathcal{G}] = a\E[X|\mathcal{G}] + b\E[Y|\mathcal{G}]$
\item $X$ is $\mathcal{G}$-measurable.
Then, $\E[XY|\mathcal{G}] = X\E[Y|\mathcal{G}]$.
\item $\mathcal{H}$ is a sub $\sigma$-algebra of $\mathcal{G}$. Then,
$\E[\E[X|\mathcal{G}]|\mathcal{H}] = \E[X|\mathcal{H}]$.
\item $X, \mathcal{G}$ are independent, then $\E[X|\mathcal{G}] = \E[X]$.
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item Exercise

\item We only need to show that $X \geq 0, Y \geq 0$ implies 2.
\begin{enumerate}
\item $X = \1_B$\\
Want: $\E[\1_B Y | \mathcal{G}] = \1_B \E[Y | \mathcal{G}]$ for $B \in \mathcal{G}$.

\item $X = \sum_{i=1}^n \alpha_i \1_{B_i}$\\
Use linearity.

\item $X \geq 0$\\
Use MCT

\end{enumerate}

\item Want: $\E[\E[X|\mathcal{G}]|\mathcal{H}] = \E[X|\mathcal{H}]$\\
Let $A \in \mathcal{H}$.
Then, $\int_A \E[\E[X|\mathcal{G}]|\mathcal{H}](\omega) d\P(\omega)
= \int_A \E[X|\mathcal{G}] d\P
= \int_A Xd\P
= \int_A \E[X|\mathcal{H}]d\P$

\item Can be shown similarly as in 2.
Check $X = \1_B$ case. (Hint: $A \in \mathcal{G} \Rightarrow A, B$ are independent.)
\end{enumerate}
\end{proof}
\end{thm}


\begin{exmp}[Revisit]
$$
\begin{aligned}
\E[X(2) | \F(1)]
&= \E[X(2) - X(1) + X(1) | \F(1)]\\
&= \E[X(2) - X(1) | \F(1)] + X(1)\\
&= \E[X(2) - X(1)] + X(1)\\
&= \frac{1}{2} + X(1)
\end{aligned}
$$
\end{exmp}



\textbf{Review}
\begin{itemize}
\item $(\O,\F,\P)$: probability space, $\G \subset \F$.
\item $X : \F$-measurable random variable.
\item $Y = \E[X | \G]$ if $Y$ is $\G$-measurable.
\item $\int_A Y(\omega) d\P(\omega) = \int_A X(\omega) d\P(\omega) \enspace \forall A \in \G$.
\end{itemize}
\begin{rem}
$\E[X|\G]$ is an expectation of $X$ when we know $\G$.
\end{rem}

\begin{rem}
$Y = Z$ a.s. and $Z$ is $\G$-measurable, then $Z = \E[X|\G]$.
\end{rem}



\begin{rem}
$(X(t))_{t\in[0,T]}$ is stochastic process adapted to $(\F(t))_{t\in[0,T]}$.
In this, $(X(t))_{t\in[0,T]}$ is a random variable in $(\O,\F,\P)$ and $X(t)$ is $\F(t)$-measurable.
$\F(s) \subset \F(t)$ for all $s < t$ and $\F(0) = \{\phi,\O\}$.
\end{rem}

\begin{rem}
We can define $\F$ as $\F(t) = \bigcup_{s : s\leq t} \sigma(X(s))$
\end{rem}

\begin{defn}[Martingale, Markov Process]
\leavevmode
\begin{enumerate}
\item Martingale $X(t)$\\
$\E[X(t)|\F(s)] = X(s)$ for all $s < t$.

\item Markov Process $X(t)$\\
For any borel measurable $f$, there exists some borel measurable $g$ such that $\E[f(X(t)) | \F(s)] = g(X(s))$

\end{enumerate}
\end{defn}


\begin{rem}
In Martingale, if we know all the previous value, then the expectation of the future is as same as the expectation of the present.
\end{rem}

\begin{rem}
Markov process is a generalization of Markov chain.
We only have to know the present value.
\end{rem}



\chapter{Brownian Motions}
\section{Introduction}
To study Brownian Motions, we will study:
\begin{enumerate}
\item Random Walks
\item Definition of Brownian Motions and its basic property (We will change the textbook!)
\item Constuction of Brownian Motions
\end{enumerate}


\section{Scaled Random Walks}


\begin{defn}[Random Walk]
\leavevmode
\begin{itemize}
\item
Let
$
X_i = 
\begin{cases}
1 & \text{prob } \frac{1}{2}\\
-1 & \text{prob } \frac{1}{2}
\end{cases}
; X_1, X_2, \cdots
$
independent.

\item $M_n = X_1 + \cdots + X_n$ is called \textbf{random walk}

\item $W_n(t) = \frac{1}{\sqrt n} M_{nt} (:= \frac{1}{\sqrt n}M_{[nt]})^v, \enspace t \in \{ \frac{1}{n}k, k \in \mathbb{Z}_+\}$ is called \textbf{scaled random walk}
\end{itemize}
\end{defn}


\begin{pro}
Random walk holds the following properties:
\begin{enumerate}
\item Independent Increament
\item Martingale
\item Quadratic Variation
\end{enumerate}

\begin{proof}
\leavevmode
\begin{enumerate}
\item See Def 3.2.3
\item Let $\F(n) = \sigma(X_1, X_2, \cdots, X_n) = $ smallest $\sigma$-algebra making$X_1 \sim X_n$ measurable.
Then,
\begin{itemize}
\item $M_n = X_1 + \cdots X_n$ is $\F(n)$-measurable
\item $(M_n)_{n\in\mathbb{N}}$ is stochastic process adapted to $(\F(n))_{n=0}^\infty$
\item $k < l \Rightarrow
\E[M_l | \F(k)] = \E[M_l - M_k | \F(k)] + \E[M_k | \F(k)]
= \E[M_l - M_k] + M_k
= \E[X_{k+1} + \cdots + X_l] + M_k
= \E[X_{k+1}] + \cdots + \E[X_l] + M_k
= M_k.$
\end{itemize}

\item $\sum_{i=1}^n (M_i - M_{i-1})^2 = n$
\end{enumerate}
\end{proof}
\end{pro}

\begin{defn}[Independent increament]
$M_n$ is \textbf{independent increament} if
$M_{k_1}, M_{k_2}-M_{k_1}, \cdots, M_{k_m - k_{m-1}}$ are independent
for any $k_1 < k_2 < \cdots < k_m$.
Here, $M_{k_l} - M_{k_{l-1}}$ is called increament.
If $M_n$ is a random walk, then $M_{k_1} = \sum_{i=1}^{k_1} X_i, M_{k_2-k_1} = \sum_{i=k_1}^{k_2}X_i, \cdots$ are independent.
\end{defn}



\begin{rem}
Proposition 3.2.2 holds for scaled random variable $W_n(t) = \frac{1}{n} M_{nt}$ ($t \in \frac{1}{n} \mathbb{Z}_+$).
\begin{proof}
\leavevmode
\begin{enumerate}
\item Independent Increament\\
For $t_1 < t_2 < \cdots < t_m$,
$W_n(t_1) - W_n(0), W_n(t_2) - W_n(t_1), \cdots, W_n(t_m) - W_n(t_{m-1})$ are independent,
since its increaments $W_n(t_{n+1}) - W_n(t_l) = \frac{1}{n} (M_{nt_{l+1}} - M_{nt_l})$ are independent by independent increament property of $M_n$.

\item Martingale\\
Let $\F_n(t) = \sigma(X_1, X_2, \cdots, X_{nt})$.
Then, $W_n(t) = \frac{1}{n} (X_1+X_2+\cdots+X_{nt})$ is $\F_n(t)$-measurable.
Therefore, $(W_n(t))$ is stochastic process adapted to $(\F_n(t))$.
With some computations as before, $\E[W_n(t) | \F_n(s)] = \cdots = W_n(s)$ for $s < t$.

\item Quadratic Variation\\
$\sum_{i=1}^{nt} \left(W_n(\frac{i}{n}) - W_n(\frac{i-1}{n})\right)^2
= \sum_{i=1}^{nt} \left[ \frac{1}{\sqrt n} (M_i - M_{i-1})\right]^2
= \sum_{i=1}^{nt} \frac{1}{n} \cdot 1
= t$
\end{enumerate}
\end{proof}
\end{rem}



\begin{exmp}
Let $f \in C^1([0,t])$.
Then,
$$
\begin{aligned}
\sum_{i=1}^{nt} \left( f(\frac{i}{n}) - f(\frac{i-1}{n}) \right)^2
&= \sum_{i=1}^{nt} \left[ \frac{1}{n} f'(\frac{x_i}{n})\right]^2\\
&= \frac{1}{n} \frac{1}{n} \sum_{i=1}^{nt} \left( f'(\frac{x_i}{n}\right)^2) \quad ( \rightarrow \int_0^t [f'(x)]^2dx)\\
&\leq \frac{c}{n} \quad (\rightarrow 0)
\end{aligned}
$$
It is the most different property between random process and deterministic function:
Q.V. of random variable is constant but Q.V. of $C^1$ function is zero.
\end{exmp}


\begin{thm}[Central Limit Theorem]
Let $Y_1, Y_2, \cdots$ are independent and identically distributed (called i.i.d.) with mean $0$ and variation $1$
$(\E(Y_i) = 0, Var(Y_i) = \E(Y_i^2) = 1)$.
Then, 
\begin{equation}
\frac{1}{\sqrt n}\left[Y_1 + \cdots Y_n \right] \rightarrow N(0,1^2)\tag{$\bigstar$}
\end{equation}
\end{thm}

\begin{rem}
Meaning of $\bigstar$:
$$\P\left[\frac{1}{n} (Y_1 + \cdots + Y_n) \in [a,b]\right] \rightarrow \int_a^b \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx.$$
\end{rem}


$W_n(t) = \frac{1}{n} M_{nt} = \frac{1}{n} (X_1 + \cdots + X_{nt})
= \sqrt{t} \frac{1}{\sqrt{nt}} (X_1 + \cdots + X_{nt}) \sim N(0,t)$



\textit{cf.} $N(\mu, \sigma^2)$ is a normal random variable with mean $\mu$ and variation $\sigma^2$.
Using the above,
$$
\lim_{n\rightarrow\infty} \P\left[ W_n(t) \in [a,b]\right] = \int_a^b \frac{1}{\sqrt{2\pi t}} e^{-x^2/2t}dx.
$$

$
W_n(t) = \frac{1}{n^{\frac{1}{2} + \alpha}} M_{nt}
\begin{cases}
\alpha < 0 \quad |W_n(t)| \rightarrow \infty\\
\alpha > 0 \quad |W_n(t)| \rightarrow 0\\
\end{cases}
$


\begin{rem}
$\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}$ is a heat kernel in PDE.
\end{rem}

\vspace{5mm}

\textbf{Summary}
\begin{enumerate}
\item Independent Increament
\item Martingale
\item Markov Process
\item $W_n(t) \sim N(0,t)$\\
$W_n(t) - W_n(s) \sim N(0, t-s)$
\item Q.V. in $[0,t] = t$.
\end{enumerate}



\textbf{Review}\\
$X_1, X_2, \cdots$ are i.i.d. and
$X_i = \begin{cases} \pm 1 & 1/2 \\ -1 & 1/2\end{cases}$.\\
Random walk: $\mu_n = X_1 + \cdots + X_n$.\\
Scaled random walk: $W_n(t) = \frac{1}{\sqrt n}M_{nt}$. Then,
\begin{enumerate}
\item $W_n(0)=0)$
\item Independent Increament\\
$t_1 < t_2 < \cdots < t_n$, then
$W_n(t_1), W_n(t_2) - W_n(t_1), \cdots, W_n(t_n) - W_n(t_{n-1})$ are independent.
\item Asymptotic Normal\\
$W_n(t) - W_n(s) \sim N(0,t-s)$ as $n\rightarrow\infty$.
\end{enumerate}



% ========================================
% !New Book!
% ========================================
\part{Introduction to stochastic integral}



\setcounter{chapter}{1}

\chapter{Brownian Motion}
\section{Definition of Brownian Motion}

\begin{defn}[Stochastic Process]
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item Let $(\O, \F, \P)$ be a probability space.
\item $[0,\infty)$ with Borel $\sigma$-algebra
\item $X:[0,\infty) \times \O \rightarrow \R$, measurable.
\end{itemize}
Then, $X$ is a \textbf{stochastic process} if
\begin{enumerate}
\item $X(t,\cdot) : \O \rightarrow \R$ is random variable
\item $X(\cdot,\omega) : [0,\infty) \rightarrow \R$ is measurable.
\end{enumerate}
\end{defn}



\begin{rem}
$X(t,\cdot) \Rightarrow X(t)$: random variable in $\O$.
$X(t) : \omega \mapsto [X(t)](\omega) = X(t,\omega)$.\\
For each $t \in [0,\infty)$ there exists random variable $X(t) : \O\rightarrow \R$.
If we pick $\omega \in \O$, then each $X(t_i)$ is determined simultaneously by $X(t_i)(\omega)$.
\end{rem}


\begin{rem}
We can work in $[0,T]$ instead of $[0,\infty)$.
In fact, we can define in $[0,T]$ and extend to $[0,\infty)$, but it is extremly difficult.
\end{rem}


\begin{defn}[Brownian Motion in $[0,\infty)$]
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $t \in [0,\infty)$, $\omega \in \O((\O,\F,\P)$: probability space)
\item Stoch. Process $B(t,\omega)$
\end{itemize}
$B$ is called \textbf{Brownian Motion} if
\begin{enumerate}
\item $B(0,\omega) = 0$ a.s. (i.e.,$\P\left[\{\omega: B(0,\omega) = 0\}\right]=1$)
\item $B(\cdot, \omega) : [0,\infty) \rightarrow \R$ is a continuous function a.s.
\item $\forall 0 \leq s < t$, $B(t) - B(s) \sim N(0,t-s)$
\item Independent Increament
\end{enumerate}
\end{defn}

\begin{rem}
$B(0,\omega)$ is a measurable function.
\end{rem}

\begin{rem}
$B(t) \sim N(0,t)$ by 3 with $s=0$.
\end{rem}


\begin{rem}
$(B(t))_{t\geq 0} : \O \rightarrow \R$
\begin{itemize}
\item $B(t)$ itself is a normal distribution
\item $B(t) - B(s) : \O \rightarrow \R$ is normal distribution with variance $t-s$.
\end{itemize}
\end{rem}

\begin{rem}
Brownian motion is a continuous version of random walk: random walk has property 1,4 and has property 3 with $n\rightarrow\infty$.
\end{rem}



\begin{thm}
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $s < t$ : $\E[B(s)B(t)] = s$

\item $t_1 < t_2 < \cdots < t_n \Rightarrow (B(t_1), B(t_2), \cdots, B(t_n))$ is jointly normal with $\mu = (0,0,\cdots,0$ and $Var = C$.
$(C_{ij} = t_{\min(i,j)} \enspace \forall i,j)$.
\end{enumerate}

\begin{proof}
\leavevmode
\begin{enumerate}
\item
$\E[B(s)B(t)] = \E[B(s)(B(t)-B(s))] + \E[B(s)^2]
= \E(B(s))\E(B(t)-B(s)) + s = s$.

\item
Let $\vec{v} = (B(t_1), B(t_2)-B(t_1),\cdots,B(t_m)-B(t_{m-1}))$. Then,
$$
\text{PDF of } \vec{v}
= \frac{1}{\sqrt{2\pi t_1}}e^{-\frac{x_1^2}{2t_1}} \cdot \frac{1}{\sqrt{2\pi (t_2 - t_1)}}e^{-\frac{x_2^2}{2(t_2-t_1)}} \cdots \frac{1}{\sqrt{2\pi (t_m-t_{m-1})}}e^{-\frac{x_m^2}{2(t_m-t_{m-1})}}.
$$

Therefore, $\vec{v}$ is jointly normal with $\mu = 0$ and $Var = \text{diag}(t_1, t_2-t_1, \cdots, t_n-t_{n-1}) = D$, and,
$$
\vec{W}
= (B(t_1), \cdots, B(t_m))
= \vec{v}
\begin{bmatrix}
1 & 1 &\cdots &1\\
0 & 1 &\cdots &1\\
\vdots & \vdots & \vdots & \vdots\\
0 & 0 &\cdots &1\\
\end{bmatrix}
= \vec{v}E.
$$
Thus, $\vec{W}$ is jointly normal with $\mu = (0,0,\cdots,0)$ and $Var = EDE^T = C$.
\end{enumerate}
\end{proof}
\end{thm}


\begin{defn}[Filtration for Brownian Motion]
$$
\begin{aligned}
\F_t &= \sigma(B(s) : s \leq t)\\
&= \text{smallest } \sigma\text{-algebra containing } \{\omega: (B(s))(\omega) \in A\} \enspace \forall s \in [0,t], A: \text{Borel}\\
&= \text{smallest } \sigma\text{-algebra making } \forall B_s, s\in[0,t] \text{ measurable}
\end{aligned}
$$
\end{defn}


\begin{rem}
\leavevmode
\begin{enumerate}
\item $(B(t))_{t\geq0}$: Stochastic process adapted to the filtration $(\F_t)$.
\item $(B(t), \F(t))$: Martingale.
\end{enumerate}

\begin{lem}
$B(t) - B(s)$ is independent of $\F_s$ $(s < t)$.
\begin{proof}[Proof of lemma]
\leavevmode
\begin{enumerate}
\item $B(t) - B(s)$ is independent of $\sigma (B(s_1), B(s_2), \cdots, B(s_n))$ for $0 < s_1 < \cdots < s_n \leq s$, and check that  $\sigma (B(s_1), B(s_2), \cdots, B(s_n)) = \sigma(B(s_1), B(s_2)-B(s_1), \cdots, B(s_n)-B(s_{n-1}))$ 
\item Let $\mathcal{H} = \bigcup_{m=1}^\infty\bigcup_{0 < s_1 < \cdots < s_n \leq s} \sigma(B(s_1), \cdots, B(s_n))$.
Then, $\mathcal{H}$ is a closed under finite intersection.
i.e., $A_1, A_2, \cdots, A_n \in \mathcal{H} \Rightarrow A_1 \cap A_2 \cap \cdots \cap A_n \in \mathcal{H}$.
\item $B(t) - B(s)$ is independent of $\mathcal{H}$ by 1.
Then, $B(t) - B(s)$ is independent of $\overline{\mathcal{H}} = \F_t$ : smallest $\sigma$-algebra containing $\mathcal{H}$. (Midterm 1 problem 2)
\end{enumerate}
\end{proof}
\end{lem}

\begin{proof}
\begin{enumerate}
\item By construction
\item $s < t \Rightarrow
\E[B(t)|\F_s] = \E[B(t)-B(s)|\F_s] + \E[B(s)|\F_s] = \E[B(t) - B(s)] + B(s)$.
\end{enumerate}
\end{proof}
\end{rem}




\chapter{Constuction of Brownian Motion}

There are three ways to construct Brownian motion.
One is by Wiener, one is by Kolmogorov, and one is by Leby.
Wiener's method gives the existence of Brownian motion in natural way.
Kolmogorov's method gives the property of Brownian motion with sample path with awful $\omega$.
Levy's method gives an instruction for Brownian motion with wierd $\omega$.

\section{Wiener Space}
Let $C = C_0[0,1] = \{ f : f \text{ is continuous on } [0,1] \text{ and } f(0) = 0\}$.
We give a norm to $C$ by $\|f\| = \sup_{0 \leq x \leq 1} |f(x)| = \max_{0 \leq x \leq 1} |f(x)|$,
and distance $d(f,g) = \|f-g\|$.
Thus, there is an open ball $B_r(x) = \{ y : d(x,y) < r \}$ and topology(open set) of $C$.
Now, there is Borel $\sigma$-algebra = smallest $\sigma$-algebra containing all open sets.

\vspace{5mm}

\textbf{Notation}\\
From now, let $\mathcal{B}(C)$ be Borel $\sigma$-algebra in $C$.

\begin{defn}[Cylindrical Sets]
A cylindrical sets $\mathcal{R}$ is a collection of subsets of $C$ of the form
$$A = \left\{ f \in C : \left(f(t_1), f(t_2), \cdots, f(t_m)\right) \in U,
0 < t_1 < t_2 < \cdots < t_m \leq 1, U \in \mathcal{B}(\R^n)\right\},$$
and $A$ is called \textbf{cylindrical set}.
\end{defn}

\textit{cf.} For $m = 1$, $\{ f : f(t_1) \in U_k \} = A_k \in \mathcal{R}$,
then $\bigcup_{k=1}^\infty A_k = \{ f : f(t_1) \in \bigcup_{k=1}^\infty U_k\} \in \mathcal{R}$.

\begin{rem}
$\mathcal{R}$ is not a $\sigma$-algebra.
\end{rem}

\begin{exmp}
$\left\{ (f(t_1), f(t_2)) \in (-1,1) \times (2,3)\right\}
= \left\{ f : f(t_1) \in (-1,1), f(t_2) \in (2,3) \right\}$.
\end{exmp}



\begin{defn}
Let $\mu : \mathcal{R} \rightarrow [0,1]$ such that
$$
\mu(A) = \iint \cdots \int \frac{1}{\sqrt{2\pi t_1}} e^{-\frac{x^2}{2t_1}} \frac{1}{\sqrt{2\pi (t_2-t_1)}} e^{-\frac{x^2}{2(t_2-t_1)}} \cdots \frac{1}{\sqrt{2\pi (t_mt_{m-1})}} e^{-\frac{x^2}{2(t_m - t_{m-1})}} dx_1 \cdots dx_m,
$$
and it is a natural definition since $B(t_i)$'s are jointly normal.
\end{defn}




\begin{thm}[Wiener]
$\mu$ is a countably additive($\sigma$-additive) function on $\mathcal{R}$.
In other words,
$A_1, A_2, \cdots$ are disjoint members of $\R$ and $\bigcup_{k=1}^\infty A_k \in \mathcal{R}$,
then $\mu(\bigcup_{k=1}^\infty A_k) = \sum_{k=1}^\infty \mu(A_k)$.
\end{thm}



\textbf{Summary}
\begin{itemize}
\item $C = C_0[0,1] \Rightarrow \mathcal{B}(C)$ is Borel $\sigma$-algebra
\item $\mathcal{R}$: collection of subsets of $C$
\item $\mu : \mathcal{R} \rightarrow [0,1]$ is $\sigma$-additive (Wiener)
\end{itemize}

\underline{Fact:} $\mathcal{R}$ is a \textbf{Ring} in the sense that
\begin{enumerate}
\item $\phi \in \mathcal{R}$
\item $A, B \in \mathcal{R} \Rightarrow A \cup B \in \mathcal{R}$
\item $A, B \in \mathcal{R} \Rightarrow A \backslash B \in \mathcal{R}$
\end{enumerate}

\begin{thm}[Caratheodory Extension Theorem]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $\mu : \mathcal{R} \rightarrow [0,1]$ is $\sigma$-additive
\item $\mathcal{R}$ is a ring
\item $\overline{\mathcal{R}}$ is a smallest $\sigma$-algebra containing $\mathcal{R}$.
\end{enumerate}
Then, there exists unique extension of $\mu$ to $\overline{\mathcal{R}}$ which is a measure.
\end{thm}


\begin{rem}
$\mu : \overline{\mathcal{R}}$ is a probability measure.
\end{rem}
\begin{rem}
We can check that $\mathcal{B}(C) \subset \overline{\mathcal{R}}$. (it suffices to check that $B\in\overline{\mathcal{R}}$ for all open ball $B$)
\end{rem}



\textbf{Conclusion:} $(C,\overline{\mathcal{R}}, \mu)$ is a probability space and says \textbf{Wiener space}.


\begin{thm}
$B: [0,1] \times C \rightarrow \mathcal{R}$, and let $B(t,\omega) = \omega(t)$.
Then, $B$ is a Brownian motion.
\end{thm}






% \section{Borel-Cantelli Lemma and Chebyshev Inequality}

% \section{Kolmogorov's Extension and continuity Theorems}

% \section{Levy's Interpolation Method}


















































% ========================================
% TA Session
% ========================================



\begin{appendices}
\chapter{TA Session}
\begin{exmp}[1.2.2]
Let $(\Omega_\infty, \mathcal{F}_\infty, \mathbb{P})$ be the independent, infinite coin-toss space.
Define stock price by
$$
\begin{aligned}
S_0(\omega) &= 4 \quad \text{for all } \omega \in \Omega_\infty\\
S_1(\omega) &=
\begin{cases}
8 & \text{if } \omega_1 = H\\
2 & \text{if } \omega_1 = T
\end{cases}
\\
S_2(\omega) &=
\begin{cases}
16 & \text{if } \omega_1 = \omega_2 = H\\
4 & \text{if } \omega_1 \neq \omega_2\\
1 & \text{if } \omega_1 = \omega_2 = T
\end{cases}
\\
\text{and in general}
\\
S_{n+1}(\omega) &=
\begin{cases}
2S_n(\omega) & \text{if } \omega_{n+1} = H\\
\frac{1}{2}S_n(\omega) & \text{if } \omega_{n+1} = T
\end{cases}
\end{aligned}
$$

Then, $S_0, S_1, \cdots, $ are random variable.\\
For example, $\mathbb{P}(S_2 = 4) = \mathbb{P}(A_{HT} \cup A_{TH}) = 2pq$

\end{exmp}



\vspace{5mm}

\begin{exmp}[2.2.2]
Let $\Omega$ be a three independent coin-toss space.
Stock price random variables $S_0, S_1, \cdots,$ are the same as the previous example.
Let the probability measure $\mathbb{P}$ be given by
$$\mathbb{P}(HHH) = p^3, \mathbb{P}(HHT) = p^2q, \cdots, \mathbb{P}(TTT) = q^3.$$
Assume $0 < p < 1$.
Then, the random variables $S_2$ and $S_3$ are not independent.\\
$\because$ Consider the sets $\{S_3 = 32\} = \{ HHH \}$ and $\{S_2 = 16\} = \{HHH, HHT \}$ whose probabilities are
$\mathbb{P}(S_3 = 32) = p^3$ and $\mathbb{P}(S_2 = 16) = p^2$.
In order to have Independence, $p^3 = \mathbb{P}(S_3 = 32) = \mathbb{P}(S_2 = 16 \text{ and } S_3 = 32) = \mathbb{P}(S_2 = 16)\mathbb{P}(S_3 = 32) = p^5\Rightarrow\!\Leftarrow$.

The random variables $S_2$ and $S_3/S_2$ are independent.
The $\sigma$-algebra generated by $S_2$ comprises $\phi, \Omega$, the atoms\\
$\{S_2 = 16\} = \{ HHH, HHT\}
, \{S_2=  4\} = \{ HTH, HTT, THH, THT\}
, \{S_2 = 1\} = \{TTH, TTH\}$, and their unions.\\

The $\sigma$-algebra generated by $S_3/S_2$ comprises $\phi, \Omega$ and\\
$\{S_3/S_2 = 2\} = \{HHH, HTH, THH, TTH\}
, \{S_3/S_2 = \frac{1}{2} \} = \{ HHT, HTT, THT, TTT\}$\\

For $A \in \sigma(S_2), B \in \sigma(S_3/S_2)$,
$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.

ex) $p^3 = \mathbb{P}(S_2 = 16 \text{ and } S_3/S_2 = 2\} = \mathbb{P}(S_2 = 16)\mathbb{P}(S_3/S_2 = 2)) = p^2 p = p^3$

\end{exmp}

\vspace{5mm}

\begin{exmp}[2.2.10 Uncorrelated, dependent normal random variables]
\leavevmode\\
Let $X, Z$ be random variable satisfying
$$
\begin{aligned}
X&: \text{ standard normal random variable}\\
Z&: \text{ independent of } X,\ \mathbb{P}(Z = 1) = \frac{1}{2},\ \mathbb{P}(Z = -1) = \frac{1}{2}
\end{aligned}
$$

Define $Y = ZX$.
Show
\begin{enumerate}
\item $Y$ is standard normal random variable
\item $X$ and $Y$ are uncorrelated but they are dependent.
\end{enumerate}


\begin{proof}
\leavevmode
\begin{enumerate}
\item
$$
\begin{aligned}
F_Y(b) &= \mathbb{P}(Y \leq b)\\
&= \mathbb{P}(Y \leq b \text{ and } Z = 1)
+ \mathbb{P}(Y \leq b \text{ and } Z = -1)\\
&= \mathbb{P}(X \leq b \text{ and } Z = 1)
+ \mathbb{P}(X \geq -b \text{ and } Z = -1)\\
&= \mathbb{P}(X \leq b)\mathbb{P}(Z=1)
+ \mathbb{P}(X \geq -b)\mathbb{P}(Z=-1)\\
&= \frac{1}{2}N(b) + \frac{1}{2}N(b)\\
&=N(b)
\end{aligned}
$$

\item
Since $\mathbb{E}X = \mathbb{E}Y = 0$,
$$Cov(X,Y) = \mathbb{E}[XY] = \mathbb{E}[ZX^2] = \mathbb{E}[Z]\mathbb{E}[X^2] = 0$$
$\therefore X$ and $Y$ are uncorrelated.\\
If $X$ and $Y$ are independent, $|X|$ and $|Y|$ are independent.
But $\mathbb{P}(|X| \leq 1, |Y| \leq 1) = \mathbb{P}(|X| \leq 1) = N(1) - N(-1)$, and $\mathbb{P}(|X| \leq 1, |Y| \leq 1) = \mathbb{P}(|X| \leq 1) \mathbb{P}(|Y| \leq 1) = (N(1)-N(-1))^2
\Rightarrow\!\Leftarrow$

\end{enumerate}
\end{proof}

Let $\mu_{X,Y}$ be a joint distribution measure of $(X,Y)$.
Since $|X| = |Y|$, $(X,Y)$ takes values only in the set
$C = \{(x,y): x = \pm y\}$.\\
It follows that for any measurable function $f$,
$$
\int_{-\infty}^\infty \int_{-\infty}^\infty \mathbb{1}_C(x,y) f_{X,Y}(x,y) dydx = 0
$$
$\therefore$ There is no joint density $f_{X,Y}$ for $(X,Y)$.


$$
\begin{aligned}
F_{X,Y}(a,b) &= \mathbb{P}(X \leq a, Y \leq b)\\
&= \mathbb{P}(X \leq a, X \leq b, Z = 1) + \mathbb{P}(X \leq a, -X \leq b, Z = -1)\\
&= \frac{1}{2} \mathbb{P}(X \leq a \wedge b) + \frac{1}{2}\mathbb{P}(-b \leq X \leq a)\\
&= \frac{1}{2} N(a \wedge b) + \frac{1}{2} ((N(a)-N(-b)) \vee 0)
\end{aligned}
$$
\end{exmp}

\begin{exmp}[2.2.12]
Let $(X,Y)$ be jointly normal with the density
$$
f_{X,Y}(x,y) = \frac{1}{2\pi \sigma_1\sigma_2 \sqrt{1-\rho^2}}
\exp \left(-\frac{1}{2(1-\rho^2)} \left[\frac{(x-\mu_1)^2}{\sigma_1^2} - \frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2}\right]\right)$$
Define $W = Y - \frac{\rho\sigma_2}{\sigma_1} X$.
Then, $X$ and $W$ are independent.

Note that linear combination of jointly normal random variables are jointly normal
(i.e., $(X,W)$ is jointly normal).

Thus it suffices to show that $Cov(X,W) = 0$ (by Thm 2.2.9)
$$
\begin{aligned}
Cov(X,W) &= \mathbb{E}[(X-\mathbb{E}X)(W-\mathbb{E}W)]\\
&= \mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] - \mathbb{E}[\frac{\rho\sigma_2}{\sigma_1}(X-\mathbb{E}X)^2]\\
&= Cov(X,Y) - \frac{\rho\sigma_2}{\sigma_1}\sigma_1^2\\
&= 0
\end{aligned}
$$

Let $f_{X,W}$ be joint density of $X$ and $W$.

$$
\begin{aligned}
\mathbb{E}[W] &= \mu_2 - \frac{\rho\sigma_2\mu_1}{\sigma_1} =: \mu_3\\
\mathbb{E}[(W-\mathbb{E}W)^2] &= \mathbb{E}[(Y-\mathbb{E}Y)^2] - \frac{2\rho\sigma_2}{\sigma_1}\mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] + \frac{\rho^2\sigma_2^2}{\sigma_1^2}\mathbb{E}[(X-\mathbb{E}X)^2]\\
&= \sigma^2 - \frac{2\rho\sigma_2}{\sigma_1}\rho\sigma_1\sigma_2 + \frac{\rho^2\sigma_2^2}{\sigma_1^2}\sigma_1^2\\
&=(1-\rho^2)\sigma_2^2 =: \sigma_3^2
\end{aligned}
$$

$\therefore f_{X,W}(x,w) = \frac{1}{2\pi\sigma_1\sigma_3} \exp\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2} - \frac{(w-\mu_3)^2}{2\sigma_3^2}\right)$.

Note that we have decomposed $Y$ into the linear combination $Y = \frac{\rho\sigma_2}{\sigma_1}X + W$ of a pair of independent normal random variables $X$ and $W$.

\end{exmp}

\vspace{5mm}

\begin{exmp}[2.3.3]
Let $\mathcal{G} = \sigma(X)$. Observe estimate $Y$ based on $X$ and error.
$$
\begin{aligned}
\mathbb{E}[Y|X] &= \frac{\rho\sigma_2}{\sigma_1} + \mathbb{E}[W] = \frac{\rho\sigma_2}{\sigma_1}(X-\mu_1) + \mu_2.\\
Y-\mathbb{E}[Y|X] &= W - \mathbb{E}[W]
\end{aligned}
$$
Note that the error is random variable with expected value zero and independent of the estimation $\mathbb{E}[Y|X]$.
\end{exmp}

\end{appendices}
\end{document}
