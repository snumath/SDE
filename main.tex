\documentclass[12pt]{report}
\usepackage[left=2cm,right=2.6cm,top=2.6cm,bottom=3cm,a4paper]{geometry}
\renewcommand{\baselinestretch}{1.3}

\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[inline]{enumitem}
\usepackage{linegoal}
\usepackage{amsmath,amssymb,latexsym,amsfonts, amsthm}
%\usepackage{verbatim}
\usepackage{xcolor}
%\usepackage{listings}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{kotex}

%\usepackage[math]{iwona}
\usepackage[tracking]{microtype}
\usepackage[sc,osf]{mathpazo}
%\usepackage{kpfonts}

\usepackage[all]{xy}

%\setlist[itemize]{label=\tiny\textbullet}
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}

% Modify Chapter Headings
% Options: Sonny, Lenny, Glenn, Conny, Rejne, Bjarne, Bjornstrup
% \usepackage[Lenny]{fncychap}

% Modify Section Style
\usepackage[explicit]{titlesec}
\usepackage{soul}
%\definecolor{titleblue}{HTML}{4a7aa4}
\newbox\TitleUnderlineTestBox
\newcommand*\TitleUnderline[1]
    {%
        \bgroup
        \setbox\TitleUnderlineTestBox\hbox{\colorbox{black}\strut}%
        \setul{\dimexpr\dp\TitleUnderlineTestBox-.3ex\relax}{.3ex}%
        \ul{#1}%
        \egroup
    }
\newcommand*\SectionNumberBox[1]
    {%
        \colorbox{black}
            {%
                \makebox[1.5em][c]
                {%
                    \vspace{7mm}
                    \color{white}%
                    \strut
                    \csname the#1\endcsname
                }
            }%
        \TitleUnderline{\ \ \ }%
    }
\titleformat{\section}
    {\Large\bfseries\sffamily\color{black}}
    {\SectionNumberBox{section}}
    {0pt}
    {\TitleUnderline{#1}}

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}



% tableofcontents with clickable
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, % make the links colored
    linkcolor=blue, % color TOC links in blue
    urlcolor=red, % color URLs in red
    linktoc=all % 'all' will create links for everything in the TOC
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New command
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\1}{\mathbb{1}}
\renewcommand{\O}{\Omega}
%\newcommand{\RP}{\mathbb{RP}}
%\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\r}{\rightarrow}
%\newcommand{\g}{\gamma}
%\newcommand{\E}{\mathcal{E}}
%\renewcommand{\S}{\mathcal{S}}
%\newcommand{\T}{\mathcal{T}}
%\newcommand{\M}{\mathfrak{M}}

%\newcommand{\inft}{\int_{-\infty}`^\infty}
%\newcommand{\rk}{\text{rank }}

\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\newcommand{\ri}{\Rightarrow}
\newcommand{\bigslant}[2]{{\raisebox{.1em}{$#1$}\left/\raisebox{-.1em}{$#2$}\right.}}

\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem 지정해주는 곳

\newtheoremstyle{break}
{\topsep}{\topsep}%
{\itshape}{}%
{\bfseries}{}%
{\newline}{}%
\theoremstyle{break}
\newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter

\newtheoremstyle{newdef}% name
{}%         Space above, empty = `usual value'
{}%         Space below
{}%         Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}%Thm head font
{}%        Punctuation after thm head
{\newline}% Space after thm head: \newline = linebreak
{}%         Thm head spec
\theoremstyle{newdef}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{remark}
\newtheorem*{exmp}{Example} % no numbering example
\newtheorem*{lem}{Lemma}
\newtheorem*{rem}{Remark} % no numbering remark



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{titlepage}
\centering
{\scshape\LARGE Seoul National University \par}
\vspace{1cm}
{\scshape\Large Lecture Note\par}
\vspace{5.5cm}
{\huge\bfseries Introduction to Stochastic \\Differential Equations\par}
\vspace{1.5cm}
\large Lecture by Seo Insuk \\
Notes taken by Lee Youngjae


\vfill
\vspace{1cm}\par
{\large \today\par}
\end{titlepage}

\setlength{\parindent}{0cm}

\tableofcontents

\setcounter{chapter}{-1}

\chapter{Introduction}
E-mail: \textit{insuk.seo@snu.ac.kr, 27-212}\\
Office Hour: Tuesday 15:00 - 16:00

Grading
\begin{itemize}
\item Mid-terms 1 (15\%, 10/10 or 17)
\item Mid-terms 2 (15\%, 11/7)
\item Fianl-term (40\%)
\item Assignment (20\%, 8-10 times)
\item Attendance (10\%, absent: -2\%, late: -1\%)
\end{itemize}


Let $X$ be a standard normal random variable in $\R$.
i.e., $\mathbb{P}[X \in [a,b]] = \int_a^b \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx$.
(Central Limit Theorem) If $x_1, x_2, \cdots, x_n \in X, E(x_i) = m, Var(x_i) = \sigma^2$, then
$$
\frac{\frac{x_1-m}{\sigma}+\frac{x_2-m}{\sigma}+\cdots+\frac{x_n-m}{\sigma}}{\sqrt n} \rightarrow X
$$

In this class, we study dynamic version of this theorem.
If $(W_t)_{t\geq 0}$ be a fluctuation, then $(W_t)_{t\geq 0}$ be a random variable in $C[0,T]$

\begin{exmp}
$\frac{dX_t}{dt} = rX_t; dX_1 = rX_tdt$. Then, $X_t = X_0 e^{rt}$ (unrisky assets, bank)\\
$dX_t = rX_tdt + \sigma X_tdW_t$, $\sigma:$ volatility (risky assets, stock)
\end{exmp}

We will study:
\begin{enumerate}
\item Probability Space
\item Random Variable
\item Expectation
\end{enumerate}

Textbooks:
\begin{enumerate}
\item Stochastic Calculus for Finance \RNum{2} (Shreve), covering chapter 1-3 or 4
\item Introduction to Stochastic Integration (Hui-Hsiung Kuo)
\end{enumerate}



\chapter{General Probability Theory}
\section{Infinite Probability Spaces}
There are three elements consisting probability space:
\begin{itemize}
\item $S$: Sample space
\item $\mathcal{E}$: Family of events $\mathcal{E} \subset 2^S$ ($\sigma$-algebra in measure theory)
\item $\mathbb{P}$: probability $\Rightarrow \mathbb{P}(E)$ is defined for all $E \in \mathcal{E}$ ($\mu$ with $\mu(S)=1$)
\end{itemize}

\begin{exmp}
\begin{minipage}[t]{\linegoal}
\begin{enumerate}
\item Toss a coin twice (H for Head, T for Tail)\\
Then, $S = \{HH, HT, TT, TT\}$
\item Uniform random variable in $[0,1]^3$\\
Then, $S = [0,1]^3$.
If $E = [0,\frac{1}{2}]^3$, then $\mathbb{P}(E) = Vol(E) = \frac{1}{8}$\\
\end{enumerate}
\end{minipage}

How to define $\mathcal{E}$?\\
In example 2, let $\mathcal{E} = $ family of all subsets of $[0,1]^3$ naively.
But Banach-Tarski Paradox says there are disjoint sets $E,F$ with $\mathbb{P}(E\cup F) \neq \mathbb{P}(E) + \mathbb{P}(F)$ in this $\mathcal{E}$.
Therefore we cannot naively set $\mathcal{E}$ (Use measure theory)\\

In example 1, suppose that we cannot see the second flip.
If $\{HH\} \not\in \mathcal{E}$ and $\{HT, HH\}\in\mathcal{E}$, then $\mathcal{E} = \{\phi, \{HH,HT\}, \{TH,TT\}, \{HH,HT,TH,TT\}\}$
\end{exmp}


\begin{defn}[Measure]
Let $\Omega$ be a non-empty set and $\mathcal{F}$ be family of subsets of $\Omega$ with
\begin{enumerate}
\item $\phi \in \mathcal{F}$
\item $A \in \mathcal{F} \Rightarrow A^C \in \mathcal{F}$
\item $A_1, A_2, \cdots \in \mathcal{F} \Rightarrow \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
\end{enumerate}
We say $\mathcal{F}$ as \textbf{$\sigma$-algebra} or \textbf{$\sigma$-field}, $A \in \mathcal{F}$ as \textbf{measurable}, and $\Omega$ as \textbf{measurable space}.
\end{defn}


\textit{Exercises.}
\leavevmode
\begin{enumerate}[label = \arabic*)]
\item $\Omega \in \mathcal{F}$
\item $A_1, A_2, \cdots \in \mathcal{F}$, then $A_1\cap A_2 \cdots \in \mathcal{F}$
\item $A_1, A_2, \cdots \in A_n \in \mathcal{F}$, then $A_1 \cup \cdots \cup A_n, A_1 \cap \cdots \cap A_n \in \mathcal{F}$.
\item $A, B \in \mathcal{F}$, then $A-B \in \mathcal{F}$
\end{enumerate}


\begin{defn}[Topological Space]
(See Rudin: \textit{Real and Complex Analysis, Chapter 1.})
Let $\Theta$ be non-empty set and $\tau$ be family of subsets of $\Theta$ with
\begin{enumerate}
\item $\phi, \Theta \in \tau$
\item $V_1, \cdots V_n \in \tau \Rightarrow V_1 \cap \cdots \cap V_n \in \tau$
\item $V_\alpha \in \tau \enspace \forall \alpha \in I \Rightarrow \bigcup_{\alpha\in I}V_\alpha \in \tau$.
\end{enumerate}
We say $V\in\tau$ be an \textbf{open set}, and $(\Theta,\tau)$ be a \textbf{topological space}.
\end{defn}


\begin{defn}[Measurable Function]
$f : (\Omega, \mathcal{F}) \rightarrow (\Theta, \tau)$ is \textbf{measurable} if
$f^{-1}(V) \in \mathcal{F} \enspace \forall V \in \tau$
\end{defn}


\begin{defn}[Positive Measure]
Let $\Omega$ be non-empty set and $\mathcal{F}$ be $\sigma$-algebra.
Then $\mu: \mathcal{F} \rightarrow [0,\infty]$ is called \textbf{measurable} if
\begin{enumerate}
\item $A_1, A_2, \cdots$: disjoint members of $\mathcal{F} \Rightarrow \mu(A_1\cup A_2\cup \cdots) = \sum_{i=1}^\infty \mu(A_i)$
\item $\mu(A) < \infty$ for some $A \in \mathcal{F}$,
\end{enumerate}
and $(\Omega, \mathcal{F}, \mu)$ is called a \textbf{measure space}.
\end{defn}


\begin{defn}[probability space, random variable]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $(\Omega, \mathcal{F}, \mathbb{P})$ is called a \textbf{probability space} if $\mathbb{P}(\Omega) = 1$.
\item $X$ is called a \textbf{random variable} if it is a function from $(\Omega, \mathcal{F}, \mathbb{P})$ to $\mathbb{R}$
\end{enumerate}
\end{defn}
% Note: Here the usage of the expression "be called as" is incorrect. 
% See: https://www.quora.com/What-is-the-difference-between-called-and-called-as


\underline{Next Class}
\begin{itemize}
\item Borel sets on $\mathbb{R}$ or $\mathbb{R}^d$
\item Lebesgue Measure
\item Lebesgue Integral (Define Expectation of random variable)
\end{itemize}

\vspace{5mm}

Last class, we define a sample space $\Omega$, a $\sigma$-algebra $\mathcal{F}$, and a (positive) measure $\mu : \mathcal{F} \rightarrow [0,\infty]$.
\vspace{5mm}

\textit{Exercises.}
\begin{itemize}
\item $A_1 \subset A_2 \subset \cdots \Rightarrow \mu(\bigcup_{i=1}^\infty A_i) = \lim_{n\rightarrow\infty}\mu(A_n)$
\item $A_1 \supset A_2 \supset \cdots, \mu(A_1) < \infty \Rightarrow \mu(\bigcap_{i=1}^\infty A_i) = \lim_{n\rightarrow\infty}\mu(A_n)$
\end{itemize}
% These are called continuity from below and continuity from above, resp.

\begin{thm}[Rudin 1.10]
Let $\mathcal{F}_0$ be a collection of subset of $\Omega$.
Then, $\exists! \mathcal{F}^*$ minimal $\sigma$-algebra containing $\mathcal F_0$.

\begin{proof}
Let $\{\mathcal{F}_\alpha, \alpha \in I\}$ be a family of $\sigma$-algebra containing $\mathcal{F}_0$.
Then, $\mathcal{F}^* = \bigcap_{\alpha\in I} F_\alpha$ satisfies the three condition:
1) contain $\mathcal{F}_0$
2) $\sigma$-algebra
3) minimal (trivial, $\mathcal{F}^* \subset \mathcal{F}_\alpha)$
\end{proof}
\end{thm}


\begin{defn}[Borel measurable]
$\mathcal{B}$ is a \textbf{Borel $\sigma$-algebra} on the topological space $(\Theta,\tau)$ if
$\mathcal{B}$ is a minimal $\sigma$-algebra containing $\tau$, and $B$ is a \textbf{Borel measurable} if $B \in \mathcal{B}$.
\end{defn}

\begin{rem}[Completion of measure space, Rudin 1.15]
\leavevmode\\
Consider an extension $(\Omega, \mathcal{F}, \mu) \rightarrow (\Omega, \overline{\mathcal{F}}, \mu)$ where
\begin{enumerate}
\item $\overline{\mathcal{F}} = \{ A \cup N : A \in \mathcal{F}, N \subset A_0 \in \mathcal{F}, \mu(A_0) = 0 \}$
\item $\mu(A \cup N) = \mu(A)$
\end{enumerate}
Then, (Check!)
\begin{enumerate}
\item (well-definedness) $A_1 \cup N_1 = A_2 \cup N_2 \Rightarrow \mu(A_1) = \mu(A_2)$
\item $\mu: \overline{\mathcal{F}}$ is $\sigma$-algebra.
\item $\mu : \overline{\mathcal{F}} \rightarrow [0,\infty]$ is a measure
\end{enumerate}
\end{rem}

\begin{exmp}
\leavevmode
\begin{enumerate}[label = \arabic*)]
\item $\mathbb{R}$
$$
\xymatrix@R+0.25em@C+2em{
\ar @{} [dr] |{}
\mathcal{F}_0 = \tau \ar[r]^{1.10} & \mathcal{B} \ar[r]^{\text{completion}} & \overline{\mathcal{B}}\\
\mathcal{L} \ar[r]^{\text{Rudin CH 2}} & \mathcal{L} \ar[r]^{\text{completion}} \ar[r] & \mathcal{L}
}
$$



\item $C[0,T] = \Omega = \{f ; f : [0,T] \rightarrow \mathbb{R}, \text{continuous}\}$.\\
Define $\mathcal{F}_0 = \{\bigcup_{t_1,t_2,\cdots,t_k}(A_1,A_2,\cdots,A_k)
: 0 \leq t_1 < t_2 < \cdots < t_k \leq T; A_1, \cdots A_k \in \overline{\mathcal{B}}
\}$.
We call $\{f \in C[0,T]: f(t_1) \in A_1, f(t_2) \in A_2, \cdots, f(t_k) \in A_k\}$ as \textbf{cylindrical set}.
Consider
% 깔끔하게좀 바꾸자
$$
\begin{aligned}
\mathcal{F}_0 &\overset{1.10}{\longrightarrow} &\mathcal{B} &\overset{\text{completion}}{\longrightarrow} &\overline{\mathcal{B}}\\
\mathbb{P}_{\text{BM}} &\overset{\text{KET}}{\longrightarrow} &\mathbb{P}_{\text{BM}} &\overset{\text{completion}}{\longrightarrow} &\mathbb{P}^*_{\text{BM}}
\end{aligned}
$$
(KET refers Kolmogorov's Extension Thm)
\end{enumerate}
\end{exmp}

\section{Random Variables and Distributions}
\begin{defn}
$f : \Omega \rightarrow \mathbb{R}$ is measurable if $f^{-1}(V) \in \mathcal{F}$ for any open set $V \subset \mathbb{R}$.
\end{defn}

\begin{rem}
$\mathcal{B}(\mathbb{R})$ = Borel $\sigma$-algebra in $\mathbb{R}$.
\end{rem}

\begin{rem}
If $f$: measurable, then $f^{-1}(B) \in \mathcal{F}$ for any $B \in \mathcal{B}(\mathbb{R})$.
\begin{proof}
Let $G = \{ A \subset \mathbb{R} : f^{-1}(A) \in \mathcal{F} \}$.
Then, $\tau \subset G$, $G: \sigma$-algebra (check!), hence $\mathcal{B}(\mathbb{R}) \subset G$.
\end{proof}
\end{rem}


\begin{defn}
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $(\Omega, \mathcal{F}, \mathbb{P})$ is a \textbf{probability space} if $(\mathbb{P}(\Omega) = 1$.
\item $X$ is a \textbf{random variable} if $X : \Omega \rightarrow \mathbb{R}$ is measurable.
\end{itemize}
\end{defn}

\begin{exmp}
\leavevmode
\begin{enumerate}
\item Toss a coin Twice.\\
$\Omega = \{HH,HT,TH,TT\}$,
$\mathcal{F} = 2^\Omega = \{$all subsets of $\Omega\}$,
$\mathbb{P}(A) = \frac{1}{4}|A|, \enspace A \in \mathcal{F}$.\\
Then, $X = $ the number of $H$'s is a random variable with $X(HH) = 2, X(HT)=X(TH)=1, X(TT)=1$.

\item Uniform random variable in $[0,1]$\\
$\Omega=  [0,1]$,
$\mathcal{F} = \{B \in \mathcal{B}(\mathbb{R}) : B \subset [0,1]\}$,
$\mathbb{P}(B) = \mathcal{L}(B)$ ($\mathbb{P}([0,1]) = \mathcal{L}([0,1]) = 1$).\\
Then, $X : [0,1] \rightarrow \mathbb{R}$ with $X(x) = x$ is a (uniform) random variable in $[0,1]$.
\end{enumerate}
\end{exmp}


\begin{rem}
$\mathcal{L}$: Lebesgue measure on $\mathbb{R}$. i.e.,$\mathcal{L}(a,b) = b-a$.
Then, $\mathcal{L}(\{a\}) = 0$\\
$(\because
\{a\} = \bigcap_{i=1}^\infty (a-\frac{1}{n}, a+\frac{1}{n})
\Rightarrow \mathcal{L}(\{a\}) = \lim_{n\rightarrow\infty} \mathcal{L}((a-\frac{1}{n}, a+\frac{1}{n})) = 0
)$\\
Similarly, $\mathcal{L}([a,b]) = \mathcal{L}([a,b)) = \mathcal{L}((a,b]) = b-a$,
$\mathcal{L}(\mathbb{Q}) = \sum_{q \in \mathbb{Q}}\mathcal{L}(\{q\}) = 0$.
\end{rem}

Return to uniform random variable,
$$\mathbb{P}[X \in (a,b)] = \mathbb{P}[\{x : X(x) \in (a,b)\}] = \mathbb{P}[(a,b)] = b-a.$$

\begin{defn}[Distribution measure on $X$]
$X$ is a random variable in $(\Omega, \mathcal{F}, \mathbb{P})$.
$\mu_X$ is a \textbf{distribution measure} on $X$ if $\mu_X$ is a probability measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ such that 
$$
\mu_X(B) = \mathbb{P}[X \in B]
= \mathbb{P}[\{\omega : X(\omega) \in B\}]
= \mathbb{P}[X^{-1}(B)] \ \forall B \in \mathcal{B}(\mathbb{R})
$$
\end{defn}


\begin{defn}[Probability density function]
$f$ is a \textbf{probability density function} of $X$ if $\mu_X((a,b)) = \int_a^b f(x)dx$
\end{defn}

\begin{rem}
There is a measure with no pdf: Dirac measure
\end{rem}

\begin{rem}
Lebesgue-Radon-Nikodym decomposition implies that any measure can be decomposed as density part and singular part.
\end{rem}

\begin{exmp}[Standard Normal random variable]
\leavevmode\\
Let $\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$.
Define $F : (0,1) \rightarrow \mathbb{R}$ by $F(x) = N^{-1}(x)$ for $N(X) = \int_{-\infty}^x \phi(y)dy$.\\
Let $\Omega = (0,1), \mathcal{F} = \{B \in \mathcal{B}(\mathbb{R}) : B \subset (0,1)\}, \mathbb{P}(A) = \mathcal{L}(A) : A \in \mathcal{B}(\mathbb{R})$.\\
Then, $Y : \Omega \ni x \mapsto F(x) \in \mathbb{R}$ is a random variable with
$$
\begin{aligned}
\mathbb{P}[Y \in (a,b)]
&= \mathbb{P}[\{x : Y(x) \in (a,b)\}]\\
&= \mathbb{P}[\{x \in (N(a),N(b))\}]\\
&= N(b)-N(a) = \int_a^b \phi(x)dx,
\end{aligned}
$$
and a density function is $\phi$.
\end{exmp}


\vspace{5mm}
Previous Question: In the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and random variable $X : \Omega \rightarrow \mathbb{R}$, the random element or random realization $\omega \in \Omega$ is a element of events in sample space.
For example, $\omega = HHTTH$ is a random element in tossing a coin five times, and $X(\omega) = 3$. ($X(\omega)$ = \# of Heads)\\
In the previous example(Standard Normal random variable), define $(\Omega, \mathcal{F}, \mathbb{P}) = ((0,1), \mathcal{B}(0,1), \mathbb{P})$, $\mathbb{P}((a,b)) = b-a$, $F(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy$, $X : (0,1) \ni \omega \mapsto F^{-1}(\omega) \in \mathbb{R}$.
Then, $X$ is called a standard normal random variable.







\section{Expectations}
In the following, let $\Omega = (\Omega, \mathcal{F}, \mathbb{P})$.
Let $X : \Omega \rightarrow \mathbb{R}$.
Then the expection $\mathbb{E}(X)$ is a mean of $X(\omega)$ with respect to the randomness of $\omega$ (given by $\mathbb{P}$)


\begin{defn}[Lebesgue Integration]
$(\Omega, \mathcal{F}, \mu)$ is a measure space, and $f : \Omega \rightarrow \mathbb{R}$ is a measurable function.
\begin{enumerate}[label = (\arabic*)]
\item $f : \Omega \rightarrow [0,\infty)$\\
Let $0 = y_0 < y_1 < y_2 < \cdots \rightarrow \mathbb{R}$ be a partition of $[0,\infty)$,\\
$\Pi = \{y_0, y_1, y_2, \cdots\}$ : $\|\Pi\| = \sup_{i\geq 1}|y_i-y_{i-1}|$, and\\
$\text{LS}_\pi = \sum_{i=0}^\infty y_i \mu[f^{-1}([y_i,y_{i+1}))]$.\\
In Rudin's book, $\lim_{\|\Pi\|\rightarrow 0} \text{LS}_\Pi$ converges to an element belonging to $[0,\infty]$.\\
Now, $\int fd\mu := \lim_{\|\Pi\|\rightarrow 0}\text{LS}_{\Pi}$ is called a \textbf{Lebesgue Integral}.

\item $f : \Omega \rightarrow \mathbb{R}$\\
Let $f^+ = \max\{f,0\} \geq 0$, and $f^- = -\min\{f,0\} \geq 0$.
Then, $f = f^+ - f^-$, and $|f| = f^+ + f^-$.
If $\int f^+d\mu < \infty$ and $\int f^- d\mu < \infty$, then we say $f$ is Lebesgue integrable and $f \in L^1(\mu)$.
The Lebesgue integral of $f = \int fd\mu$ is defined as $\int f^+ d\mu - \int f^- d\mu$
\end{enumerate}
\end{defn}

\begin{rem}
\leavevmode
\begin{enumerate}
\item $\int f^+ d\mu < \infty$ and $\int f^- d\mu = \infty$, then $\int fd\mu = -\infty$.
The others are defined similarly.
\item $f \in L^1(\mu) \Leftrightarrow \int |f|d\mu < \infty$.
\end{enumerate}
\end{rem}


\begin{exmp}[Riemann vs Lebesgue Integral (p19-22)]
\leavevmode
\begin{itemize}
\item $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathcal{L})$ Lebesgue measure where $\mathcal{L}((a,b)) = b-a$.
\item $f : \mathbb{R} \rightarrow \mathbb{R} \in L^1(\mathcal{L})$
\item (Def) $A \subset \mathbb{R}$, $\int_A fd\mu := \int f \mathbb{1}_A d\mu$, where $\mathbb{1}_A(x) = 1$ if $x \in A$, and $0$ otherwise.
\end{itemize}
If $f$ is Riemann integrable, then $\int_{[a,b]} fd\mathcal{L} = \int_a^b fdx$.

Riemann integral is a limit of approximation by a partition of $x$-axis.
On the other hand, Lebesgue integral is a limit of approximation by a partition of $y$-axis with preimage.
Partition of $x$-axis is sensitive to fluctuation and restricted to Euclidean space, while partition of $y$-axis is not.
For example, $f(x) = \mathbb{1}_\mathbb{Q}(x)$ is Lebesgue integrable, but it is not Riemann integrable since it is sensitive to fluctuation.
\end{exmp}

\begin{defn}[Almost everywhere, 1.1.5 in Textbook]
$P(x)$ is a property at $x \in \mathbb{R}$.
We say $P$ holds \textbf{almost everywhere} (or a.e.) in $\mathbb{R}$ if and only if
$\mathcal{L}(\{x:P(x)$ does not hold $\} = 0$.
\end{defn}

\begin{exmp}
$f(x) = [x]$ is continuous almost everywhere.
\end{exmp}


\begin{thm}
$f$ is Riemann integrable if and only if $f$ is continuous a.e.
\end{thm}

\textit{Exercises.} $f = g$ a.e. $\Rightarrow \int fd\mathcal{L} = \int g d\mathcal{L}$.

\begin{defn}[Almost surely]
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space.
The event $A (\in \mathcal{F})$ occurs \textbf{almost surely} (a.s.) if $\mathbb{P}(A) = 1$.
\end{defn}

\begin{exmp}
Let $X$ be a uniform random variable in $(0,1)$.
Let $A = \{ X(\omega) \neq \frac{1}{2}\}$; $\mathbb{P}(A) = 1$.
\end{exmp}



\begin{defn}[Expectation, 1.3.3. in Textbook]
\textbf{Expectation} of $X : \Omega \rightarrow \mathbb{R}$ is defined by
$$\mathbb{E}(X) := \int_\Omega X d\mathbb{P} \quad \text{if} \quad \int_{\Omega}|X|d\mathbb{P} < \infty$$ 
\end{defn}

\begin{thm}[1.3.4 in Textbook]
\leavevmode
\vspace{-6mm}
\begin{enumerate}
\item $X$ takes finite number of values $\{x_1,x_2, \cdots, x_n\} \Rightarrow \mathbb{E}(X) = \sum_{i=1}^n x_i\mathbb{P}(X = x_i)$
\item $X, Y$: random variables, $E(|X|), E(|Y|) < \infty$,
\begin{enumerate}[label = (\roman*)]
\item $X \leq Y$ a.s. (i.e. $\mathbb{P}[\{X(\omega) \leq Y(\omega)\}]=1$), then $\mathbb{E}(X) \leq \mathbb{E}(Y)$
\item $X = Y$ a.s. $\Rightarrow \mathbb{E}(X) = \mathbb{E}(Y)$
\end{enumerate}
\item $X, Y$: random variables, $\mathbb{E}(|X|), \mathbb{E}(|Y|) < \infty \Rightarrow \mathbb{E}(\alpha X + \beta Y) = \alpha \mathbb{E}(X) + \beta \mathbb{E}(Y)$.
\item Jensen's Inequality: $\phi : \mathbb{R} \rightarrow \mathbb{R}$ is a convex function $\Rightarrow \phi(\mathbb{E}(X)) \leq \mathbb{E}(\phi(X))$\\
(c.f. $\phi(t) = t^2$)
\end{enumerate}

\begin{proof}[Proof of 4.]
Define $S_\phi = \{(a,b) \in \mathbb{R}^2 : a + bt \leq \phi(t) \enspace \forall t\}$.
Then $\forall t \in \mathbb{R}, \phi(t) = \sup_{(a,b) \in S_\phi} \{a+bt\}$.
In fact, it is a equivalent condition. Now,
$$
\begin{aligned}
\phi(\mathbb{E}[X]) &= \sup_{a,b\in S_\phi} \{a + b\mathbb{E}[X]\}\\
&= \sup_{a,b \in S_\phi} \mathbb{E}[a+bX]\\
&\leq \mathbb{E}[\sup_{a,b\in S_\phi}(a+bx)] = \mathbb{E}[\phi(X)] \quad (\text{Check!})
\end{aligned}
$$
\end{proof}
\end{thm}


\begin{exmp}[Dirac Measure in $\mathbb{R}$]
$(\mathbb{R}, \mathcal{B}(\mathbb{R}), \delta_y) \enspace (y \in \mathbb{R})$ is a probability space with $\delta_y(A) = 1$ if $y \in A$, and $0$ otherwise.
Then, $\int_\mathbb{R} fd\delta_y = f(y)$ (Check!)
% The standard expression is 'a measure "on" a space', not "in"

\vspace{6mm}
Consider modeling: $X$: random variable such that probability of $x_i = p_i$ with $\sum_{i=1}^n p_i = 1$.
Then, $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mu)$ with $\mu = \sum_{i=1}^n p_i\delta_{x_i}$ is a probability space, and $P(X = x_i) = p_i$ for $X : \mathbb{R} \ni \omega \mapsto \omega \in \mathbb{R}$: Example of thm 1.3.4.
\end{exmp}


\textit{Summary:}
\begin{itemize}
\item Probability space: $(\Omega, \mathcal{F}, \mathbb{P})$
\item Random variables: $X: \Omega \rightarrow \mathbb{R}$
\item Expectation: $E(X) = \int Xd\mathbb{P}$
\end{itemize}





\section{Convention of Integrals}
We will use this section when we define the Brownian motion.
\begin{defn}
\leavevmode
\vspace{-6mm}
\begin{enumerate}[label = (\arabic*)]
\item
Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, and
$f, f_1, f_2, \cdots$ be measurable ($\Omega \rightarrow \mathbb{R}$).
Then, $f_n \rightarrow f$ \textbf{almost everywhere} (a.e.) if
$$
\mu[\{\omega:(f_n(\omega))_{n=1}^\infty \text{ does not converge to } f(\omega)\}] = 0
$$

\item
Let ($\Omega, \mathcal{F}, \mathbb{P}$) be a probability space, $X, X_1, X_2, \cdots$ be random variables.
Then, $X_n \rightarrow X$ \textbf{almost surely} (a.s.) if
$$\mathbb{P}[\{\omega: (X_n(\omega))_{n=1}^\infty \text{ does not converge to } X(\omega)\}] = 0$$
\end{enumerate}
\end{defn}


\textit{Question:}
$f_n \rightarrow $ a.e. Then, $\int f_nd\mu \rightarrow \int fd\mu$?
$X_n \rightarrow X$ a.s. Then, $\mathbb{E}[X_n] \rightarrow \mathbb{E}[X]$?


\begin{thm}[Monotone Convergence Theorem. 1.4.5 in Textbook]
$0 \leq f_1 \leq f_2 \leq \cdots$(or decreasing), and $f_n \rightarrow f$ a.e.
Then, $\int f_nd\mu \rightarrow \int fd\mu$.
\end{thm}

\begin{thm}[Dominated Convergence Theorem. 1.4.9 in Textbook]
$\exists g \in L^1(\mu)$ such that $|f_n| \leq g$ for all $n$, and $f_n \rightarrow f$ a.e.
Then, $\int f_n d\mu \rightarrow \int f d\mu$.
\end{thm}

\begin{cor}
$\exists Y \in L^1(\mathbb{P})$ such that $|X_n| \leq Y$ for all $n$, and $X_n \rightarrow X$ a.s.
Then, $\mathbb{E}[X_n] \rightarrow \mathbb{E}[X]$.
\end{cor}

\begin{exmp}
Let $f_n(x) = 
\begin{cases}
n^2x & \text{if } 0 \leq x \leq \frac{1}{n},\\
-n^2x + n & \text{if } \frac{1}{n} < x \leq \frac{2}{n},\\
0 & \text{otherwise}.
\end{cases}
$
Then, $f_n \rightarrow 0$ a.e. and $\int f_n dx = 1$.
\end{exmp}

\section{Computation of Expectations}

\textbf{Notation:} ($X : \Omega \ni \omega \mapsto X(\omega) \in \mathbb{R}$)
\begin{itemize}
\item $\mathbb{E}[X] = \int X d\mathbb{P} = \int_\Omega X(\omega) d\mathbb{P}(\omega)$
\item $\int_B X(\omega)d\mathbb{P}(\omega) := \int \mathbb{1}_B(\omega) X(\omega) d\mathbb{P}(\omega)$
\end{itemize}

\textbf{Recall:} $X$: random variable, $\mu_X$: distribution measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$,
$\mu_X(B) = \mathbb{P}(X \in B)$.


\begin{thm}
$g \in L^1(\mu_X)$.
Then, 
$\mathbb{E}[g(X)] = \int_\mathbb{R} g(x)d\mu_X(x) (:= \int g d\mu_X)$.

\begin{exmp}
$g(x) = x$.
$\int |x| d\mu_X(x) < \infty \Rightarrow \mathbb{E}[X] = \int xd\mu_X(x)$.
\end{exmp}

\begin{proof}
First, prove the thm holds for $g \geq 0$, then prove for general $g$ by $g = g^+ - g^-$.
\begin{enumerate}[label = (\arabic*)]
\item $g = \mathbb{1}_B$\\
By thm 1.3.4. (1), $E[\mathbb{1}_B(X)] = 1 \cdot \mathbb{P}[\mathbb{1}_B(X) = 1] = \mathbb{P}(X\in B) = \mu_X(B)
= \int \mathbb{1}_B(x) d\mu_X(x)$.
\item $g = \sum_{k=1}^n \alpha_k \mathbb{1}_{B_k}$\\
Trivial by linearity.
\item $g \geq 0$\\
By MCT.
See \textit{Rudin} chapter 1 for details.
\end{enumerate}
\end{proof}
\end{thm}


\textbf{Recall:} $X:$ random variable, $X$ has density function $f_X$ if
$$
\begin{aligned}
\mu_X((a,b)) &= \int_a^b f_X(x)dx \enspace \forall a,b.\\
\mu_X(B) &= \int_B f_Xd\mathcal{L} = \int_B f_X(x) d\mathcal{L}(x) = \int_B f_X(x)dx.
\end{aligned}
$$

\begin{thm}
$g \in L^1(\mu_X)$.
Then, $E(g(X)) = \int_\mathbb{R} g(x)f_X(x)dx$.

\begin{exmp}
Let $X$ be standard normal. i.e., $f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ (regardless what $X$ be).
Then, $E(X^4) = \int_\mathbb{R} x^4 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} = 3$.
\end{exmp}
\end{thm}



\chapter{Information and Conditioning}
\section{Information and $\sigma$-algebras}

\begin{exmp}
Toss a coin Three times.
$\Omega = \{ HHH, HHT, \cdots, TTT\}$.\\
$A_H = \{ HHH, HHT, HTH, HTT\}$,
$A_T = \{ THT, THT, TTH, TTT\}$.\\
Let $\mathcal{F}(1) = \{ \phi, \Omega, A_H, A_T\}$ so that
it is a $\sigma$-algebra containing the randomness up to time 1.\\
Similarly, define $A_{HH}, A_{HT}, A_{TH}, A_{TT}$.\\
Let $\mathcal{F}(2) = \{ \phi, \Omega, A_{HH}, A_{HT}, A_{TH}, A_{TT}, A_{HH} \cup A_{HT}, \cdots, A_{TT}^C\}$
so that it is a $\sigma$-algebra containing the randomness up to time 2, and define $\mathcal{F}(0)$ similarly, and let $\mathcal{F}(0) = \{ \phi, \Omega\}$.\\
Then, $\mathcal{F}(0) \subset \mathcal{F}(1) \subset \mathcal{F}(2) \subset \mathcal{F}(3)$.
Let $X_t =$ \# of heads until time $t$.
Then, $X_t$ is $\mathcal{F}(t)$=measurable. for each $t$.\\
Now, $\{ X_1 = 1\} = \{ \omega: X_1(\omega) = 1 \} = A_H$, and $\{X_1 = 0\} = \{ \omega : X_1(\omega) = 0\} = A_T$.
\end{exmp}

% 시간이 지나면서 random variable의 값이 계속 바뀌는데, 시점 t에서 주식 값이 evolute하느냐를 판단을 하고 싶다.
% 이 상황에서는 X_t와 함께 증가하는 \sigma-algebra가 필요하다.
% 시점 t에서 필요한 정보를 모아놓은 것이 sigma-algebra가 된다
% X_t는 시점 t까지의 information만 모아놓은 것.
% 우리는 X_t가 discrete한 경우가 아니라 주식처럼 X_t가 continuous한 경우를 다룬다
% 이를 확률미분방정식으로 나타내고 이걸 배운다.





\begin{defn}[$\sigma$-algebra generated by $X$]
$\Omega$ is a set, $X : \Omega \rightarrow \mathbb{R}$.
$\sigma(X) = \{ A \subset \Omega : A = X^{-1}(B)$ for some $B \in \mathcal{B}(\mathbb{R})\}$.
Then, $\sigma(X)$ is a $\sigma-$algebra(exercise) and it is called a \textbf{$\sigma$-algebra generated by} $X$.
\end{defn}

\begin{rem}
$X$ is a random variable in $(\Omega, \sigma(X))$.\\
$X$ is a random variable in $(\Omega, \mathcal{F})$, then $\sigma(X) \subset \mathcal{F}$ (exercise)
\end{rem}


\begin{defn}[$\mathcal{F}$-measurable]
$(\Omega, \mathcal{F})$: measure space.
$X : \Omega \rightarrow \mathbb{R}$.
$X$ is called \textbf{$\mathcal{F}$-measurable} if $\sigma(X) \subset \mathcal{F}$.
i.e., $X$: measurable with respect to $(\Omega, \mathcal{F}$).
\end{defn}


In example, $X(t)$ is $\mathcal{F}(t)$-measurable $\forall t$ (check!)\\
\textit{cf.} $X(t) : \Omega \rightarrow \mathbb{R}$.
$(X(t))^{-1}(B) \in \mathcal{F}(t) \enspace \forall B \in \mathcal{B}(\mathbb{R})$.\\
Enough to check $(X(t))^{-1}(\{0\}), (X(t))^{-1}(\{1\}), \cdots, (X(t))^{-1}(\{t\})$.\\
$\mathcal{F}(t)$ has enough information to determine $X(t)$ in the sense that
$\{\omega: (X(t))(\omega) \in B\} \in \mathcal{F}(t) \enspace \forall B \in \mathcal{B}(\mathbb{R})$.

\begin{defn}[Filtration, Stochastic Process]
$\Omega$: non-empty set, $T > 0$.
\begin{enumerate}
\item If $\mathcal{F}(t)$ is a $\sigma$-algebra $\forall t \in [0,T] \in T$ and $s < t \Rightarrow \mathcal{F}(s) \subset \mathcal{F}(t)$,
then $(\mathcal{F}(t) : t \in [0,T])$ is called a \textbf{filtration}
\item If $X(t) : \Omega \rightarrow \mathbb{R}$ is $\mathcal{F}(t)$-measurable $\forall t \in [0,T]$,
then $(X(t) : t \in [0,T])$ is called \textbf{Stochastic Process adopted to the filtration} $\mathcal{F}(t)$.
\end{enumerate}
\end{defn}


\section{Independence}

%\section{Conditional Expectation}
$X : \Omega \rightarrow \mathbb{R}$, $\mathcal{F}$: $\sigma$-algebra on $\Omega$.
\begin{enumerate}
\item $\mathcal{F}$ has full information to determine $X \Rightarrow X$ is $\mathcal{F}$-measurable. (2.1)
\item $\mathcal{F}$ has no information to determine $X \Rightarrow X$ is independent to $\mathcal{F}$. (2.2)
\item $\mathcal{F}$ has a partition information to determine $X \Rightarrow$ (2.3)
\end{enumerate}




\begin{defn}[independent]
$(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space.
$A, B \in \mathcal{F}$ is \textbf{independent} if $\mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)$.
\end{defn}

\textit{Question:} $X, Y$ are random variables in $(\Omega, \mathcal{F}, \mathbb{P})$.
If $X, Y$ are independent, then $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$, but the converse does not hold.


\begin{defn}
$(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space.
$\mathcal{G}, \mathcal{H} \subset \mathcal{F}$ are sub $\sigma$-algebras of $\mathcal{F}$.
$X, Y : \Omega \rightarrow \mathbb{R}$ are random variables in $(\Omega, \mathcal{F}, \mathbb{P})$.
\begin{enumerate}
\item $\mathcal{G}, \mathcal{H}$: independent iff $\P(A\cap B) = \P(A) \P(B) \enspace \forall A \in \mathcal{G}, B \in \mathcal{H}$.
\item $X, Y$: independent iff $\sigma(X), \sigma(Y)$ are independent.
\item $X, \mathcal{G}$: independent iff $\sigma(X), \mathcal{G}$ are independent.
\end{enumerate}
\end{defn}



\begin{defn}
$(\O, \F, \P)$ is a probability space.\\
$\mathcal{G}_1, \mathcal{G}_2, \cdots \mathcal{G}_n, \cdots$: sub $\sigma$-algebra of $\F$.
$X_1, X_2, \cdots, X_n, \cdots$: random variable in $(\O, \F, \P)$.
\begin{enumerate}
\item $\mathcal{G}_1, \cdots, \mathcal{G}_2$ are independent iff $\P(A_1 \cap \cdots \cap A_n) = \P(A_1) \cdots \P(A_n)$ for $A_1 \in \mathcal{G}_1, \cdots, A_n \in \mathcal{G}_n$.
\item $X_1, \cdots, X_n$ are independent iff $\sigma(X_1) \sim \sigma(X_n)$ are independent.
\item $\mathcal{G}_1, \mathcal{G}_2, \cdots$ are independent iff $\mathcal{G}_1 \sim \mathcal{G}_n$ are independent $\forall n$.
\item $X_1, X_2, \cdots$ are independent iff $X_1 \sim X_n$ are independent $\forall n$.
\end{enumerate}
\end{defn}

\begin{exmp}
Toss a coin three times.
\begin{enumerate}
\item $X(2), X(3)$ are not independent.\\
$\P(\{X(2) = 2\} \cap \{ X(3) = 1\}) \neq \P(X(2) = 2)\P(X(3)=1)$.
\item $X(2), X(3)-X(2)$ are independent.\\
Why: $X(2)$ is an information at tossing first, second times, and $X(3)$ is an information at tossing third time.
\end{enumerate}
\end{exmp}


\begin{defn}[Joint distribution]
$(\O, \F, \P)$ is a probability space. $X, Y$ are random variables in $\Omega$.
$(X, Y) : \O \ni \omega \mapsto (X(\omega), Y(\omega)) \in \R^2$
\begin{enumerate}
\item Joint Distribution Measure in $\R^2$\\
$\mu_{X,Y}(C) = \P((X,Y) \in C) $
for $C \in \mathcal{B}(\R^2)$.\\
(Note: We checked that $\{\omega : (X(\omega), Y(\omega)) \in C \} \in \mathcal{F}$ in real analysis.)

\item Joint Cumulative Distribution Function\\
$F_{X,Y}(a,b) = \P( X \leq a, Y \leq b) = \mu_{X,Y} ((-\infty, a] \times (-\infty, b])$ (check!)

\item Joint Probability Distribution Function\\
If $f_{X,Y} : \R^2 \r \R$ is Borel-measurable and satisfies
$\mu_{X,Y}(A\times B) = \int_B\int_A f_{X,Y}(x,y)dxdy$ for all $A, B \in \mathcal{B}(\R)$, then $f_{X,Y}$ is called a joint probability density function (jpdf)
\end{enumerate}
\end{defn}


\begin{thm}
$(\O,\F,\P)$ is a probability space, $X, Y$ are random variables in $\Omega$. Then, the followings are equivalent.
\begin{enumerate}[label=(\roman*)]
\item $X,Y$ are independent
\item $\mu_{X,Y}(A\times B) = \mu_X(A)\mu_Y(B) \enspace \forall A,B \in \mathcal{B}(\R)$
\item $F_{X,Y}(a,b) = F_X(a)F_Y(b) \enspace \forall a,b\in\R$
\item $\mathbb{E}[e^{uX+vY}] = \mathbb{E}[e^{uX}] \mathbb{E}[e^{vY}]$
\end{enumerate}
\end{thm}

\begin{rem}
If JPDF $f_{X,Y}$ exists, then (i) to (iv) $\Leftrightarrow f_{X,Y}(x,y) = f_X(x)f_Y(y)$ a.e.
\end{rem}

\begin{thm}
$X,Y$ are independent if and only if
$f, g : \R \r \R$ Borel-measurable, $\mathbb{E}[|f(X)g(Y)|] < \infty$ implies that
$\mathbb{E}[f(X)g(Y)] = \mathbb{E}[f(X)]\mathbb{E}[g(Y)]$.
\end{thm}

\begin{rem}
$f(x) = g(x) = x$ : $\mathbb{E}[|XY|] < \infty \Rightarrow \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$
\end{rem}


\begin{proof}
Details are exercises.
\begin{enumerate}[label = (\arabic*)]
\item $f = \mathbb{1}_A, g = \mathbb{1}_B$
\item $f, g$ are simple functions
\item $f, g \geq 0$
\item $f, g$ are general.
\end{enumerate}
\end{proof}






\underline{Review}\\
$\mathcal{G, H}$ are independent if
$\forall A \in \mathcal{G}, B \in \mathcal{H} \Rightarrow \P(A \cap B) = \P(A)\P(B).$\\
$X, Y$ are independent if $\sigma(X), \sigma(Y)$ are independent.\\
* $\sigma(X) = \{A \in \Omega : A = X^{-1}(B) \text{ for some } B \in \mathcal{B}(\R)\}$.\\
* $\mu_{X,Y}(C) = \P((X,Y) \in C) \enspace \forall C \in \mathcal{B}(\R^2)$.

\vspace{5mm}

Thm. T.F.A.E.C:
\begin{enumerate}
\item $X,Y$ are independent
\item $\mu_{X,Y}(A\times B) = \mu_X(A)\mu_Y(B)$
\item $\F_{X,Y}(x,y) = \F_X(x)\F_Y(y)$
\item (If JPDF $f_{X,Y}$ exists) $f_{X,Y}(x,y) = f_X(x)f_Y(y)$
\end{enumerate}




\begin{thm}
$(\O,\F,\P)$ is a probability space. $X,Y$ are independent random variables, $f, g : \R \r \R$ are Borel measurable.
Then, $f(X), g(Y)$ are independent.

\begin{proof}
$A \in \sigma(f(X))$;
$A = (f\circ X)^{-1}(B)$ for some $B \in \mathcal{\R}
= X^{-1}(f^{-1}(B)) \in \sigma(X)$.\\
$\therefore \sigma(f(X)) \subset \sigma(X)$, $\sigma(g(Y)) \subset \sigma(Y)
\Rightarrow \sigma(f(X)), \sigma(g(Y))$ are independent.
\end{proof}
\end{thm}

\begin{cor}
$\mathbb{E}[f(X)g(Y)] = \mathbb{E}[f(X))\mathbb{E}(g(Y)]$.
\end{cor}



\begin{defn}
$X,Y$ are random variables in $(\O,\F,\P)$.
\begin{enumerate}
\item $Var(X) = \E[(X-\E[X])^2] = \E[X^2] - \E[X]^2$
\item $std(X) = \sqrt{Var(X)}$
\item $Cov(X,Y) = \E[XY] - \E[X]\E[Y]$
\item $corr(X,Y) = cov(X,Y) / (std(X)std(Y))$
\end{enumerate}
\end{defn}

\begin{exmp}
\leavevmode
\vspace{-6mm}
\begin{itemize}
\item $X$: standard normal random variable $(N(0,1^2))$
\item $\P(Z = 1) = \P(Z = -1) = \frac{1}{2}$ ($X,Z$ are independent)
\item $Y = XZ $. Then
\begin{enumerate}[label = \arabic*)]
\item Y is standard normal,
\item $corr(X,Y) = 0$.
\item $X,Y$ are not independent.
\end{enumerate}
\end{itemize}
\end{exmp}



\begin{defn}[Jointly normal]
$X, Y$ are \textbf{jointly normal} with mean $m = (m_X, m_Y),\ Var(C) = 
\begin{bmatrix}
    C_{11} & C_{12} \\
    C_{21} & C_{22}
\end{bmatrix}
$ if
$$f_{X,Y}(z) = \frac{1}{\sqrt{(2\pi)^2\det C}}e^{-\frac{1}{2}(z-m)C^{-1}(z-m)^\mathsf{T}}$$
\end{defn}

\begin{thm}
$X,Y$ are jointly normal and uncorrelated $(C_{12} = C_{21} = 0)$.
Then, they are independent.
\end{thm}

\section{Conditional Expectation}
$(\O,\F,\P)$ is a probability space. $\int_A X d\P := \int \mathbb{1}_A X d\P = \int \mathbb{1}_A (\omega) X(\omega) d\P(\omega)$.
\begin{lem}
$\int_A Xd\P = \int_A Y d\P$ for all $A \in \F$ if and only if $X = Y$ a.s.
\begin{proof}
$A_n = \{ \omega : X(\omega) - Y(\omega) > \frac{1}{n}\},
B_n = \{ \omega : X(\omega) - Y(\omega) < -\frac{1}{n}\}$.
Then,
$$0 = \int_{A_n}(X-Y)d\P \geq \int_{A_n}\frac{1}{n}d\P = \frac{1}{n} \int \1_{A_n} d\P
= \frac{1}{n} \P(A_n)$$
Thus, $\P(A_n) = 0 \enspace \forall n$. Similarly, $\P(B_n) = 0$.
Now, $\{ \omega:X(\omega) \neq Y(\omega)\} = \left(\bigcup_{n=1}^\infty A_n\right) \cup \left(\bigcup_{n=1}^\infty B_n\right) \Rightarrow$ measure $0$.
\end{proof}
\end{lem}

\textit{Intuition}.
$(\O,\F,\P)$ is given, $X : \F$-measurable random variable, $\mathcal{G} \subset \F$ is a sub $\sigma$-algebra.
If we know nothing, then we expect $X$ as $\E[X]$.
If we know $\F$, then we expect $X$ as $X$.
Now, if we know $\mathcal{G}$, then we expect $X$ as $\E[X | \mathcal{G}]$ (what is it?)



\begin{defn}[Conditional Expectation]
$(\O,\F,\P)$ is a probability space.
$X \in L^1(\P)$ is a random variable.
$\mathcal{G}$ is a sub $\sigma$-algebra of $\F$.
We define $\E[X | \mathcal{G}]$ as
\begin{enumerate}
\item $\mathcal{G}$-measurable random variable
\item $\int_A \E[X|\mathcal{G}](\omega) d\P(\omega) = \int_A X(\omega) d\P(\omega)$.
\end{enumerate}
\end{defn}

\textit{Question.} $\E[X|\mathcal{G}]$ exists? (Yes! proof skip).
unique? (Yes! up to a.s.)

\begin{rem}
Lemma implies determine $X$ (a.s.) is equivalent to know $\int_A Xd\P \enspace \forall A \in \F$.\\
In this sense, conditional expectation $Y = \E[X|\mathcal{G}]$ is knowing $\int_A Yd\P = \int_A X d\P \enspace \forall A \in \mathcal{G}$.
\end{rem}


\begin{exmp}
Toss a coin three times.\\
$\F(0) \subset \F(1) \subset \F(2) \subset \F(3)$.
$X(t)$ is a number of heads until $t$ times; $X(t)$ is $\F(t)$-measurable.
If $\F(1) = \{ \phi, \Omega, A_H, A_T\}$, then
$\E[X(2) | \F(1)] = X(1) + \frac{1}{2}$, since we know the information of 1st flip.
\begin{proof}
Want: $\int_A (X(1) + \frac{1}{2})d\P = \int_A X(2) d\P$ for all $A \in \F(1)$
(\textit{c.f} $\P(\omega) = \frac{1}{8} \enspace \forall \omega \in \Omega$).
For $A = A_H$, $\int \1_{A_H}(\omega) = (X(1)(\omega) + \frac{1}{2})d\P(\omega) = \frac{3}{2}\P(A_H) = \frac{3}{4}$.

$\int \1_{A_H}(\omega)(X(2))(\omega) d\P(\omega)
= \sum_{\omega \in A_H} (X(2))(\omega)\P(\omega) = \frac{1}{8}(2+2+1+1) = \frac{3}{4}$

\end{proof}
\end{exmp}

\begin{rem}
$\mathcal{G} = \sigma(Y)$;
$\E[X|\mathcal{G}] = \E[X | \sigma(Y)] := \E[X|Y]$
\end{rem}

\begin{thm}
$X, Y$ are independent random variable in $(\O,\F,\P)$, $\mathcal{G}$ is a sub $\sigma$-algebra of $\F$.
\begin{enumerate}
\item $\E[aX + bY | \mathcal{G}] = a\E[X|\mathcal{G}] + b\E[Y|\mathcal{G}]$
\item $X$ is $\mathcal{G}$-measurable.
Then, $\E[XY|\mathcal{G}] = X\E[Y|\mathcal{G}]$.
\item $\mathcal{H}$ is a sub $\sigma$-algebra of $\mathcal{G}$. Then,
$\E[\E[X|\mathcal{G}]|\mathcal{H}] = \E[X|\mathcal{H}]$.
\item $X, \mathcal{G}$ are independent, then $\E[X|\mathcal{G}] = \E[X]$.
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item Exercise

\item We only need to show that $X \geq 0, Y \geq 0$ implies 2.
\begin{enumerate}
\item $X = \1_B$\\
Want: $\E[\1_B Y | \mathcal{G}] = \1_B \E[Y | \mathcal{G}]$ for $B \in \mathcal{G}$.

\item $X = \sum_{i=1}^n \alpha+i \1_{B_i}$\\
Use linearity.

\item $X \geq 0$\\
Use MCT

\end{enumerate}

\item Want: $\E[\E[X|\mathcal{G}]|\mathcal{H}] = \E[X|\mathcal{H}]$\\
Let $A \in \mathcal{H}$.
Then, $\int_A \E[\E[X|\mathcal{G}]|\mathcal{H}](\omega) d\P(\omega)
= \int_A \E[X|\mathcal{G}] d\P
= \int_A Xd\P
= \int_A \E[X|\mathcal{H}]d\P$

\item Can be shown similarly as in 2.
Check $X = \1_B$ case. (Hint: $A \in \mathcal{G} \Rightarrow A, B$ are independent.)
\end{enumerate}
\end{proof}
\end{thm}


\begin{exmp}[Revisit]
$$
\begin{aligned}
\E[X(2) | \F(1)]
&= \E[X(2) - X(1) + X(1) | \F(1)]\\
&= \E[X(2) - X(1) | \F(1)] + X(1)\\
&= \E[X(2) - X(1)] + X(1)\\
&= \frac{1}{2} + X(1)
\end{aligned}
$$
\end{exmp}



\end{document}
